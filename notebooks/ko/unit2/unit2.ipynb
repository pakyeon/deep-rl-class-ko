{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pakyeon/deep-rl-class-ko/blob/main/notebooks/ko/unit2/unit2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb_ProuHiOe"
      },
      "source": [
        "# 유닛 2: FrozenLake-v1 ⛄ 및 Taxi-v3 🚕를 사용한 Q-Learning\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/thumbnail.jpg\" alt=\"Unit 2 Thumbnail\">\n",
        "\n",
        "이 노트북에서는 **처음으로 강화 학습 에이전트를 직접 코딩**하여 FrozenLake ❄️를 Q-Learning을 통해 플레이하고, 이를 커뮤니티와 공유하며 다양한 설정을 실험해볼 수 있습니다.\n",
        "\n",
        "⬇️ 아래는 **단 몇 분 만에 여러분이 얻게 될 결과물의 예시입니다.** ⬇️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRU_vXBrl1Jx"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/envs.gif\" alt=\"Environments\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPTBOv9HYLZ2"
      },
      "source": [
        "### 🎮 환경:\n",
        "\n",
        "- [FrozenLake-v1](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)\n",
        "- [Taxi-v3](https://gymnasium.farama.org/environments/toy_text/taxi/)\n",
        "\n",
        "### 📚 RL 라이브러리:\n",
        "\n",
        "- Python 및 NumPy  \n",
        "- [Gymnasium](https://gymnasium.farama.org/)\n",
        "\n",
        "우리는 튜토리얼을 지속적으로 개선하고 있으니, **이 노트북에서 문제를 발견하셨다면** [GitHub 저장소에 이슈를 등록해 주세요](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6tjI2tHQ8j"
      },
      "source": [
        "## 이 노트북의 목표 🏆\n",
        "\n",
        "이 노트북을 마치면, 여러분은 다음을 할 수 있게 됩니다:\n",
        "\n",
        "- **Gymnasium** 환경 라이브러리를 사용할 수 있습니다.  \n",
        "- Q-Learning 에이전트를 처음부터 직접 코딩할 수 있습니다.  \n",
        "- **학습시킨 에이전트와 코드를 Hub에 업로드**하고, 멋진 비디오 리플레이와 평가 점수 🔥까지 함께 공유할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viNzVbVaYvY3"
      },
      "source": [
        "## 이 노트북은 딥 강화 학습 과정(Deep Reinforcement Learning Course)의 일부입니다\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p5HnEefISCB"
      },
      "source": [
        "이 무료 강좌에서는 다음을 배우게 됩니다:\n",
        "\n",
        "- 📖 **이론과 실습**을 통해 딥 강화 학습(Deep Reinforcement Learning)을 학습합니다.  \n",
        "- 🧑‍💻 Stable Baselines3, RL Baselines3 Zoo, CleanRL, Sample Factory 2.0 같은 **유명한 딥 RL 라이브러리들을 사용하는 방법**을 배웁니다.  \n",
        "- 🤖 **다양한 독특한 환경에서 에이전트를 훈련**시킵니다.\n",
        "\n",
        "그리고 그 외 더 많은 내용을 배울 수 있어요. 📚 전체 커리큘럼은 👉 [여기에서 확인하세요](https://simoninithomas.github.io/deep-rl-course)\n",
        "\n",
        "또한 **[이곳을 클릭해 코스에 등록하세요](http://eepurl.com/ic5ZUD)**  \n",
        "(등록 시 이메일을 수집하는 이유는 각 유닛이 공개될 때 링크를 보내드리고, 챌린지나 업데이트 정보를 전달해드리기 위함입니다.)\n",
        "\n",
        "커뮤니티와 교류하고 저희와 소통하려면 디스코드 서버에 참여하는 것이 가장 좋습니다 👉🏻 https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-mo_6rXIjRi"
      },
      "source": [
        "## 사전 준비 사항 🏗️\n",
        "\n",
        "이 노트북을 시작하기 전에 다음을 완료해야 합니다:\n",
        "\n",
        "🔲 📚 **[Unit 2의 Q-Learning 이론](https://huggingface.co/deep-rl-course/unit2/introduction)**을 학습하세요 🤗"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ONOODsyrMU"
      },
      "source": [
        "## Q-Learning에 대한 간략한 요약"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V68VveLacfxJ"
      },
      "source": [
        "*Q-Learning*은 다음과 같은 방식으로 작동하는 **강화 학습(RL) 알고리즘**입니다:\n",
        "\n",
        "- *Q-함수(Q-Function)*를 학습하며, 이는 **행동 가치 함수(action-value function)**입니다. 이 함수는 내부 메모리로 *Q-테이블(Q-table)*에 **모든 상태-행동 쌍의 값을 저장**합니다.\n",
        "\n",
        "- 어떤 상태와 행동이 주어졌을 때, Q-함수는 **Q-테이블에서 해당 값**을 찾아냅니다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-function-2.jpg\" alt=\"Q function\"  width=\"100%\"/>\n",
        "\n",
        "- 훈련이 완료되면, 우리는 **최적의 Q-함수**, 즉 **최적의 Q-테이블**을 얻게 됩니다.\n",
        "\n",
        "- 그리고 **최적의 Q-함수**를 갖게 되면, 결국 **각 상태마다 최적의 행동을 알 수 있으므로, 이는 곧 최적의 정책(optimal policy)**을 의미합니다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/link-value-policy.jpg\" alt=\"Link value policy\"  width=\"100%\"/>\n",
        "\n",
        "하지만 초반에는 **Q-테이블이 아무 쓸모 없습니다.** 각 상태-행동 쌍에 대해 임의의 값(보통 0으로 초기화됨)을 주기 때문입니다. 그러나 환경을 탐험하고 Q-테이블을 점차 업데이트하면서 **더 나은 근사값**을 얻게 됩니다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/q-learning.jpeg\" alt=\"q-learning.jpeg\" width=\"100%\"/>\n",
        "\n",
        "다음은 Q-Learning의 의사 코드(pseudocode)입니다:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEtx8Y8MqKfH"
      },
      "source": [
        "# 첫 번째 강화 학습 알고리즘을 코딩해 보겠습니다. 🚀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kdxb1IhzTn0v"
      },
      "source": [
        "이 실습을 [인증 과정](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)에 포함시키기 위해서는, 학습시킨 Taxi 모델을 Hub에 업로드하고 **결과가 4.5 이상**이어야 합니다.\n",
        "\n",
        "결과를 확인하려면, [리더보드](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)로 이동하여 본인의 모델을 찾으세요.  \n",
        "**결과 값 = 평균 보상(mean_reward) - 보상의 표준편차(std of reward)** 입니다.\n",
        "\n",
        "인증 절차에 대한 더 자세한 정보는 아래 링크에서 확인할 수 있습니다 👉  \n",
        "[https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gpxC1_kqUYe"
      },
      "source": [
        "## 의존성 설치 및 가상 디스플레이 생성 🔽\n",
        "\n",
        "이 노트북에서는 리플레이 영상을 생성할 필요가 있습니다. Colab에서는 **환경을 렌더링하기 위한 가상 화면(virtual screen)**이 필요하며, 이를 통해 프레임을 녹화할 수 있습니다.\n",
        "\n",
        "따라서 아래 셀은 필요한 라이브러리들을 설치하고 가상 화면을 생성 및 실행합니다 🖥\n",
        "\n",
        "설치될 주요 라이브러리는 다음과 같습니다:\n",
        "\n",
        "- `gymnasium`: FrozenLake-v1 ⛄ 및 Taxi-v3 🚕 환경을 포함합니다.  \n",
        "- `pygame`: FrozenLake-v1 및 Taxi-v3의 UI 렌더링에 사용됩니다.  \n",
        "- `numpy`: Q-테이블을 다루는 데 사용됩니다.  \n",
        "\n",
        "🤗 Hugging Face Hub는 누구나 모델과 데이터셋을 공유하고 탐색할 수 있는 중심 플랫폼입니다. 버전 관리, 지표, 시각화 등의 기능이 포함되어 있어 다른 사람들과 쉽게 협업할 수 있습니다.\n",
        "\n",
        "Q-Learning을 사용하는 모든 딥 RL 모델은 여기에서 확인할 수 있어요 👉  \n",
        "[https://huggingface.co/models?other=q-learning](https://huggingface.co/models?other=q-learning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit2/requirements-unit2.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n71uTX7qqzz2"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!apt install ffmpeg xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K6XC13pTfFiD"
      },
      "source": [
        "새로 설치된 라이브러리를 적용하려면 **런타임을 재시작해야 할 때가 있습니다**.  \n",
        "다음 셀은 **런타임을 강제로 중단시킬 것이므로, 재연결 후 여기부터 코드를 다시 실행해야 합니다**.  \n",
        "이 트릭 덕분에 **가상 화면을 정상적으로 실행할 수 있을 것입니다**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kuZbWAkfHdg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaY1N4dBrabi"
      },
      "outputs": [],
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-7f-Swax_9x"
      },
      "source": [
        "## 패키지 임포트 📦\n",
        "\n",
        "설치한 라이브러리 외에도, 다음과 같은 추가 패키지를 사용합니다:\n",
        "\n",
        "- `random`: 무작위 수를 생성하기 위해 사용됩니다 (에이전트의 ε-탐욕 정책에서 활용됩니다).  \n",
        "- `imageio`: 리플레이 비디오를 생성하는 데 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcNvOAQlysBJ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "import pickle5 as pickle\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp4-bXKIy1mQ"
      },
      "source": [
        "이제 Q-Learning 알고리즘을 코딩할 준비가 되었습니다 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xya49aNJWVvv"
      },
      "source": [
        "# Part 1: 프로즌 레이크 ⛄ (미끄럽지 않은 버전)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAvihuHdy9tw"
      },
      "source": [
        "## [FrozenLake 환경 ⛄] 생성 및 이해하기  \n",
        "---  \n",
        "\n",
        "💡 환경을 사용하기 시작할 때 좋은 습관은 **문서를 확인하는 것**입니다.  \n",
        "\n",
        "👉 https://gymnasium.farama.org/environments/toy_text/frozen_lake/  \n",
        "\n",
        "---  \n",
        "\n",
        "Q-Learning 에이전트를 훈련시켜 **시작 지점(S)에서 목표 지점(G)까지 얼어있는 타일(F)만 밟고 구멍(H)은 피해서 이동**하도록 만들 것입니다.  \n",
        "\n",
        "환경 크기는 두 가지 중 선택할 수 있습니다:  \n",
        "\n",
        "- `map_name=\"4x4\"`: 4x4 격자  \n",
        "- `map_name=\"8x8\"`: 8x8 격자  \n",
        "\n",
        "환경은 두 가지 모드를 가집니다:  \n",
        "\n",
        "- `is_slippery=False`: 얼어있는 호수가 미끄럽지 않아서 에이전트가 **항상 의도한 방향으로 이동**합니다 (결정적).  \n",
        "- `is_slippery=True`: 얼어있는 호수가 미끄러워서 에이전트가 **항상 의도한 방향으로 이동하지 않을 수 있습니다** (확률적)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaW_LHfS0PY2"
      },
      "source": [
        "지금은 간단하게 4x4 맵과 미끄럽지 않은 설정으로 진행합니다.  \n",
        "환경을 시각화하는 방식을 지정하는 `render_mode`라는 매개변수를 추가합니다.  \n",
        "이번 경우에는 **환경의 영상을 마지막에 기록하고 싶기 때문에 render_mode를 `rgb_array`로 설정해야 합니다**.\n",
        "\n",
        "[문서에 설명된 것처럼](https://gymnasium.farama.org/api/env/#gymnasium.Env.render), `\"rgb_array\"`는 환경의 현재 상태를 나타내는 **단일 프레임을 반환**합니다.  \n",
        "이 프레임은 `(x, y, 3)` 형태의 `np.ndarray`로, `x`×`y` 픽셀 이미지의 RGB 값을 나타냅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IzJnb8O3y8up"
      },
      "outputs": [],
      "source": [
        "# 4x4 맵과 ​​미끄럽지 않은 버전, render_mode=\"rgb_array\"를 사용하여 FrozenLake-v1 환경을 만듭니다.\n",
        "env = gym.make() # TODO 올바른 매개변수를 사용하세요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji_UrI5l2zzn"
      },
      "source": [
        "### 해결 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNxUbPMP0akP"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False, render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KASNViqL4tZn"
      },
      "source": [
        "다음과 같이 사용자 지정 격자를 직접 만들 수도 있습니다:\n",
        "\n",
        "```python\n",
        "desc = [\"SFFF\", \"FHFH\", \"FFFH\", \"HFFG\"]\n",
        "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
        "```\n",
        "\n",
        "하지만 지금은 **기본 환경**을 사용할 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXbTfdeJ1Xi9"
      },
      "source": [
        "### 환경이 어떻게 생겼는지 살펴보자:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [],
      "source": [
        "# 우리는 gym.make(\"<name_of_the_environment>\")를 사용하여 환경을 생성합니다. - `is_slippery=False`: 미끄럽지 않은 얼어붙은 호수 환경에서는 에이전트가 항상 의도한 방향으로 이동합니다 (결정론적).\n",
        "print(\"_____관찰 공간_____ \\n\")\n",
        "print(\"관찰 공간:\", env.observation_space)\n",
        "print(\"샘플 관찰값:\", env.observation_space.sample())  # 임의의 관찰값 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "`Observation Space Shape Discrete(16)`을 통해 관찰값이 정수이며, 이는 **에이전트의 현재 위치를 current_row * ncols + current_col (row와 col은 0부터 시작)** 으로 표현한다는 것을 알 수 있습니다.\n",
        "\n",
        "예를 들어, 4x4 맵에서 목표 위치는 다음과 같이 계산할 수 있습니다: 3 * 4 + 3 = 15. 가능한 관찰값의 수는 맵의 크기에 따라 달라집니다. **예를 들어, 4x4 맵에는 16개의 가능한 관찰값이 있습니다.**\n",
        "\n",
        "예시로, 상태(state) = 0은 다음과 같은 모습입니다:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/frozenlake.png\" alt=\"FrozenLake\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We5WqOBGLoSm"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____행동 공간_____ \\n\")\n",
        "print(\"행동 공간 크기:\", env.action_space.n)\n",
        "print(\"샘플 행동:\", env.action_space.sample())  # 임의의 행동 선택"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyxXwkI2Magx"
      },
      "source": [
        "행동 공간(에이전트가 취할 수 있는 가능한 행동들의 집합)은 4개의 이산적인 행동으로 구성되어 있습니다 🎮:  \n",
        "- 0: 왼쪽으로 이동  \n",
        "- 1: 아래로 이동  \n",
        "- 2: 오른쪽으로 이동  \n",
        "- 3: 위로 이동  \n",
        "\n",
        "보상 함수 💰:  \n",
        "- 목표 지점 도달: +1  \n",
        "- 구멍에 빠짐: 0  \n",
        "- 얼어있는 칸 도달: 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pFhWblk3Awr"
      },
      "source": [
        "## Q-테이블 생성 및 초기화 🗄️\n",
        "\n",
        "(👀 의사코드의 Step 1)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
        "\n",
        "이제 Q-테이블을 초기화할 시간입니다!  \n",
        "몇 개의 행(상태)과 열(행동)을 사용할지 알기 위해, 우리는 행동 공간과 관찰 공간을 알아야 합니다.  \n",
        "이 값들은 앞에서 알았지만, 알고리즘이 다양한 환경에 맞게 일반화되도록 **프로그래밍적으로** 얻는 것이 좋습니다.  \n",
        "\n",
        "Gym은 이를 위한 방법을 제공합니다:  \n",
        "`env.action_space.n`와 `env.observation_space.n`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3ZCdluj3k0l"
      },
      "outputs": [],
      "source": [
        "state_space =\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space =\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCddoOXM3UQH"
      },
      "outputs": [],
      "source": [
        "# Q-테이블을 (상태 공간, 행동 공간) 크기로 생성하고, 각 값을 0으로 초기화해봅시다. np.zeros는 튜플 (a, b)를 필요로 합니다.\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable =\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9YfvrqRt3jdR"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67OdoKL63eDD"
      },
      "source": [
        "### 해결 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HuTKv3th3ohG"
      },
      "outputs": [],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnrb_nX33fJo"
      },
      "outputs": [],
      "source": [
        "# Q-테이블을 (상태 공간, 행동 공간) 크기로 생성하고, 각 값을 0으로 초기화해봅시다. np.zeros는 튜플 (a, b)를 필요로 합니다.\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0WlgkVO3Jf9"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Atll4Z774gri"
      },
      "source": [
        "## 탐욕적 정책 정의 🤖\n",
        "\n",
        "Q-Learning은 **오프-정책** 알고리즘이기 때문에 두 가지 정책을 사용합니다. 이는 **행동을 취할 때와 가치 함수를 업데이트할 때 다른 정책을 사용한다**는 것을 의미합니다.\n",
        "\n",
        "- **엡실론-탐욕적 정책** (행동 정책)\n",
        "- **탐욕적 정책** (가치 함수 업데이트 정책)\n",
        "\n",
        "탐욕적 정책은 Q-Learning 에이전트가 훈련을 완료했을 때 최종적으로 가지게 될 정책이기도 합니다. 탐욕적 정책은 Q-테이블을 사용하여 행동을 선택하는 데 사용됩니다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/off-on-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3SCLmLX5bWG"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # 착취: 가장 높은 상태-행동 값을 가진 행동을 취하기\n",
        "  action =\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2_-8b8z5k54"
      },
      "source": [
        "#### 해결 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se2OzWGW5kYJ"
      },
      "outputs": [],
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # 착취: 가장 높은 상태-행동 값을 가진 행동을 취하기\n",
        "  action = np.argmax(Qtable[state][:])\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flILKhBU3yZ7"
      },
      "source": [
        "## 엡실론-탐욕적 정책 정의 🤖\n",
        "\n",
        "엡실론-탐욕적 정책은 탐색/착취의 균형을 처리하는 훈련 정책입니다.\n",
        "\n",
        "엡실론-탐욕적 정책의 아이디어는 다음과 같습니다:\n",
        "\n",
        "- *확률 1 - ɛ* : **착취**를 수행합니다 (즉, 에이전트는 가장 높은 상태-행동 값이 있는 행동을 선택합니다).\n",
        "  \n",
        "- *확률 ɛ* : **탐색**을 수행합니다 (무작위 행동을 시도합니다).\n",
        "\n",
        "훈련이 진행됨에 따라 점차적으로 **엡실론 값을 줄여가며, 탐색은 점점 적게 하고, 착취는 더 많이 하게 됩니다.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-4.jpg\" alt=\"Q-Learning\" width=\"100%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bj7x3in3_Pq"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # 0과 1 사이의 무작위 숫자 생성\n",
        "  random_num =\n",
        "  # random_num > epsilon이면 --> 착취\n",
        "  if random_num > epsilon:\n",
        "    # 현재 상태에서 가장 높은 Q값을 가지는 행동 선택\n",
        "    # np.argmax가 여기서 유용할 수 있습니다\n",
        "    action =\n",
        "  # 그렇지 않으면 --> 탐색\n",
        "  else:\n",
        "    action = # 무작위 행동 선택\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R5ej1fS4P2V"
      },
      "source": [
        "#### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYxHuckr4LiG"
      },
      "outputs": [],
      "source": [
        "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
        "  # 0과 1 사이의 무작위 숫자 생성\n",
        "  random_num = random.uniform(0, 1)\n",
        "  \n",
        "  # random_num > epsilon이면 --> 착취\n",
        "  if random_num > epsilon:\n",
        "    # 현재 상태에서 가장 높은 Q값을 가지는 행동 선택\n",
        "    # np.argmax가 여기서 유용할 수 있습니다\n",
        "    action = greedy_policy(Qtable, state)\n",
        "  # 그렇지 않으면 --> 탐색\n",
        "  else:\n",
        "    # 무작위 행동 선택\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "  return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hW80DealcRtu"
      },
      "source": [
        "## 하이퍼파라미터 정의 ⚙️\n",
        "\n",
        "탐험(exploration)과 관련된 하이퍼파라미터는 가장 중요한 요소 중 하나입니다.\n",
        "\n",
        "- 에이전트가 **충분히 상태 공간을 탐험**하여 올바른 가치 근사를 학습할 수 있도록 해야 합니다. 이를 위해 epsilon을 점진적으로 감소시키는 방식이 필요합니다.\n",
        "- epsilon을 너무 빠르게 줄이면 (decay_rate가 너무 크면), **에이전트가 문제를 해결하지 못하고 갇힐 위험이 있습니다**. 이는 상태 공간을 충분히 탐험하지 못해 학습이 불완전해지기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1tWn0tycWZ1"
      },
      "outputs": [],
      "source": [
        "# 학습 파라미터\n",
        "n_training_episodes = 10000  # 전체 학습 에피소드 수\n",
        "learning_rate = 0.7          # 학습률\n",
        "\n",
        "# 평가 파라미터\n",
        "n_eval_episodes = 100        # 전체 평가(테스트) 에피소드 수\n",
        "\n",
        "# 환경 파라미터\n",
        "env_id = \"FrozenLake-v1\"     # 환경 이름\n",
        "max_steps = 99               # 에피소드당 최대 스텝 수\n",
        "gamma = 0.95                 # 감가율 (discount factor)\n",
        "eval_seed = []               # 평가 시 환경의 시드\n",
        "\n",
        "# 탐험 파라미터\n",
        "max_epsilon = 1.0             # 초기 탐험 확률\n",
        "min_epsilon = 0.05            # 최소 탐험 확률\n",
        "decay_rate = 0.0005           # 탐험 확률의 지수 감쇠율"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDb7Tdx8atfL"
      },
      "source": [
        "## 학습 루프 메서드 생성\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit3/Q-learning-2.jpg\" alt=\"Q-Learning\" width=\"100%\"/>\n",
        "\n",
        "학습 루프는 다음과 같은 흐름으로 진행됩니다:\n",
        "\n",
        "```\n",
        "전체 학습 에피소드 수만큼 반복:\n",
        "\n",
        "  epsilon 감소 (탐험이 점점 덜 필요해짐)\n",
        "  환경 초기화\n",
        "\n",
        "    max_steps만큼 반복:\n",
        "      epsilon-greedy 정책을 사용하여 행동 At 선택\n",
        "      행동 a를 취하고, 다음 상태 s'와 보상 r 관측\n",
        "      Bellman 방정식을 사용해 Q값 Q(s,a) 업데이트:\n",
        "        Q(s,a) ← Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      종료 조건이면 에피소드 종료\n",
        "      다음 상태를 현재 상태로 갱신\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paOynXy3aoJW"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # epsilon 감소 (탐험이 점점 덜 필요해지므로)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # 환경 초기화\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # 반복\n",
        "    for step in range(max_steps):\n",
        "      # epsilon-greedy 정책을 사용하여 At 행동 선택\n",
        "      action =\n",
        "\n",
        "      # At 행동을 취하고 Rt+1과 St+1을 관측\n",
        "      # 행동 (a)을 취하고 결과 상태(s')와 보상 (r)을 관찰하세요.\n",
        "      new_state, reward, terminated, truncated, info =\n",
        "\n",
        "      # Q(s,a) 업데이트: Q(s,a) ← Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] =\n",
        "\n",
        "      # 종료 조건이면 에피소드 종료\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # 다음 상태를 현재 상태로 갱신\n",
        "      state = new_state\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnpk2ePoem3r"
      },
      "source": [
        "#### 해결 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IyZaYbUAeolw"
      },
      "outputs": [],
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in tqdm(range(n_training_episodes)):\n",
        "    # epsilon 감소 (탐험이 점점 덜 필요해지므로)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # 환경 초기화\n",
        "    state, info = env.reset()\n",
        "    step = 0\n",
        "    terminated = False\n",
        "    truncated = False\n",
        "\n",
        "    # 반복\n",
        "    for step in range(max_steps):\n",
        "      # epsilon-greedy 정책을 사용하여 At 행동 선택\n",
        "      action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
        "\n",
        "      # At 행동을 취하고 Rt+1과 St+1을 관측\n",
        "      # 행동 (a)을 취하고 결과 상태(s')와 보상 (r)을 관찰하세요.\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "      # Q(s,a) 업데이트: Q(s,a) ← Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
        "\n",
        "      # 종료 조건이면 에피소드 종료\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "\n",
        "      # 다음 상태를 현재 상태로 갱신\n",
        "      state = new_state\n",
        "  return Qtable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLwKQ4tUdhGI"
      },
      "source": [
        "## Q-Learning 에이전트 학습 🏃"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPBxfjJdTCOH"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVeEhUCrc30L"
      },
      "source": [
        "## 이제 Q-Learning 테이블이 어떻게 생겼는지 확인해봅시다 👀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmfchsTITw4q"
      },
      "outputs": [],
      "source": [
        "Qtable_frozenlake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUrWkxsHccXD"
      },
      "source": [
        "## 평가 방법 📝\n",
        "\n",
        "- Q-Learning 에이전트를 테스트하기 위해 사용할 평가 방법을 정의했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNl0_JO2cbkm"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "  Q-Learning 에이전트를 ``n_eval_episodes`` 에피소드 동안 평가하고 평균 보상과 보상의 표준편차를 반환합니다.\n",
        "  :param env: 평가 환경\n",
        "  :param max_steps: 에피소드당 최대 스텝 수\n",
        "  :param n_eval_episodes: 에이전트를 평가할 에피소드 수\n",
        "  :param Q: Q-테이블\n",
        "  :param seed: 평가 시 사용할 시드 배열 (taxi-v3의 경우)\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in tqdm(range(n_eval_episodes)):\n",
        "    if seed:\n",
        "      state, info = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state, info = env.reset()\n",
        "    step = 0\n",
        "    truncated = False\n",
        "    terminated = False\n",
        "    total_rewards_ep = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "      # 주어진 상태에서 최대 예상 미래 보상을 가지는 행동 선택\n",
        "      action = greedy_policy(Q, state)\n",
        "      new_state, reward, terminated, truncated, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "\n",
        "      if terminated or truncated:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  \n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jJqjaoAnxUo"
      },
      "source": [
        "## Q-Learning 에이전트 평가 📈\n",
        "\n",
        "- 보통 평균 보상은 1.0이어야 합니다.\n",
        "- **환경은 상대적으로 쉽습니다**. 상태 공간이 매우 작아서 (16개) 그렇습니다. 환경을 더 복잡하게 만들고 싶다면, [미끄러운 버전](https://gymnasium.farama.org/environments/toy_text/frozen_lake/)으로 바꿔볼 수 있습니다. 이 버전은 확률적 요소를 도입해 환경을 더 복잡하게 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fAgB7s0HEFMm"
      },
      "outputs": [],
      "source": [
        "# 우리 에이전트를 평가하세요.\n",
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxaP3bPdg1DV"
      },
      "source": [
        "## 학습된 모델을 Hub에 게시하기 🔥\n",
        "\n",
        "학습 후 좋은 결과를 확인했으므로, **한 줄의 코드로 학습된 모델을 Hub 🤗에 게시할 수 있습니다**.\n",
        "\n",
        "다음은 모델 카드의 예시입니다:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/modelcard.png\" alt=\"Model card\" width=\"100%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv0k1JQjpMq3"
      },
      "source": [
        "Hub의 내부에서는 git 기반 저장소를 사용합니다 (git이 무엇인지 모른다고 걱정하지 마세요). 즉, 실험을 진행하고 에이전트를 개선하면서 새로운 버전으로 모델을 업데이트할 수 있다는 의미입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ5LrR-joIHD"
      },
      "source": [
        "#### 이 코드를 수정하지 마세요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jex3i9lZ8ksX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, snapshot_download\n",
        "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
        "\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qo57HBn3W74O"
      },
      "outputs": [],
      "source": [
        "def record_video(env, Qtable, out_directory, fps=1):\n",
        "  \"\"\"\n",
        "  에이전트의 재생 영상을 생성합니다.\n",
        "  :param env: 환경\n",
        "  :param Qtable: 에이전트의 Q-table\n",
        "  :param out_directory: 출력 디렉토리\n",
        "  :param fps: 초당 프레임 수 (taxi-v3와 frozenlake-v1에서는 1을 사용)\n",
        "  \"\"\"\n",
        "  images = []\n",
        "  terminated = False\n",
        "  truncated = False\n",
        "  state, info = env.reset(seed=random.randint(0,500))\n",
        "  img = env.render()\n",
        "  images.append(img)\n",
        "  while not terminated or truncated:\n",
        "    # 주어진 상태에서 최대 예상 미래 보상을 가지는 행동 선택\n",
        "    action = np.argmax(Qtable[state][:])\n",
        "    state, reward, terminated, truncated, info = env.step(action)  # 녹화 로직을 위해 상태는 그대로 둡니다\n",
        "    img = env.render()\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4mdUTKkGnUd"
      },
      "outputs": [],
      "source": [
        "def push_to_hub(\n",
        "    repo_id, model, env, video_fps=1, local_repo_path=\"hub\"\n",
        "):\n",
        "    \"\"\"\n",
        "    평가, 비디오 생성 및 허깅 페이스 허브에 모델 업로드.\n",
        "    이 메서드는 전체 파이프라인을 수행합니다:\n",
        "    - 모델을 평가합니다.\n",
        "    - 모델 카드를 생성합니다.\n",
        "    - 에이전트의 재생 비디오를 생성합니다.\n",
        "    - 모든 것을 허브에 푸시합니다.\n",
        "\n",
        "    :param 레포_id: 허깅 페이스 허브의 모델 레포지토리 ID\n",
        "    :param 환경\n",
        "    :param 비디오_fps: 비디오 재생을 기록할 초당 프레임 수\n",
        "    (taxi-v3 및 frozenlake-v1에서는 1을 사용합니다)\n",
        "    :param 로컬_레포_경로: 로컬 레포지토리가 있는 경로\n",
        "    \"\"\"\n",
        "    _, repo_name = repo_id.split(\"/\")\n",
        "\n",
        "    eval_env = env\n",
        "    api = HfApi()\n",
        "\n",
        "    # Step 1: 레포지토리 생성\n",
        "    repo_url = api.create_repo(\n",
        "        repo_id=repo_id,\n",
        "        exist_ok=True,\n",
        "    )\n",
        "\n",
        "    # Step 2: 파일 다운로드\n",
        "    repo_local_path = Path(snapshot_download(repo_id=repo_id))\n",
        "\n",
        "    # Step 3: 모델 저장\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        model[\"map_name\"] = env.spec.kwargs.get(\"map_name\")\n",
        "        if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "            model[\"slippery\"] = False\n",
        "\n",
        "    # 모델 피클링\n",
        "    with open((repo_local_path) / \"q-learning.pkl\", \"wb\") as f:\n",
        "        pickle.dump(model, f)\n",
        "\n",
        "    # Step 4: 모델 평가 및 평가 메트릭 포함한 JSON 생성\n",
        "    mean_reward, std_reward = evaluate_agent(\n",
        "        eval_env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"]\n",
        "    )\n",
        "\n",
        "    evaluate_data = {\n",
        "        \"env_id\": model[\"env_id\"],\n",
        "        \"mean_reward\": mean_reward,\n",
        "        \"n_eval_episodes\": model[\"n_eval_episodes\"],\n",
        "        \"eval_datetime\": datetime.datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "    # 평가 결과를 포함한 \"results.json\" 파일 생성\n",
        "    with open(repo_local_path / \"results.json\", \"w\") as outfile:\n",
        "        json.dump(evaluate_data, outfile)\n",
        "\n",
        "    # Step 5: 모델 카드 생성\n",
        "    env_name = model[\"env_id\"]\n",
        "    if env.spec.kwargs.get(\"map_name\"):\n",
        "        env_name += \"-\" + env.spec.kwargs.get(\"map_name\")\n",
        "\n",
        "    if env.spec.kwargs.get(\"is_slippery\", \"\") == False:\n",
        "        env_name += \"-\" + \"no_slippery\"\n",
        "\n",
        "    metadata = {}\n",
        "    metadata[\"tags\"] = [env_name, \"q-learning\", \"reinforcement-learning\", \"custom-implementation\"]\n",
        "\n",
        "    # 메트릭 추가\n",
        "    eval = metadata_eval_result(\n",
        "        model_pretty_name=repo_name,\n",
        "        task_pretty_name=\"reinforcement-learning\",\n",
        "        task_id=\"reinforcement-learning\",\n",
        "        metrics_pretty_name=\"mean_reward\",\n",
        "        metrics_id=\"mean_reward\",\n",
        "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
        "        dataset_pretty_name=env_name,\n",
        "        dataset_id=env_name,\n",
        "    )\n",
        "\n",
        "    # 두 딕셔너리 병합\n",
        "    metadata = {**metadata, **eval}\n",
        "\n",
        "    model_card = f\"\"\"\n",
        "  # **Q-Learning** Agent playing1 **{env_id}**\n",
        "  This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
        "\n",
        "  ## Usage\n",
        "\n",
        "  ```python\n",
        "\n",
        "  model = load_from_hub(repo_id=\"{repo_id}\", filename=\"q-learning.pkl\")\n",
        "\n",
        "  # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n",
        "  env = gym.make(model[\"env_id\"])\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "    evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])\n",
        "\n",
        "    readme_path = repo_local_path / \"README.md\"\n",
        "    readme = \"\"\n",
        "    print(readme_path.exists())\n",
        "    if readme_path.exists():\n",
        "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
        "            readme = f.read()\n",
        "    else:\n",
        "        readme = model_card\n",
        "\n",
        "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    # Readme 메타데이터에 메트릭 저장\n",
        "    metadata_save(readme_path, metadata)\n",
        "\n",
        "    # Step 6: 비디오 녹화\n",
        "    video_path = repo_local_path / \"replay.mp4\"\n",
        "    record_video(env, model[\"qtable\"], video_path, video_fps)\n",
        "\n",
        "    # Step 7. 모든 것을 Hub에 푸시\n",
        "    api.upload_folder(\n",
        "        repo_id=repo_id,\n",
        "        folder_path=repo_local_path,\n",
        "        path_in_repo=\".\",\n",
        "    )\n",
        "\n",
        "    print(\"모델이 Hub에 업로드되었습니다. 모델을 여기서 확인할 수 있습니다: \", repo_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81J6cet_ogSS"
      },
      "source": [
        "### .\n",
        "\n",
        "`push_to_hub`를 사용하면 **에이전트를 평가하고, 재생 영상을 기록하고, 모델 카드를 생성하여 Hub에 푸시할 수 있습니다**.\n",
        "\n",
        "이 방식으로:\n",
        "- **우리의 작업을 보여줄 수 있습니다** 🔥\n",
        "- **에이전트가 플레이하는 모습을 시각화할 수 있습니다** 👀\n",
        "- **다른 사람들이 사용할 수 있는 에이전트를 커뮤니티와 공유할 수 있습니다** 💾\n",
        "- **리더보드를 통해 에이전트의 성능을 확인할 수 있습니다 🏆** 👉 [Deep Reinforcement Learning Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWnFC0iZooTw"
      },
      "source": [
        "모델을 커뮤니티와 공유하려면 세 가지 단계를 더 따라야 합니다:\n",
        "\n",
        "1️⃣ (아직 하지 않았다면) Hugging Face에 계정을 생성하세요 ➡ [https://huggingface.co/join](https://huggingface.co/join)\n",
        "\n",
        "2️⃣ 로그인 후, Hugging Face 웹사이트에서 인증 토큰을 저장해야 합니다.\n",
        "- 새로운 토큰을 생성하려면 [여기](https://huggingface.co/settings/tokens)에서 **쓰기 역할**로 생성하세요.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB5nIcxR8paT"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyWc1x3-o3xG"
      },
      "source": [
        "Google Colab이나 Jupyter Notebook을 사용하지 않는 경우, 아래 명령어를 대신 사용해야 합니다: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc5AfUeFo3xH"
      },
      "source": [
        "3️⃣ 이제 `push_to_hub()` 함수를 사용하여 훈련된 에이전트를 🤗 Hub에 푸시할 준비가 되었습니다 🔥\n",
        "\n",
        "- **모델 딕셔너리**를 생성하여 하이퍼파라미터와 Q_table을 포함시켜야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FiMqxqVHg0I4"
      },
      "outputs": [],
      "source": [
        "model = {\n",
        "    \"env_id\": env_id,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"n_training_episodes\": n_training_episodes,\n",
        "    \"n_eval_episodes\": n_eval_episodes,\n",
        "    \"eval_seed\": eval_seed,\n",
        "\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "\n",
        "    \"max_epsilon\": max_epsilon,\n",
        "    \"min_epsilon\": min_epsilon,\n",
        "    \"decay_rate\": decay_rate,\n",
        "\n",
        "    \"qtable\": Qtable_frozenlake\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kld-AEso3xH"
      },
      "source": [
        "`push_to_hub` 함수를 다음과 같이 채워봅시다:\n",
        "\n",
        "- `repo_id`: 생성되거나 업데이트될 Hugging Face Hub 저장소의 이름  \n",
        "(`repo_id = {username}/{repo_name}`)  \n",
        "💡 좋은 `repo_id` 예시: `{username}/q-{env_id}`\n",
        "- `model`: 하이퍼파라미터와 Qtable을 포함한 모델 딕셔너리\n",
        "- `env`: 환경\n",
        "- `commit_message`: 커밋 메시지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sBo2umnXpPd"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpOTtSt83kPZ"
      },
      "outputs": [],
      "source": [
        "username = \"\" # 이것을 채우세요\n",
        "repo_name = \"q-FrozenLake-v1-4x4-noSlippery\"\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2875IGsprzq"
      },
      "source": [
        "축하합니다 🥳 처음부터 직접 구현하고, 학습시키고, 첫 번째 강화학습 에이전트를 업로드했습니다.  \n",
        "`FrozenLake-v1 no_slippery`는 매우 간단한 환경이었으니, 이제 더 어려운 환경에 도전해봅시다 🔥."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18lN8Bz7yvLt"
      },
      "source": [
        "# Part 2: Taxi-v3 🚖\n",
        "\n",
        "## `Taxi-v3 🚕` 만들고 이해하기  \n",
        "---\n",
        "\n",
        "💡 환경을 처음 사용할 때는 문서를 확인하는 습관을 들이세요\n",
        "\n",
        "👉 https://gymnasium.farama.org/environments/toy_text/taxi/\n",
        "\n",
        "---\n",
        "\n",
        "`Taxi-v3` 🚕에서는 격자 세계(grid world)에 R(빨강), G(초록), Y(노랑), B(파랑) 네 개의 지정된 위치가 있습니다.\n",
        "\n",
        "에피소드가 시작되면 **택시는 무작위 칸에서 시작**하고, 승객도 무작위 위치에 있습니다.  \n",
        "택시는 승객 위치로 이동하여 **승객을 태운 뒤**, 승객의 목적지(지정된 네 위치 중 하나)로 이동하고 **승객을 하차**시킵니다.  \n",
        "승객이 하차되면 에피소드가 종료됩니다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi.png\" alt=\"Taxi\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "gL0wpeO8gpej"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBOaXgtsrmtT"
      },
      "source": [
        "**500개의 이산 상태**가 있습니다. 이는 **택시 위치 25가지, 승객의 가능한 위치 5가지** (승객이 택시 안에 있는 경우 포함), 그리고 **목적지 위치 4가지**가 있기 때문입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "_TPNaGSZrgqA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are  500  possible states\n"
          ]
        }
      ],
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CdeeZuokrhit"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are  6  possible actions\n"
          ]
        }
      ],
      "source": [
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1r50Advrh5Q"
      },
      "source": [
        "행동 공간 (에이전트가 취할 수 있는 가능한 행동들의 집합)은 **6개의 이용 가능한 행동 🎮**으로 이루어진 이산적 공간입니다:\n",
        "\n",
        "- 0: 남쪽으로 이동\n",
        "- 1: 북쪽으로 이동\n",
        "- 2: 동쪽으로 이동\n",
        "- 3: 서쪽으로 이동\n",
        "- 4: 승객 태우기\n",
        "- 5: 승객 내려주기\n",
        "\n",
        "보상 함수 💰:\n",
        "\n",
        "- 다른 보상이 트리거되지 않는 한, 단계당 -1.\n",
        "- 승객 전달 시 +20.\n",
        "- “승객 태우기” 및 “승객 내려주기” 행동을 불법적으로 실행 시 -10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "US3yDXnEtY9I"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "Q-table shape:  (500, 6)\n"
          ]
        }
      ],
      "source": [
        "# state_size 행과 action_size 열(500x6)을 갖는 Q 테이블을 생성합니다.\n",
        "Qtable_taxi = initialize_q_table(state_space, action_space)\n",
        "print(Qtable_taxi)\n",
        "print(\"Q-table shape: \", Qtable_taxi .shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUMKPH0_LJyH"
      },
      "source": [
        "## 하이퍼파라미터 정의 ⚙️\n",
        "\n",
        "⚠ EVAL_SEED를 수정하지 마세요: eval_seed 배열은 **모든 학우에게 동일한 택시 시작 위치로 에이전트를 평가할 수 있도록 합니다**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "AB6n__hhg7YS"
      },
      "outputs": [],
      "source": [
        "# 훈련 파라미터\n",
        "n_training_episodes = 25000   # 총 훈련 에피소드 수\n",
        "learning_rate = 0.7           # 학습률\n",
        "\n",
        "# 평가 파라미터\n",
        "n_eval_episodes = 100         # 총 테스트 에피소드 수\n",
        "\n",
        "# EVAL_SEED 수정 금지\n",
        "eval_seed = [16,54,165,177,191,191,120,80,149,178,48,38,6,125,174,73,50,172,100,148,146,6,25,40,68,148,49,167,9,97,164,176,61,7,54,55,\n",
        "  161,131,184,51,170,12,120,113,95,126,51,98,36,135,54,82,45,95,89,59,95,124,9,113,58,85,51,134,121,169,105,21,30,11,50,65,12,43,82,145,152,97,106,55,31,85,38,\n",
        "  112,102,168,123,97,21,83,158,26,80,63,5,81,32,11,28,148] # 평가 시드, 이는 모든 학우의 에이전트가 동일한 택시 시작 위치에서 훈련되도록 보장합니다\n",
        "                                                         # 각 시드는 특정 시작 상태를 가집니다\n",
        "\n",
        "# 환경 파라미터\n",
        "env_id = \"Taxi-v3\"            # 환경 이름\n",
        "max_steps = 99                # 에피소드당 최대 스텝 수\n",
        "gamma = 0.95                  # 감가율 (할인율)\n",
        "\n",
        "# 탐험 파라미터\n",
        "max_epsilon = 1.0             # 시작 시 탐험 확률\n",
        "min_epsilon = 0.05            # 최소 탐험 확률\n",
        "decay_rate = 0.005            # 탐험 확률의 지수 감소율"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TMORo1VLTsX"
      },
      "source": [
        "## Q-Learning 에이전트 훈련시키기 🏃"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WwP3Y2z2eS-K"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "834a957357d1408fb74bf831ce046171",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "array([[  0.        ,   0.        ,   0.        ,   0.        ,\n",
              "          0.        ,   0.        ],\n",
              "       [  2.75200369,   3.94947757,   2.75200369,   3.94947757,\n",
              "          5.20997639,  -5.05052243],\n",
              "       [  7.93349184,   9.40344144,   7.93349184,   9.40367562,\n",
              "         10.9512375 ,   0.40367548],\n",
              "       ...,\n",
              "       [ -3.09408412,  12.58025   ,  -2.9070066 ,  -3.8221694 ,\n",
              "         -2.2027986 , -10.62089056],\n",
              "       [ -3.88608682,   6.53681724,  -3.95785867,  -4.13398518,\n",
              "        -12.16948305,  -5.57355163],\n",
              "       [ -1.3755    ,  -1.3755    ,  15.44934586,  18.        ,\n",
              "          5.59967048,   6.93507486]])"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)\n",
        "Qtable_taxi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPdu0SueLVl2"
      },
      "source": [
        "## 모델 딕셔너리 생성 💾 및 훈련된 모델 허브에 게시 🔥\n",
        "\n",
        "- 재현성을 위한 모든 훈련 하이퍼파라미터와 Q-테이블을 포함할 모델 딕셔너리를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "0a1FpE_3hNYr"
      },
      "outputs": [],
      "source": [
        "model = {\n",
        "    \"env_id\": env_id,\n",
        "    \"max_steps\": max_steps,\n",
        "    \"n_training_episodes\": n_training_episodes,\n",
        "    \"n_eval_episodes\": n_eval_episodes,\n",
        "    \"eval_seed\": eval_seed,\n",
        "\n",
        "    \"learning_rate\": learning_rate,\n",
        "    \"gamma\": gamma,\n",
        "\n",
        "    \"max_epsilon\": max_epsilon,\n",
        "    \"min_epsilon\": min_epsilon,\n",
        "    \"decay_rate\": decay_rate,\n",
        "\n",
        "    \"qtable\": Qtable_taxi\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dhQtiQozhOn1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84a9eb9a77494526b6226c961a97e7a8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f000ee6a1daa466e94d9161a73e95ccc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcdc712ad6d54d8dbaa85531aa210946",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c85f473d943b4f54ae3f986ec193e8bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (550, 350) to (560, 352) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ac22a7f07b84a13800c2a94b5fec1eb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "replay.mp4:   0%|          | 0.00/117k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d63e1f19c61f48e5918b5156a8139dcb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "q-learning.pkl:   0%|          | 0.00/24.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ed5907e11e2e4fdea57cf6a1ef950778",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "모델이 Hub에 업로드되었습니다. 모델을 여기서 확인할 수 있습니다:  https://huggingface.co/Yeongi/q-Taxi-v3\n"
          ]
        }
      ],
      "source": [
        "username = \"\" # 이것을 채우세요\n",
        "repo_name = \"\" # 이것을 채우세요\n",
        "push_to_hub(\n",
        "    repo_id=f\"{username}/{repo_name}\",\n",
        "    model=model,\n",
        "    env=env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgSdjgbIpRti"
      },
      "source": [
        "이제 모델이 Hugging Face Hub에 업로드되었으니, 리더보드를 통해 여러분의 `Taxi-v3` 결과를 다른 학습자들과 비교해볼 수 있습니다 🏆 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/taxi-leaderboard.png\" alt=\"Taxi Leaderboard\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzgIO70c0bu2"
      },
      "source": [
        "# Part 3: 허브에서 불러오기 🔽\n",
        "\n",
        "Hugging Face Hub 🤗의 멋진 점은 커뮤니티에서 공유한 강력한 모델들을 아주 쉽게 불러올 수 있다는 것입니다.\n",
        "\n",
        "Hub에서 저장된 모델을 불러오는 방법은 매우 간단합니다:\n",
        "\n",
        "1. https://huggingface.co/models?other=q-learning 에 접속해 q-learning으로 저장된 모든 모델 리스트를 확인합니다.  \n",
        "2. 원하는 모델을 선택하고 `repo_id`를 복사합니다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit2/copy-id.png\" alt=\"Copy id\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTth6thRoC6X"
      },
      "source": [
        "3. 그런 다음 `load_from_hub` 함수를 사용할 때는 다음 두 가지를 지정해주면 됩니다:\n",
        "- `repo_id`: 복사한 저장소 ID  \n",
        "- `filename`: 해당 저장소 안에 저장된 모델 파일 이름"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtrfoTaBoNrd"
      },
      "source": [
        "#### 이 코드를 수정하지 마세요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Eo8qEzNtCaVI"
      },
      "outputs": [],
      "source": [
        "from urllib.error import HTTPError\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "\n",
        "def load_from_hub(repo_id: str, filename: str) -> str:\n",
        "    \"\"\"\n",
        "    Hugging Face Hub에서 모델을 다운로드합니다.\n",
        "    :param repo_id: Hugging Face Hub에서 모델 저장소의 ID\n",
        "    :param filename: 저장소에서 모델 zip 파일 이름\n",
        "    \"\"\"\n",
        "    # Hub에서 모델을 가져오고, 로컬 디스크에 다운로드 및 캐시합니다\n",
        "    pickle_model = hf_hub_download(\n",
        "        repo_id=repo_id,\n",
        "        filename=filename\n",
        "    )\n",
        "\n",
        "    with open(pickle_model, 'rb') as f:\n",
        "        downloaded_model_file = pickle.load(f)\n",
        "\n",
        "    return downloaded_model_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_sM2gNioPZH"
      },
      "source": [
        "### ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "JUm9lz2gCQcU"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b58d8892ee24ad4b005fd86f4153474",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "q-learning.pkl:   0%|          | 0.00/24.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'env_id': 'Taxi-v3', 'max_steps': 99, 'n_training_episodes': 25000, 'n_eval_episodes': 100, 'eval_seed': [16, 54, 165, 177, 191, 191, 120, 80, 149, 178, 48, 38, 6, 125, 174, 73, 50, 172, 100, 148, 146, 6, 25, 40, 68, 148, 49, 167, 9, 97, 164, 176, 61, 7, 54, 55, 161, 131, 184, 51, 170, 12, 120, 113, 95, 126, 51, 98, 36, 135, 54, 82, 45, 95, 89, 59, 95, 124, 9, 113, 58, 85, 51, 134, 121, 169, 105, 21, 30, 11, 50, 65, 12, 43, 82, 145, 152, 97, 106, 55, 31, 85, 38, 112, 102, 168, 123, 97, 21, 83, 158, 26, 80, 63, 5, 81, 32, 11, 28, 148], 'learning_rate': 0.7, 'gamma': 0.95, 'epsilon': 1.0, 'max_epsilon': 1.0, 'min_epsilon': 0.05, 'decay_rate': 0.005, 'qtable': array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
            "         0.        ],\n",
            "       [ 2.75200369,  3.94947757,  2.75200369,  3.94947757,  5.20997639,\n",
            "        -5.05052243],\n",
            "       [ 7.93349184,  9.40367562,  7.93349184,  9.40367562, 10.9512375 ,\n",
            "         0.40367562],\n",
            "       ...,\n",
            "       [10.9512375 , 12.58025   , 10.9512375 ,  9.40367562,  1.9512375 ,\n",
            "         1.9512375 ],\n",
            "       [ 5.20997639,  6.53681725,  5.20997639,  6.53681725, -3.79002361,\n",
            "        -3.79002361],\n",
            "       [16.1       , 14.295     , 16.1       , 18.        ,  7.1       ,\n",
            "         7.1       ]])}\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "846c7c39ac834f3bbb055387902a5a91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(7.56, 2.706732347314747)"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = load_from_hub(repo_id=\"ThomasSimonini/q-Taxi-v3\", filename=\"q-learning.pkl\") # 다른 모델을 사용해 보세요\n",
        "\n",
        "print(model)\n",
        "env = gym.make(model[\"env_id\"])\n",
        "\n",
        "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "O7pL8rg1MulN"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "caf84f19bd444be5ace49ffe3575ba18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "q-learning.pkl:   0%|          | 0.00/933 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93a81007341c476a8413509e6ff5ae3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "(1.0, 0.0)"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = load_from_hub(repo_id=\"ThomasSimonini/q-FrozenLake-v1-no-slippery\", filename=\"q-learning.pkl\") # 다른 모델을 사용해 보세요\n",
        "\n",
        "env = gym.make(model[\"env_id\"], is_slippery=False)\n",
        "\n",
        "evaluate_agent(env, model[\"max_steps\"], model[\"n_eval_episodes\"], model[\"qtable\"], model[\"eval_seed\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQAwLnYFPk-s"
      },
      "source": [
        "## 추가 도전 과제들 🏆\n",
        "\n",
        "**가장 좋은 학습 방법은 직접 해보는 것!**  \n",
        "지금까지 본 것처럼, 현재의 에이전트는 아주 뛰어난 성능을 보이지는 않아요.  \n",
        "첫 번째 제안은 **학습 횟수를 늘리는 것**입니다. 예를 들어 1,000,000 스텝으로 학습한 결과는 꽤 괜찮았습니다!\n",
        "\n",
        "[리더보드](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)에서 당신의 에이전트를 확인해보세요.  \n",
        "**정상에 오를 수 있을까요?**\n",
        "\n",
        "리더보드 순위를 올리기 위한 몇 가지 아이디어:\n",
        "\n",
        "- 더 많은 스텝으로 학습하기  \n",
        "- 클래스메이트들이 사용한 **하이퍼파라미터**를 참고해 다른 설정 시도해보기  \n",
        "- **새롭게 학습한 모델을 Hub에 업로드**하기 🔥  \n",
        "\n",
        "빙판 위를 걷거나 택시 운전이 너무 지루하다면?  \n",
        "**환경을 바꿔보세요!**  \n",
        "예를 들어 `FrozenLake-v1`의 미끄러운(slippery) 버전은 어떤가요?  \n",
        "[Gymnasium 문서](https://gymnasium.farama.org/)를 참고해서 작동 방식도 확인하고, 재미있게 실험해보세요 🎉"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-fW-EU5WejJ"
      },
      "source": [
        "_____________________________________________________________________\n",
        "\n",
        "축하합니다 🥳 처음으로 강화학습 에이전트를 직접 구현하고, 학습시키고, 업로드까지 완료했습니다!\n",
        "\n",
        "Q-Learning을 이해하는 것은 **가치 기반(value-based) 방법론을 이해하는 데 있어 매우 중요한 단계**입니다.\n",
        "\n",
        "다음 유닛에서는 Deep Q-Learning을 다루게 됩니다.  \n",
        "Q-테이블을 만들고 업데이트하는 방식은 좋은 전략이지만 — **확장성이 떨어진다는 단점이 있습니다.**\n",
        "\n",
        "예를 들어, Doom 게임을 학습하는 에이전트를 만든다고 상상해보세요.\n",
        "\n",
        "<img src=\"https://vizdoom.cs.put.edu.pl/user/pages/01.tutorial/basic.png\" alt=\"Doom\"/>\n",
        "\n",
        "Doom 같은 환경은 상태 공간이 매우 크고, 수백만 개의 상태가 존재합니다.  \n",
        "이런 환경에서는 Q-테이블을 생성하고 유지하는 것이 비효율적입니다.\n",
        "\n",
        "그래서 다음 유닛에서는 Deep Q-Learning을 배웁니다.  \n",
        "Deep Q-Learning은 **신경망을 사용하여, 주어진 상태에서 가능한 행동들 각각의 Q값을 근사**하는 알고리즘입니다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjLhT70TEZIn"
      },
      "source": [
        "Unit 3에서 만나요! 🔥  \n",
        "\n",
        "## 계속 배우고, 멋지게 나아가요 🤗"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "67OdoKL63eDD",
        "B2_-8b8z5k54",
        "8R5ej1fS4P2V",
        "Pnpk2ePoem3r"
      ],
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "learn2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
