{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pakyeon/deep-rl-class-ko/blob/main/notebooks/ko/unit4/unit4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjRWziAVU2lZ"
      },
      "source": [
        "# Unit 4: PyTorchë¡œ ì²˜ìŒìœ¼ë¡œ ë”¥ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì¸ Reinforce êµ¬í˜„í•˜ê¸°. ê·¸ë¦¬ê³  ê·¸ ê°•ì¸í•¨ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ğŸ’ª\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/thumbnail.png\" alt=\"thumbnail\"/>\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” ì²˜ìŒìœ¼ë¡œ ë”¥ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì¸ Reinforce(ëª¬í…Œì¹´ë¥¼ë¡œ ì •ì±… ê²½ì‚¬ë¼ê³ ë„ ë¶ˆë¦¼)ë¥¼ ì²˜ìŒë¶€í„° ì§ì ‘ êµ¬í˜„í•˜ê²Œ ë©ë‹ˆë‹¤.\n",
        "\n",
        "ReinforceëŠ” *ì •ì±… ê¸°ë°˜ ë°©ë²•(Policy-based method)*ì…ë‹ˆë‹¤: í–‰ë™ ê°€ì¹˜ í•¨ìˆ˜ ì—†ì´ **ì •ì±…ì„ ì§ì ‘ ìµœì í™”**í•˜ë ¤ëŠ” ë”¥ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.\n",
        "\n",
        "ì •í™•íˆ ë§í•˜ìë©´, ReinforceëŠ” *ì •ì±… ê²½ì‚¬ ë°©ë²•(Policy-gradient method)*ì´ë©°, ì´ëŠ” *ì •ì±… ê¸°ë°˜ ë°©ë²•*ì˜ í•˜ìœ„ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. ì´ëŠ” **ê²½ì‚¬ ìƒìŠ¹ë²•ì„ ì‚¬ìš©í•´ ìµœì ì˜ ì •ì±… ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì •í•¨ìœ¼ë¡œì¨ ì •ì±…ì„ ì§ì ‘ ìµœì í™”**í•˜ë ¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n",
        "\n",
        "ê°•ì¸í•¨ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ìœ„í•´, ìš°ë¦¬ëŠ” ì´ ì•Œê³ ë¦¬ì¦˜ì„ ë‘ ê°€ì§€ ê°„ë‹¨í•œ í™˜ê²½ì—ì„œ í•™ìŠµì‹œí‚¬ ê²ƒì…ë‹ˆë‹¤:\n",
        "- Cartpole-v1  \n",
        "- PixelcopterEnv\n",
        "\n",
        "â¬‡ï¸ **ì´ ë…¸íŠ¸ë¶ì˜ ë§ˆì§€ë§‰ì— ì—¬ëŸ¬ë¶„ì´ ë‹¬ì„±í•˜ê²Œ ë  ì˜ˆì‹œì…ë‹ˆë‹¤.** â¬‡ï¸"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4rBom2sbo7S"
      },
      "source": [
        "  <img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/envs.gif\" alt=\"Environments\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPLwsPajb1f8"
      },
      "source": [
        "### ğŸ® í™˜ê²½:\n",
        "\n",
        "- [CartPole-v1](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)\n",
        "- [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n",
        "\n",
        "### ğŸ“š ê°•í™”í•™ìŠµ ë¼ì´ë¸ŒëŸ¬ë¦¬:\n",
        "\n",
        "- Python  \n",
        "- PyTorch\n",
        "\n",
        "íŠœí† ë¦¬ì–¼ì„ ì§€ì†ì ìœ¼ë¡œ ê°œì„ í•˜ê³  ìˆìœ¼ë‹ˆ, **ì´ ë…¸íŠ¸ë¶ì—ì„œ ë¬¸ì œê°€ ë°œê²¬ë˜ë©´** [GitHub ì €ì¥ì†Œì— ì´ìŠˆë¥¼ ë“±ë¡í•´ ì£¼ì„¸ìš”](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_WSo0VUV99t"
      },
      "source": [
        "## ì´ ë…¸íŠ¸ë¶ì˜ ëª©í‘œ ğŸ†\n",
        "\n",
        "ì´ ë…¸íŠ¸ë¶ì„ ë§ˆì¹˜ë©´ ì—¬ëŸ¬ë¶„ì€:\n",
        "\n",
        "- **PyTorchë¥¼ ì‚¬ìš©í•´ Reinforce ì•Œê³ ë¦¬ì¦˜ì„ ì²˜ìŒë¶€í„° ì§ì ‘ êµ¬í˜„**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
        "- **ê°„ë‹¨í•œ í™˜ê²½ì„ í†µí•´ ì—ì´ì „íŠ¸ì˜ ê°•ì¸í•¨ì„ í…ŒìŠ¤íŠ¸**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
        "- **í›ˆë ¨ëœ ì—ì´ì „íŠ¸ë¥¼ Hubì— ì—…ë¡œë“œí•˜ê³ **, ì˜ìƒ ë¦¬í”Œë ˆì´ì™€ í‰ê°€ ì ìˆ˜ì™€ í•¨ê»˜ ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ”¥."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEPrZg2eWa4R"
      },
      "source": [
        "## ì´ ë…¸íŠ¸ë¶ì€ ë”¥ ê°•í™”í•™ìŠµ ê°•ì˜ì˜ ì¼ë¶€ì…ë‹ˆë‹¤\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p5HnEefISCB"
      },
      "source": [
        "ì´ ë¬´ë£Œ ê°•ì˜ì—ì„œëŠ” ë‹¤ìŒì„ ë°°ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n",
        "\n",
        "- ğŸ“– **ì´ë¡ ê³¼ ì‹¤ìŠµ**ì„ í†µí•´ ë”¥ ê°•í™”í•™ìŠµ(Deep Reinforcement Learning)ì„ í•™ìŠµí•©ë‹ˆë‹¤.  \n",
        "- ğŸ§‘â€ğŸ’» Stable Baselines3, RL Baselines3 Zoo, CleanRL, Sample Factory 2.0 ê°™ì€ **ìœ ëª…í•œ Deep RL ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©ë²•**ì„ ìµí™ë‹ˆë‹¤.  \n",
        "- ğŸ¤– **ë‹¤ì–‘í•œ í™˜ê²½ì—ì„œ ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµ**ì‹œì¼œ ë´…ë‹ˆë‹¤.  \n",
        "\n",
        "ê·¸ë¦¬ê³  ë” ë§ì€ ë‚´ìš©ì„ ì›í•œë‹¤ë©´ ğŸ“š **ì „ì²´ ì»¤ë¦¬í˜ëŸ¼ì€ ì—¬ê¸°ì„œ í™•ì¸í•˜ì„¸ìš”** ğŸ‘‰ https://simoninithomas.github.io/deep-rl-course\n",
        "\n",
        "ë˜í•œ, ë§¤ ìœ ë‹›ì´ ê³µê°œë  ë•Œë§ˆë‹¤ ë§í¬ì™€ ë„ì „ ê³¼ì œ, ì—…ë°ì´íŠ¸ ë‚´ìš©ì„ ë°›ì•„ë³´ë ¤ë©´  \n",
        "**[ìˆ˜ê°• ì‹ ì²­](http://eepurl.com/ic5ZUD)** ì„ ê¼­ í•´ì£¼ì„¸ìš”!  \n",
        "(ì´ë©”ì¼ì„ í†µí•´ ì •ë³´ë¥¼ ë³´ë‚´ë“œë¦¬ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.)\n",
        "\n",
        "ì»¤ë®¤ë‹ˆí‹° ë° ê°•ì‚¬ì§„ê³¼ì˜ êµë¥˜ë¥¼ ì›í•œë‹¤ë©´  \n",
        "**ë””ìŠ¤ì½”ë“œ ì„œë²„ì— ì°¸ì—¬í•˜ì„¸ìš”** ğŸ‘‰ğŸ» https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjY-eq3eWh9O"
      },
      "source": [
        "## ì‚¬ì „ ì¤€ë¹„ ì‚¬í•­ ğŸ—ï¸  \n",
        "ë…¸íŠ¸ë¶ì„ ì‹œì‘í•˜ê¸° ì „ì— ë‹¤ìŒì„ ì™„ë£Œí•´ì•¼ í•©ë‹ˆë‹¤:\n",
        "\n",
        "ğŸ”² ğŸ“š [Unit 4ë¥¼ ì½ê³  Policy Gradientì— ëŒ€í•´ í•™ìŠµí•˜ê¸°](https://huggingface.co/deep-rl-course/unit4/introduction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsh4ZAamchSl"
      },
      "source": [
        "# Reinforce ì•Œê³ ë¦¬ì¦˜ì„ ì²˜ìŒë¶€í„° ì§ì ‘ êµ¬í˜„í•´ë³´ì ğŸ”¥\n",
        "\n",
        "ì´ ì‹¤ìŠµì„ ì¸ì¦ ê³¼ì •ì— ì œì¶œí•˜ë ¤ë©´, í›ˆë ¨í•œ ëª¨ë¸ì„ Hubì— ì—…ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "\n",
        "- `Cartpole-v1`ì—ì„œ **ê²°ê³¼ê°€ 350 ì´ìƒ**ì¼ ê²ƒ  \n",
        "- `PixelCopter`ì—ì„œ **ê²°ê³¼ê°€ 5 ì´ìƒ**ì¼ ê²ƒ\n",
        "\n",
        "ê²°ê³¼ë¥¼ í™•ì¸í•˜ë ¤ë©´ ë¦¬ë”ë³´ë“œì— ë“¤ì–´ê°€ì„œ ìì‹ ì˜ ëª¨ë¸ì„ ì°¾ìœ¼ì„¸ìš”.  \n",
        "**ê²°ê³¼ = í‰ê·  ë³´ìƒ - ë³´ìƒ í‘œì¤€í¸ì°¨(mean_reward - std of reward)** ì…ë‹ˆë‹¤.  \n",
        "**ë¦¬ë”ë³´ë“œì— ìì‹ ì˜ ëª¨ë¸ì´ ë³´ì´ì§€ ì•Šìœ¼ë©´, í˜ì´ì§€ í•˜ë‹¨ìœ¼ë¡œ ê°€ì„œ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.**\n",
        "\n",
        "ì¸ì¦ ê³¼ì •ì— ëŒ€í•œ ìì„¸í•œ ì •ë³´ëŠ” ë‹¤ìŒ ë§í¬ë¥¼ ì°¸ê³ í•˜ì„¸ìš” ğŸ‘‰ https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoTC9o2SczNn"
      },
      "source": [
        "## íŒ ğŸ’¡  \n",
        "Colabì—ì„œ ì´ ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•  ë•ŒëŠ” **Google Driveì— ë³µì‚¬ë³¸ì„ ë§Œë“¤ì–´ì„œ ì‹¤í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.**  \n",
        "ê·¸ë˜ì•¼ **ì„¸ì…˜ì´ ë§Œë£Œë˜ë”ë¼ë„** ë…¸íŠ¸ë¶ì´ Google Driveì— ì €ì¥ë˜ì–´ ë‹¤ì‹œ ì²˜ìŒë¶€í„° ì‘ì„±í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë³µì‚¬ ë°©ë²•:  \n",
        "`Ctrl + S` ë˜ëŠ” `íŒŒì¼ > Google ë“œë¼ì´ë¸Œì— ì‚¬ë³¸ ì €ì¥` ì„ í´ë¦­í•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU4FVzaoM6fC"
      },
      "source": [
        "## GPU ì„¤ì •í•˜ê¸° ğŸ’ª  \n",
        "- **ì—ì´ì „íŠ¸ì˜ í›ˆë ¨ì„ ê°€ì†í•˜ê¸° ìœ„í•´ GPUë¥¼ ì‚¬ìš©í•  ì˜ˆì •ì…ë‹ˆë‹¤**. ì´ë¥¼ ìœ„í•´ `ëŸ°íƒ€ì„ > ëŸ°íƒ€ì„ ìœ í˜• ë³€ê²½`ìœ¼ë¡œ ì´ë™í•˜ì„¸ìš”.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV0NyFdQM9ZG"
      },
      "source": [
        "- `í•˜ë“œì›¨ì–´ ê°€ì†ê¸° > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTpYcVZVMzUI"
      },
      "source": [
        "## ê°€ìƒ ë””ìŠ¤í”Œë ˆì´ ìƒì„± ğŸ”½\n",
        "\n",
        "ë…¸íŠ¸ë¶ì„ ì‹¤í–‰í•˜ëŠ” ë™ì•ˆ ë¦¬í”Œë ˆì´ ë¹„ë””ì˜¤ë¥¼ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì½”ë©ì—ì„œëŠ” **í™˜ê²½ì„ ë Œë”ë§(ê·¸ë¦¬ê³  í”„ë ˆì„ì„ ê¸°ë¡)í•  ìˆ˜ ìˆë„ë¡ ê°€ìƒ í™”ë©´ì´ í•„ìš”í•©ë‹ˆë‹¤.**\n",
        "\n",
        "ë”°ë¼ì„œ ë‹¤ìŒ ì…€ì€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•˜ê³  ê°€ìƒ í™”ë©´ ğŸ–¥ì„ ìƒì„± ë° ì‹¤í–‰í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip install pyvirtualdisplay\n",
        "!pip install pyglet==1.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sr-Nuyb1dBm0"
      },
      "outputs": [],
      "source": [
        "# ê°€ìƒ ë””ìŠ¤í”Œë ˆì´\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjrLfPFIW8XK"
      },
      "source": [
        "## ì˜ì¡´ì„± ì„¤ì¹˜ ğŸ”½\n",
        "\n",
        "ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” ì˜ì¡´ì„±ë“¤ì„ ì„¤ì¹˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤ìŒ íŒ¨í‚¤ì§€ë“¤ì„ ì„¤ì¹˜í•©ë‹ˆë‹¤:\n",
        "\n",
        "- `gym`  \n",
        "- `gym-games`: PyGameìœ¼ë¡œ ë§Œë“¤ì–´ì§„ ì¶”ê°€ Gym í™˜ê²½ë“¤  \n",
        "- `huggingface_hub`: ğŸ¤— ëª¨ë¸ê³¼ ë°ì´í„°ì…‹ì„ ê³µìœ í•˜ê³  íƒìƒ‰í•  ìˆ˜ ìˆëŠ” ì¤‘ì•™ í—ˆë¸Œ. ë²„ì „ ê´€ë¦¬, ë©”íŠ¸ë¦­, ì‹œê°í™” ë“± í˜‘ì—…ì— ìœ ìš©í•œ ê¸°ëŠ¥ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
        "\n",
        "â€œì™œ ìµœì‹  ë²„ì „ì¸ gymnasiumì´ ì•„ë‹ˆë¼ gymì„ ì„¤ì¹˜í•˜ë‚˜ìš”?â€ë¼ê³  ê¶ê¸ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  \n",
        "**ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” gym-gamesê°€ ì•„ì§ gymnasiumì— ë§ê²Œ ì—…ë°ì´íŠ¸ë˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.**\n",
        "\n",
        "ë‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì£¼ìš” ì°¨ì´ì :\n",
        "- `gym`ì—ì„œëŠ” `terminated`ì™€ `truncated`ê°€ ì—†ê³ , ëŒ€ì‹  `done`ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤.  \n",
        "- `env.step()`ì˜ ë°˜í™˜ê°’ì´ `state, reward, done, info` í˜•ì‹ì…ë‹ˆë‹¤.\n",
        "\n",
        "Gymê³¼ Gymnasiumì˜ ì°¨ì´ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì—¬ê¸°ë¥¼ ì°¸ê³ í•˜ì„¸ìš” ğŸ‘‰ https://gymnasium.farama.org/content/migration-guide/\n",
        "\n",
        "Reinforce ëª¨ë¸ ëª¨ìŒ ğŸ‘‰ https://huggingface.co/models?other=reinforce  \n",
        "ë”¥ ê°•í™”í•™ìŠµ ëª¨ë¸ ì „ì²´ ğŸ‘‰ https://huggingface.co/models?pipeline_tag=reinforcement-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8ZVi-uydpgL"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit4/requirements-unit4.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAHAq6RZW3rn"
      },
      "source": [
        "## íŒ¨í‚¤ì§€ ì„í¬íŠ¸ ğŸ“¦\n",
        "\n",
        "ì„¤ì¹˜í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì™¸ì—ë„, ë‹¤ìŒê³¼ ê°™ì€ íŒ¨í‚¤ì§€ë¥¼ í•¨ê»˜ ì„í¬íŠ¸í•©ë‹ˆë‹¤:\n",
        "\n",
        "- `imageio`: ë¦¬í”Œë ˆì´ ì˜ìƒì„ ìƒì„±í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V8oadoJSWp7C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gym\n",
        "import gym\n",
        "import gym_pygame\n",
        "\n",
        "# Hugging Face Hub\n",
        "from huggingface_hub import notebook_login # Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ì—¬ ëª¨ë¸ì„ Hubì— ì—…ë¡œë“œí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
        "import imageio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfxJYdMeeVgv"
      },
      "source": [
        "## GPUê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê¸°\n",
        "\n",
        "- GPUê°€ ìˆëŠ”ì§€ í™•ì¸í•´ë´…ì‹œë‹¤.\n",
        "- ë§Œì•½ ìˆë‹¤ë©´ `device:cuda0`ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaJu5FeZxXGY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5TNYa14aRav"
      },
      "outputs": [],
      "source": [
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBPecCtBL_pZ"
      },
      "source": [
        "ì´ì œ Reinforce ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤ ğŸ”¥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KEyKYo2ZSC-"
      },
      "source": [
        "# ì²« ë²ˆì§¸ ì—ì´ì „íŠ¸: CartPole-v1 ê²Œì„ í”Œë ˆì´ ğŸ¤–"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haLArKURMyuF"
      },
      "source": [
        "## CartPole í™˜ê²½ ìƒì„± ë° ì‘ë™ ë°©ì‹ ì´í•´í•˜ê¸°  \n",
        "### [í™˜ê²½ ğŸ®](https://www.gymlibrary.dev/environments/classic_control/cart_pole/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AH_TaLKFXo_8"
      },
      "source": [
        "### ì™œ CartPole-v1ì²˜ëŸ¼ ë‹¨ìˆœí•œ í™˜ê²½ì„ ì‚¬ìš©í•˜ëŠ”ê°€?\n",
        "[Reinforcement Learning Tips and Tricks](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)ì—ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼, ì—ì´ì „íŠ¸ë¥¼ ì²˜ìŒë¶€í„° ì§ì ‘ êµ¬í˜„í•  ë•ŒëŠ” **ê¹Šì´ ìˆëŠ” í™˜ê²½ìœ¼ë¡œ ë“¤ì–´ê°€ê¸° ì „ì— ê°„ë‹¨í•œ í™˜ê²½ì—ì„œ ì œëŒ€ë¡œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•˜ê³  ë²„ê·¸ë¥¼ ì°¾ì•„ì•¼ í•œë‹¤.** ê°„ë‹¨í•œ í™˜ê²½ì—ì„œëŠ” ë²„ê·¸ë¥¼ ì°¾ëŠ” ê²ƒì´ í›¨ì”¬ ì‰½ê¸° ë•Œë¬¸ì´ë‹¤.\n",
        "\n",
        "> ì¥ë‚œê° ë¬¸ì œì—ì„œ \"ìƒì¡´ ì‹ í˜¸\"ë¥¼ í™•ì¸í•´ë³´ë¼\n",
        "\n",
        "> êµ¬í˜„ì´ ì œëŒ€ë¡œ ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì ì  ë” ì–´ë ¤ìš´ í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ë³´ë¼ (ê²°ê³¼ëŠ” RL zooì™€ ë¹„êµ ê°€ëŠ¥). ì´ ë‹¨ê³„ì—ì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ í•„ìš”í•˜ë‹¤.\n",
        "\n",
        "___\n",
        "### CartPole-v1 í™˜ê²½\n",
        "\n",
        "> ë§‰ëŒ€ê°€ ì›€ì§ì´ì§€ ì•ŠëŠ” ê´€ì ˆë¡œ ì¹´íŠ¸ì— ì—°ê²°ë˜ì–´ ìˆê³ , ì¹´íŠ¸ëŠ” ë§ˆì°°ì´ ì—†ëŠ” íŠ¸ë™ ìœ„ì—ì„œ ì›€ì§ì¸ë‹¤. ì§„ì(ë§‰ëŒ€)ëŠ” ì¹´íŠ¸ ìœ„ì— ìˆ˜ì§ìœ¼ë¡œ ì„¸ì›Œì ¸ ìˆìœ¼ë©°, ëª©í‘œëŠ” ì¹´íŠ¸ë¥¼ ì™¼ìª½ ë˜ëŠ” ì˜¤ë¥¸ìª½ìœ¼ë¡œ ì›€ì§ì—¬ ë§‰ëŒ€ë¥¼ ê· í˜• ìƒíƒœë¡œ ìœ ì§€í•˜ëŠ” ê²ƒì´ë‹¤.\n",
        "\n",
        "ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” CartPole-v1ë¶€í„° ì‹œì‘í•œë‹¤. ëª©í‘œëŠ” **ë§‰ëŒ€ê°€ ê· í˜•ì„ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ ì¹´íŠ¸ë¥¼ ì™¼ìª½ ë˜ëŠ” ì˜¤ë¥¸ìª½ìœ¼ë¡œ ë°€ì–´ì£¼ëŠ” ê²ƒ**ì´ë‹¤.\n",
        "\n",
        "ì—í”¼ì†Œë“œëŠ” ë‹¤ìŒ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¼ë„ ì¶©ì¡±í•˜ë©´ ì¢…ë£Œëœë‹¤:\n",
        "- ë§‰ëŒ€ì˜ ê°ë„ê°€ Â±12Â°ë¥¼ ì´ˆê³¼í•  ë•Œ\n",
        "- ì¹´íŠ¸ì˜ ìœ„ì¹˜ê°€ Â±2.4ë¥¼ ì´ˆê³¼í•  ë•Œ\n",
        "- ì—í”¼ì†Œë“œ ê¸¸ì´ê°€ 500ì„ ì´ˆê³¼í•  ë•Œ\n",
        "\n",
        "ë§‰ëŒ€ê°€ ê· í˜•ì„ ìœ ì§€í•˜ëŠ” ë§¤ ì‹œê°„ë§ˆë‹¤ ë³´ìƒ ğŸ’° +1ì„ ì–»ëŠ”ë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POOOk15_K6KA"
      },
      "outputs": [],
      "source": [
        "env_id = \"CartPole-v1\"  \n",
        "# í™˜ê²½ ìƒì„±  \n",
        "env = gym.make(env_id)\n",
        "\n",
        "# í‰ê°€ìš© í™˜ê²½ ìƒì„±  \n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "# ìƒíƒœ ê³µê°„ê³¼ í–‰ë™ ê³µê°„ ê°€ì ¸ì˜¤ê¸°  \n",
        "s_size = env.observation_space.shape[0]  \n",
        "a_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FMLFrjiBNLYJ"
      },
      "outputs": [],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"ìƒíƒœ ê³µê°„ì˜ í¬ê¸°:\", s_size)\n",
        "print(\"ì„ì˜ì˜ ìƒíƒœ ìƒ˜í”Œ:\", env.observation_space.sample())  # ëœë¤ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lu6t4sRNNWkN"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"í–‰ë™ ê³µê°„ì˜ í¬ê¸°:\", a_size)\n",
        "print(\"ì„ì˜ì˜ í–‰ë™ ìƒ˜í”Œ:\", env.action_space.sample())  # ëœë¤ í–‰ë™ ì„ íƒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SJMJj3WaFOz"
      },
      "source": [
        "## ì´ì œ Reinforce ì•„í‚¤í…ì²˜ë¥¼ êµ¬ì¶•í•´ë³´ì  \n",
        "ì´ êµ¬í˜„ì€ ë‹¤ìŒ ë‘ ê°€ì§€ êµ¬í˜„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œë‹¤:  \n",
        "- [PyTorch ê³µì‹ ê°•í™” í•™ìŠµ ì˜ˆì œ](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)  \n",
        "- [Udacity Reinforce](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)  \n",
        "- [Chris1nexusì— ì˜í•œ í†µí•© ê°œì„ ](https://github.com/huggingface/deep-rl-class/pull/95)  \n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49kogtxBODX8"
      },
      "source": [
        "ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ë‹¤ìŒê³¼ ê°™ë‹¤:  \n",
        "- ë‘ ê°œì˜ ì™„ì „ ì—°ê²° ê³„ì¸µ (fc1ê³¼ fc2)  \n",
        "- fc1ì—ëŠ” ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì‚¬ìš©  \n",
        "- fc2ëŠ” í–‰ë™ì— ëŒ€í•œ í™•ë¥  ë¶„í¬ë¥¼ ì¶œë ¥í•˜ê¸° ìœ„í•´ Softmax ì‚¬ìš©"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2LHcHhVZvPZ"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        # ë‘ ê°œì˜ ì™„ì „ ì—°ê²° ë ˆì´ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ìˆœë°©í–¥ ì „ë‹¬ì„ ì •ì˜í•©ë‹ˆë‹¤.\n",
        "        # ìƒíƒœëŠ” fc1ìœ¼ë¡œ ì´ë™í•œ í›„ ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n",
        "\n",
        "        # fc1 ì¶œë ¥ì€ fc2ë¡œ ì´ë™í•©ë‹ˆë‹¤.\n",
        "\n",
        "        # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "        ì£¼ì–´ì§„ ìƒíƒœì—ì„œ í–‰ë™ì„ ì·¨í•©ë‹ˆë‹¤.\n",
        "        \"\"\"\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = np.argmax(m)\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOMrdwSYOWSC"
      },
      "source": [
        "### í•´ê²° ë°©ë²•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGdhRSVrOV4K"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = np.argmax(m)\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTGWL4g2eM5B"
      },
      "source": [
        "ë‚´ê°€ ì‹¤ìˆ˜í–ˆì–´, ì–´ë””ì„œ í–ˆëŠ”ì§€ ë§ì¶°ë³¼ ìˆ˜ ìˆê² ì–´?\n",
        "\n",
        "- ì•Œì•„ë³´ë ¤ë©´ ìˆœë°©í–¥ ì „íŒŒë¥¼ í•´ë³´ì:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwnqGBCNePor"
      },
      "outputs": [],
      "source": [
        "debug_policy = Policy(s_size, a_size, 64).to(device)\n",
        "debug_policy.act(env.reset())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14UYkoxCPaor"
      },
      "source": [
        "- ì—¬ê¸°ì„œ ì˜¤ë¥˜ ë©”ì‹œì§€ê°€ `ValueError: The value argument to log_prob must be a Tensor` ë¼ê³  ë‚˜íƒ€ë‚˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- ì´ëŠ” `m.log_prob(action)`ì—ì„œ `action`ì´ Tensorì—¬ì•¼ í•˜ì§€ë§Œ, **ê·¸ë ‡ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.**\n",
        "\n",
        "- ì´ìœ ë¥¼ ì•„ì‹œê² ìŠµë‹ˆê¹Œ? act í•¨ìˆ˜ë¥¼ í™•ì¸í•˜ê³  ì™œ ì‘ë™í•˜ì§€ ì•ŠëŠ”ì§€ íŒŒì•…í•´ ë³´ì„¸ìš”.\n",
        "\n",
        "ì¡°ì–¸ ğŸ’¡: ì´ êµ¬í˜„ì—ëŠ” ë¬´ì–¸ê°€ ì˜ëª»ëœ ë¶€ë¶„ì´ ìˆìŠµë‹ˆë‹¤. act í•¨ìˆ˜ì—ì„œ **í–‰ë™ì˜ í™•ë¥  ë¶„í¬ë¡œë¶€í„° í–‰ë™ì„ ìƒ˜í”Œë§í•˜ê¸°ë¥¼ ì›í•œë‹¤ëŠ” ê²ƒ**ì„ ê¸°ì–µí•˜ì„¸ìš”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfGJNZBUP7Vn"
      },
      "source": [
        "### (ì§„ì§œ) í•´ê²° ë°©ë²•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho_UHf49N9i4"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgJWQFU_eUYw"
      },
      "source": [
        "CartPoleì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ **ë²„ê·¸ê°€ ê°„ë‹¨í•œ í™˜ê²½ì´ ì•„ë‹Œ ìš°ë¦¬ì˜ í†µí•© ê³¼ì •ì—ì„œ ë°œìƒí–ˆë‹¤ëŠ” ê²ƒì„ ì•Œê¸° ë•Œë¬¸ì—** ë””ë²„ê¹…ì´ ë” ì‰¬ì› ìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-20i7Pk0l1T"
      },
      "source": [
        "- **í–‰ë™ì˜ í™•ë¥  ë¶„í¬ë¡œë¶€í„° í–‰ë™ì„ ìƒ˜í”Œë§í•˜ê¸°ë¥¼ ì›í•˜ê¸° ë•Œë¬¸ì—**, í•­ìƒ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í–‰ë™ë§Œì„ ì¶œë ¥í•˜ëŠ” `action = np.argmax(m)`ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n",
        "\n",
        "- í™•ë¥  ë¶„í¬ P(.|s)ì—ì„œ í–‰ë™ì„ ìƒ˜í”Œë§í•˜ëŠ” `action = m.sample()`ë¡œ ëŒ€ì²´í•´ì•¼ í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MXoqetzfIoW"
      },
      "source": [
        "### Reinforce í›ˆë ¨ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬ì¶•í•´ ë´…ì‹œë‹¤\n",
        "ë‹¤ìŒì€ Reinforce ì•Œê³ ë¦¬ì¦˜ì˜ ì˜ì‚¬ì½”ë“œì…ë‹ˆë‹¤:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/pg_pseudocode.png\" alt=\"Policy gradient pseudocode\"/>\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmcXG-9i2Qu2"
      },
      "source": [
        "- ë¦¬í„´ Gt (6í–‰)ë¥¼ ê³„ì‚°í•  ë•Œ, **ì‹œì  të¶€í„° ì‹œì‘í•˜ëŠ”** í• ì¸ëœ ë³´ìƒì˜ í•©ê³„ë¥¼ ê³„ì‚°í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "- ì™œ ê·¸ëŸ´ê¹Œìš”? ìš°ë¦¬ì˜ ì •ì±…ì€ **ê²°ê³¼ì— ê·¼ê±°í•˜ì—¬ í–‰ë™ì„ ê°•í™”í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤**: ì¦‰, í–‰ë™ì„ ì·¨í•˜ê¸° ì „ì— ì–»ì€ ë³´ìƒì€ ì†Œìš©ì´ ì—†ìŠµë‹ˆë‹¤ (ê·¸ í–‰ë™ ë•Œë¬¸ì— ë°œìƒí•œ ê²ƒì´ ì•„ë‹ˆë¯€ë¡œ), **í–‰ë™ í›„ì— ì˜¤ëŠ” ë³´ìƒë§Œì´ ì¤‘ìš”í•©ë‹ˆë‹¤**.\n",
        "\n",
        "- ì´ê²ƒì„ ì½”ë”©í•˜ê¸° ì „ì— reward-to-go ì •ì±… ê²½ì‚¬ë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ëŠ” [ê³¼ê±°ì— í˜„í˜¹ë˜ì§€ ë§ˆì„¸ìš”(don't let the past distract you)](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#don-t-let-the-past-distract-you) ì„¹ì…˜ì„ ì½ì–´ë³´ì‹œëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n",
        "\n",
        "ìš°ë¦¬ëŠ” ê° ì‹œì ì˜ ë¦¬í„´ì„ **íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ê¸° ìœ„í•´** [Chris1nexus](https://github.com/Chris1nexus)ê°€ ì½”ë”©í•œ í¥ë¯¸ë¡œìš´ ê¸°ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì£¼ì„ì— ì ˆì°¨ê°€ ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤. [PR ì„¤ëª…ì„ í™•ì¸í•˜ëŠ” ê²ƒ](https://github.com/huggingface/deep-rl-class/pull/95)ë„ ì£¼ì €í•˜ì§€ ë§ˆì„¸ìš”.\n",
        "í•˜ì§€ë§Œ ì „ë°˜ì ìœ¼ë¡œ ì•„ì´ë””ì–´ëŠ” **ê° ì‹œì ì˜ ë¦¬í„´ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê³„ì‚°í•˜ëŠ” ê²ƒ**ì…ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O554nUGPpcoq"
      },
      "source": [
        "ë‘ ë²ˆì§¸ë¡œ ê¶ê¸ˆí•˜ì‹¤ ì ì€ **ì™œ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ”ê°€** ì¼ ê²ƒì…ë‹ˆë‹¤. ê²½ì‚¬ í•˜ê°•(Gradient Descent)ì´ ì•„ë‹Œ ê²½ì‚¬ ìƒìŠ¹(Gradient Ascent)ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆì—ˆëŠ”ë°ìš”?\n",
        "\n",
        "- ìš°ë¦¬ëŠ” íš¨ìš© í•¨ìˆ˜ $J(\\theta)$ë¥¼ ìµœëŒ€í™”í•˜ê¸°ë¥¼ ì›í•˜ì§€ë§Œ, PyTorchëŠ” TensorFlowì²˜ëŸ¼ **ëª©ì  í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒ**ì´ ë” ì¢‹ìŠµë‹ˆë‹¤.\n",
        "    - íŠ¹ì • ì‹œì ì—ì„œ í–‰ë™ 3ì„ ê°•í™”í•œë‹¤ê³  ê°€ì •í•´ ë´…ì‹œë‹¤. í›ˆë ¨ ì „ì— ì´ í–‰ë™ì˜ í™•ë¥  PëŠ” 0.25ì…ë‹ˆë‹¤.\n",
        "    - ë”°ë¼ì„œ ìš°ë¦¬ëŠ” $\\theta$ë¥¼ ìˆ˜ì •í•˜ì—¬ $\\pi_\\theta(a_3|s; \\theta) > 0.25$ê°€ ë˜ë„ë¡ í•˜ê³ ì í•©ë‹ˆë‹¤.\n",
        "    - ëª¨ë“  Pì˜ í•©ì€ 1ì´ ë˜ì–´ì•¼ í•˜ë¯€ë¡œ, $\\pi_\\theta(a_3|s; \\theta)$ë¥¼ ìµœëŒ€í™”í•˜ë©´ **ë‹¤ë¥¸ í–‰ë™ì˜ í™•ë¥ ì´ ìµœì†Œí™”ë©ë‹ˆë‹¤.**\n",
        "    - ë”°ë¼ì„œ PyTorchì—ê²Œ **$1 - \\pi_\\theta(a_3|s; \\theta)$ë¥¼ ìµœì†Œí™”**í•˜ë„ë¡ ì§€ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.\n",
        "    - ì´ ì†ì‹¤ í•¨ìˆ˜ëŠ” $\\pi_\\theta(a_3|s; \\theta)$ê°€ 1ì— ê°€ê¹Œì›Œì§ì— ë”°ë¼ 0ì— ìˆ˜ë ´í•©ë‹ˆë‹¤.\n",
        "    - ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ê²½ì‚¬ê°€ $\\pi_\\theta(a_3|s; \\theta)$ë¥¼ ìµœëŒ€í™”í•˜ë„ë¡ ìœ ë„í•˜ê³  ìˆìŠµë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOdv8Q9NfLK7"
      },
      "outputs": [],
      "source": [
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
        "    # í›ˆë ¨ ì¤‘ ì ìˆ˜ ê³„ì‚°ì„ ë•ìŠµë‹ˆë‹¤.\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    # ì˜ì‚¬ì½”ë“œ 3í–‰\n",
        "    for i_episode in range(1, n_training_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state =  # TODO: í™˜ê²½ ë¦¬ì…‹\n",
        "        # ì˜ì‚¬ì½”ë“œ 4í–‰\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = # TODO í–‰ë™ ê°€ì ¸ì˜¤ê¸°\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = # TODO: í™˜ê²½ ë‹¨ê³„ ì§„í–‰\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "\n",
        "        # ì˜ì‚¬ì½”ë“œ 6í–‰: ë¦¬í„´ ê³„ì‚°\n",
        "        returns = deque(maxlen=max_t)\n",
        "        n_steps = len(rewards)\n",
        "        # ê° ì‹œì ì—ì„œì˜ í• ì¸ëœ ë¦¬í„´ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "        # ì‹œì  tì—ì„œì˜ ê°ë§ˆ-í• ì¸ëœ ë¦¬í„´ (G_t) + ì‹œì  tì—ì„œì˜ ë³´ìƒ í•©ê³„ì…ë‹ˆë‹¤.\n",
        "\n",
        "        # O(N) ì‹œê°„ ë³µì¡ë„ë¡œ, ì—¬ê¸°ì„œ Nì€ ì‹œì ì˜ ìˆ˜ì…ë‹ˆë‹¤.\n",
        "        # (í• ì¸ëœ ë¦¬í„´ G_tì˜ ì´ ì •ì˜ëŠ” Sutton&Barto 2017 2nd draft 44í˜ì´ì§€ì—\n",
        "        # ì œì‹œëœ ì´ ì–‘ì˜ ì •ì˜ë¥¼ ë”°ë¦…ë‹ˆë‹¤)\n",
        "        # G_t = r_(t+1) + r_(t+2) + ...\n",
        "\n",
        "        # ì´ ê³µì‹ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê° ì‹œì  tì—ì„œì˜ ë¦¬í„´ì€\n",
        "        # ê³„ì‚°ëœ ë¯¸ë˜ ë¦¬í„´ G_(t+1)ì„ ì¬ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ë¦¬í„´ G_të¥¼ ê³„ì‚°í•¨ìœ¼ë¡œì¨\n",
        "        # ê³„ì‚°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
        "        # G_(t-1) = r_t + gamma* G_t\n",
        "        # (ì´ëŠ” ë™ì  ê³„íšë²• ì ‘ê·¼ ë°©ì‹ì„ ë”°ë¥´ë©°, ì—¬ëŸ¬ ë²ˆ ê³„ì‚°í•˜ëŠ” ê²ƒì„ í”¼í•˜ê¸° ìœ„í•´\n",
        "        # í•´ë¥¼ ê¸°ì–µí•©ë‹ˆë‹¤)\n",
        "\n",
        "        # ìœ„ëŠ” ë‹¤ìŒ ì‹ê³¼ ë™ì¼í•˜ë¯€ë¡œ ì˜¬ë°”ë¦…ë‹ˆë‹¤ (Sutton&Barto 2017 2nd draft 46í˜ì´ì§€ ì°¸ì¡°)\n",
        "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
        "\n",
        "\n",
        "        ## ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹œì  tì—ì„œì˜ ë¦¬í„´ì„ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•©ë‹ˆë‹¤:\n",
        "        #           gamma[t] * return[t] + reward[t]\n",
        "        #\n",
        "        ## ìœ„ì—ì„œ ì œì‹œëœ ê³µì‹ì„ ì‚¬ìš©í•˜ê³ , ì²˜ìŒë¶€í„° ë§ˆì§€ë§‰ê¹Œì§€ ê³„ì‚°í•  ë•Œ í•„ìš”í•œ\n",
        "        ## ì¤‘ë³µ ê³„ì‚°ì„ í”¼í•˜ê¸° ìœ„í•´ ë§ˆì§€ë§‰ ì‹œì ë¶€í„° ì²« ì‹œì ê¹Œì§€ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "        ## ë”°ë¼ì„œ \"returns\" íëŠ” appendleft() í•¨ìˆ˜ ë•ë¶„ì— ì‹œê°„ë³µì¡ë„ O(1)ë¡œ\n",
        "        ## ìœ„ì¹˜ 0ì— ì¶”ê°€í•  ìˆ˜ ìˆì–´ t=0ë¶€í„° t=n_stepsê¹Œì§€ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë¦¬í„´ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "        ## ì¼ë°˜ì ì¸ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ëŠ” ì´ë¥¼ ìœ„í•´ O(N)ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "\n",
        "        for t in range(n_steps)[::-1]:\n",
        "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
        "            returns.appendleft(    ) # TODO: ì—¬ê¸° ì™„ì„±\n",
        "        \n",
        "        ## í›ˆë ¨ì„ ë” ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ ë¦¬í„´ì˜ í‘œì¤€í™”ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "        ## epsëŠ” ê°€ì¥ ì‘ì€ í‘œí˜„ ê°€ëŠ¥í•œ floatì´ë©°,\n",
        "        # ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„±ì„ í”¼í•˜ê¸° ìœ„í•´ ë¦¬í„´ì˜ í‘œì¤€ í¸ì°¨ì— ë”í•´ì§‘ë‹ˆë‹¤.\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "        # 7í–‰:\n",
        "        policy_loss = []\n",
        "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * disc_return)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        # 8í–‰: PyTorchëŠ” ê²½ì‚¬ í•˜ê°•ì„ ì„ í˜¸í•©ë‹ˆë‹¤.\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i_episode % print_every == 0:\n",
        "            print('ì—í”¼ì†Œë“œ {}\\tí‰ê·  ì ìˆ˜: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
        "\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YB0Cxrw1StrP"
      },
      "source": [
        "#### í•´ê²° ë°©ë²•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCNvyElRStWG"
      },
      "outputs": [],
      "source": [
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
        "    # í›ˆë ¨ ì¤‘ ì ìˆ˜ ê³„ì‚°ì„ ë•ìŠµë‹ˆë‹¤.\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    # ì˜ì‚¬ì½”ë“œ 3í–‰\n",
        "    for i_episode in range(1, n_training_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset() # TODO: í™˜ê²½ ë¦¬ì…‹\n",
        "        # ì˜ì‚¬ì½”ë“œ 4í–‰\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state) # TODO í–‰ë™ ê°€ì ¸ì˜¤ê¸°\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action) # TODO: í™˜ê²½ ë‹¨ê³„ ì§„í–‰\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break\n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "\n",
        "        # ì˜ì‚¬ì½”ë“œ 6í–‰: ë¦¬í„´ ê³„ì‚°\n",
        "        returns = deque(maxlen=max_t)\n",
        "        n_steps = len(rewards)\n",
        "        # ê° ì‹œì ì—ì„œì˜ í• ì¸ëœ ë¦¬í„´ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "        # ì‹œì  tì—ì„œì˜ ê°ë§ˆ-í• ì¸ëœ ë¦¬í„´ (G_t) + ì‹œì  tì—ì„œì˜ ë³´ìƒ í•©ê³„ì…ë‹ˆë‹¤.\n",
        "\n",
        "        # O(N) ì‹œê°„ ë³µì¡ë„ë¡œ, ì—¬ê¸°ì„œ Nì€ ì‹œì ì˜ ìˆ˜ì…ë‹ˆë‹¤.\n",
        "        # (í• ì¸ëœ ë¦¬í„´ G_tì˜ ì´ ì •ì˜ëŠ” Sutton&Barto 2017 2nd draft 44í˜ì´ì§€ì—\n",
        "        # ì œì‹œëœ ì´ ì–‘ì˜ ì •ì˜ë¥¼ ë”°ë¦…ë‹ˆë‹¤)\n",
        "        # G_t = r_(t+1) + r_(t+2) + ...\n",
        "\n",
        "        # ì´ ê³µì‹ì´ ì£¼ì–´ì¡Œì„ ë•Œ, ê° ì‹œì  tì—ì„œì˜ ë¦¬í„´ì€\n",
        "        # ê³„ì‚°ëœ ë¯¸ë˜ ë¦¬í„´ G_(t+1)ì„ ì¬ì‚¬ìš©í•˜ì—¬ í˜„ì¬ ë¦¬í„´ G_të¥¼ ê³„ì‚°í•¨ìœ¼ë¡œì¨\n",
        "        # ê³„ì‚°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "        # G_t = r_(t+1) + gamma*G_(t+1)\n",
        "        # G_(t-1) = r_t + gamma* G_t\n",
        "        # (ì´ëŠ” ë™ì  ê³„íšë²• ì ‘ê·¼ ë°©ì‹ì„ ë”°ë¥´ë©°, ì—¬ëŸ¬ ë²ˆ ê³„ì‚°í•˜ëŠ” ê²ƒì„ í”¼í•˜ê¸° ìœ„í•´\n",
        "        # í•´ë¥¼ ê¸°ì–µí•©ë‹ˆë‹¤)\n",
        "\n",
        "        # ìœ„ëŠ” ë‹¤ìŒ ì‹ê³¼ ë™ì¼í•˜ë¯€ë¡œ ì˜¬ë°”ë¦…ë‹ˆë‹¤ (Sutton&Barto 2017 2nd draft 46í˜ì´ì§€ ì°¸ì¡°)\n",
        "        # G_(t-1) = r_t + gamma*r_(t+1) + gamma*gamma*r_(t+2) + ...\n",
        "\n",
        "\n",
        "        ## ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹œì  tì—ì„œì˜ ë¦¬í„´ì„ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•©ë‹ˆë‹¤:\n",
        "        #           gamma[t] * return[t] + reward[t]\n",
        "        #\n",
        "        ## ìœ„ì—ì„œ ì œì‹œëœ ê³µì‹ì„ ì‚¬ìš©í•˜ê³ , ì²˜ìŒë¶€í„° ë§ˆì§€ë§‰ê¹Œì§€ ê³„ì‚°í•  ë•Œ í•„ìš”í•œ\n",
        "        ## ì¤‘ë³µ ê³„ì‚°ì„ í”¼í•˜ê¸° ìœ„í•´ ë§ˆì§€ë§‰ ì‹œì ë¶€í„° ì²« ì‹œì ê¹Œì§€ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
        "\n",
        "        ## ë”°ë¼ì„œ \"returns\" íëŠ” appendleft() í•¨ìˆ˜ ë•ë¶„ì— ì‹œê°„ë³µì¡ë„ O(1)ë¡œ\n",
        "        ## ìœ„ì¹˜ 0ì— ì¶”ê°€í•  ìˆ˜ ìˆì–´ t=0ë¶€í„° t=n_stepsê¹Œì§€ ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë¦¬í„´ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
        "        ## ì¼ë°˜ì ì¸ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ëŠ” ì´ë¥¼ ìœ„í•´ O(N)ì´ í•„ìš”í•©ë‹ˆë‹¤.\n",
        "\n",
        "        for t in range(n_steps)[::-1]:\n",
        "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
        "            returns.appendleft( disc_return_t * gamma + rewards[t] ) # TODO: ì—¬ê¸° ì™„ì„±\n",
        "        \n",
        "        ## í›ˆë ¨ì„ ë” ì•ˆì •ì ìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ ë¦¬í„´ì˜ í‘œì¤€í™”ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "        ## epsëŠ” ê°€ì¥ ì‘ì€ í‘œí˜„ ê°€ëŠ¥í•œ floatì´ë©°,\n",
        "        # ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„±ì„ í”¼í•˜ê¸° ìœ„í•´ ë¦¬í„´ì˜ í‘œì¤€ í¸ì°¨ì— ë”í•´ì§‘ë‹ˆë‹¤.\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "        # 7í–‰:\n",
        "        policy_loss = []\n",
        "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * disc_return)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "        # 8í–‰: PyTorchëŠ” ê²½ì‚¬ í•˜ê°•ì„ ì„ í˜¸í•©ë‹ˆë‹¤.\n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i_episode % print_every == 0:\n",
        "            print('ì—í”¼ì†Œë“œ {}\\tí‰ê·  ì ìˆ˜: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
        "\n",
        "    return scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIWhQyJjfpEt"
      },
      "source": [
        "##  í•™ìŠµ ì‹œì‘  \n",
        "- ì´ì œ ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚¬ ì¤€ë¹„ê°€ ë˜ì—ˆë‹¤.  \n",
        "- ë¨¼ì €, ëª¨ë“  í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë‹´ì€ ë³€ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.  \n",
        "- í•™ìŠµ íŒŒë¼ë¯¸í„°ëŠ” ë³€ê²½í•  ìˆ˜ ìˆìœ¼ë©°, ë³€ê²½í•˜ëŠ” ê²ƒì´ ì¢‹ë‹¤ ğŸ˜‰"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utRe1NgtVBYF"
      },
      "outputs": [],
      "source": [
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 16,\n",
        "    \"n_training_episodes\": 1000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3lWyVXBVfl6"
      },
      "outputs": [],
      "source": [
        "# ì •ì±… ìƒì„± ë° ë””ë°”ì´ìŠ¤ì— ë°°ì¹˜\n",
        "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uGf-hQCnfouB"
      },
      "outputs": [],
      "source": [
        "# TypeError: expected np.ndarray (got tuple) ë°œìƒ ì‹œ gym<0.26 ë²„ì „ ì„¤ì¹˜\n",
        "scores = reinforce(cartpole_policy,\n",
        "                   cartpole_optimizer,\n",
        "                   cartpole_hyperparameters[\"n_training_episodes\"], \n",
        "                   cartpole_hyperparameters[\"max_t\"],\n",
        "                   cartpole_hyperparameters[\"gamma\"], \n",
        "                   100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qajj2kXqhB3g"
      },
      "source": [
        "## í‰ê°€ ë°©ë²• ì •ì˜ ğŸ“\n",
        "- ì—¬ê¸°ì„œ Reinforce ì—ì´ì „íŠ¸ë¥¼ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë° ì‚¬ìš©í•  í‰ê°€ ë°©ë²•ì„ ì •ì˜í•©ë‹ˆë‹¤."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FamHmxyhBEU"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "  \"\"\"\n",
        "  ì—ì´ì „íŠ¸ë¥¼ ``n_eval_episodes`` ì—í”¼ì†Œë“œ ë™ì•ˆ í‰ê°€í•˜ê³  í‰ê·  ë³´ìƒê³¼ ë³´ìƒì˜ í‘œì¤€ í¸ì°¨ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.\n",
        "  :param env: í‰ê°€ í™˜ê²½\n",
        "  :param n_eval_episodes: ì—ì´ì „íŠ¸ë¥¼ í‰ê°€í•  ì—í”¼ì†Œë“œ ìˆ˜\n",
        "  :param policy: Reinforce ì—ì´ì „íŠ¸\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in range(n_eval_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "      action, _ = policy.act(state)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "        \n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdH2QCrLTrlT"
      },
      "source": [
        "## ì—ì´ì „íŠ¸ í‰ê°€í•˜ê¸° ğŸ“ˆ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohGSXDyHh0xx"
      },
      "outputs": [],
      "source": [
        "evaluate_agent(eval_env, \n",
        "               cartpole_hyperparameters[\"max_t\"], \n",
        "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
        "               cartpole_policy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CoeLkQ7TpO8"
      },
      "source": [
        "### í•™ìŠµëœ ëª¨ë¸ì„ Hubì— ê²Œì‹œí•˜ê¸° ğŸ”¥\n",
        "ì´ì œ í•™ìŠµ í›„ì— ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆìœ¼ë¯€ë¡œ, í•œ ì¤„ì˜ ì½”ë“œë¡œ í•™ìŠµëœ ëª¨ë¸ì„ hub ğŸ¤—ì— ê²Œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ë‹¤ìŒì€ ëª¨ë¸ ì¹´ë“œ ì˜ˆì‹œì…ë‹ˆë‹¤:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/modelcard.png\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmhs1k-cftIq"
      },
      "source": [
        "### í—ˆë¸Œì— í‘¸ì‹œ  \n",
        "#### ì´ ì½”ë“œëŠ” ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIVsvlW_8tcw"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import HfApi, snapshot_download\n",
        "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
        "\n",
        "from pathlib import Path\n",
        "import datetime\n",
        "import json\n",
        "import imageio\n",
        "\n",
        "import tempfile\n",
        "\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lo4JH45if81z"
      },
      "outputs": [],
      "source": [
        "def record_video(env, policy, out_directory, fps=30):\n",
        "  \"\"\"\n",
        "  ì—ì´ì „íŠ¸ì˜ ë¦¬í”Œë ˆì´ ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤\n",
        "  :param env: í™˜ê²½\n",
        "  :param policy: ì—ì´ì „íŠ¸ì˜ ì •ì±…\n",
        "  :param out_directory: ì¶œë ¥ ë””ë ‰í† ë¦¬\n",
        "  :param fps: ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜ (taxi-v3ì™€ frozenlake-v1ì—ì„œëŠ” 1 ì‚¬ìš©)\n",
        "  \"\"\"\n",
        "  images = []  \n",
        "  done = False\n",
        "  state = env.reset()\n",
        "  img = env.render(mode='rgb_array')\n",
        "  images.append(img)\n",
        "  while not done:\n",
        "    # í•´ë‹¹ ìƒíƒœì—ì„œ ê¸°ëŒ€ë˜ëŠ” ë¯¸ë˜ ë³´ìƒì´ ê°€ì¥ í° í–‰ë™(ì¸ë±ìŠ¤)ì„ ì„ íƒ\n",
        "    action, _ = policy.act(state)\n",
        "    state, reward, done, info = env.step(action)  # ë…¹í™” ë¡œì§ì„ ìœ„í•´ ë‹¤ìŒ ìƒíƒœë¥¼ stateë¡œ ë°”ë¡œ ì €ì¥\n",
        "    img = env.render(mode='rgb_array')\n",
        "    images.append(img)\n",
        "  imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TPdq47D7_f_"
      },
      "outputs": [],
      "source": [
        "def push_to_hub(repo_id, \n",
        "                model,\n",
        "                hyperparameters,\n",
        "                eval_env,\n",
        "                video_fps=30\n",
        "                ):\n",
        "  \"\"\"\n",
        "  ëª¨ë¸ì„ í‰ê°€í•˜ê³ , ì˜ìƒ ìƒì„± í›„ Hugging Face Hubì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.\n",
        "  ì´ ë©”ì„œë“œëŠ” ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤:\n",
        "  - ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤\n",
        "  - ëª¨ë¸ ì¹´ë“œë¥¼ ìƒì„±í•©ë‹ˆë‹¤\n",
        "  - ì—ì´ì „íŠ¸ì˜ ë¦¬í”Œë ˆì´ ì˜ìƒì„ ìƒì„±í•©ë‹ˆë‹¤\n",
        "  - ëª¨ë“  ê²°ê³¼ë¥¼ Hubì— ì—…ë¡œë“œí•©ë‹ˆë‹¤\n",
        "\n",
        "  :param repo_id: Hugging Face Hubì˜ ëª¨ë¸ ì €ì¥ì†Œ ID\n",
        "  :param model: ì €ì¥í•  PyTorch ëª¨ë¸\n",
        "  :param hyperparameters: í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "  :param eval_env: í‰ê°€ í™˜ê²½\n",
        "  :param video_fps: ë¦¬í”Œë ˆì´ ì˜ìƒ ë…¹í™” ì‹œ ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜\n",
        "  \"\"\"\n",
        "\n",
        "  _, repo_name = repo_id.split(\"/\")\n",
        "  api = HfApi()\n",
        "  \n",
        "  # 1ë‹¨ê³„: ì €ì¥ì†Œ ìƒì„±\n",
        "  repo_url = api.create_repo(\n",
        "        repo_id=repo_id,\n",
        "        exist_ok=True,\n",
        "  )\n",
        "\n",
        "  with tempfile.TemporaryDirectory() as tmpdirname:\n",
        "    local_directory = Path(tmpdirname)\n",
        "  \n",
        "    # 2ë‹¨ê³„: ëª¨ë¸ ì €ì¥\n",
        "    torch.save(model, local_directory / \"model.pt\")\n",
        "\n",
        "    # 3ë‹¨ê³„: í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
        "    with open(local_directory / \"hyperparameters.json\", \"w\") as outfile:\n",
        "      json.dump(hyperparameters, outfile)\n",
        "    \n",
        "    # 4ë‹¨ê³„: ëª¨ë¸ í‰ê°€ ë° ê²°ê³¼ ì €ì¥\n",
        "    mean_reward, std_reward = evaluate_agent(eval_env, \n",
        "                                            hyperparameters[\"max_t\"],\n",
        "                                            hyperparameters[\"n_evaluation_episodes\"], \n",
        "                                            model)\n",
        "    # í‰ê°€ ì‹œê° ì €ì¥\n",
        "    eval_datetime = datetime.datetime.now()\n",
        "    eval_form_datetime = eval_datetime.isoformat()\n",
        "\n",
        "    evaluate_data = {\n",
        "          \"env_id\": hyperparameters[\"env_id\"], \n",
        "          \"mean_reward\": mean_reward,\n",
        "          \"n_evaluation_episodes\": hyperparameters[\"n_evaluation_episodes\"],\n",
        "          \"eval_datetime\": eval_form_datetime,\n",
        "    }\n",
        "\n",
        "    # JSON íŒŒì¼ë¡œ ì €ì¥\n",
        "    with open(local_directory / \"results.json\", \"w\") as outfile:\n",
        "        json.dump(evaluate_data, outfile)\n",
        "\n",
        "    # 5ë‹¨ê³„: ëª¨ë¸ ì¹´ë“œ ìƒì„±\n",
        "    env_name = hyperparameters[\"env_id\"]\n",
        "    \n",
        "    metadata = {}\n",
        "    metadata[\"tags\"] = [\n",
        "          env_name,\n",
        "          \"reinforce\",\n",
        "          \"reinforcement-learning\",\n",
        "          \"custom-implementation\",\n",
        "          \"deep-rl-class\"\n",
        "      ]\n",
        "\n",
        "    # í‰ê°€ ì§€í‘œ ì¶”ê°€\n",
        "    eval = metadata_eval_result(\n",
        "        model_pretty_name=repo_name,\n",
        "        task_pretty_name=\"reinforcement-learning\",\n",
        "        task_id=\"reinforcement-learning\",\n",
        "        metrics_pretty_name=\"mean_reward\",\n",
        "        metrics_id=\"mean_reward\",\n",
        "        metrics_value=f\"{mean_reward:.2f} +/- {std_reward:.2f}\",\n",
        "        dataset_pretty_name=env_name,\n",
        "        dataset_id=env_name,\n",
        "      )\n",
        "\n",
        "    # ë©”íƒ€ë°ì´í„° ë³‘í•©\n",
        "    metadata = {**metadata, **eval}\n",
        "\n",
        "    model_card = f\"\"\"\n",
        "  # **Reinforce** Agent playing **{env_id}**\n",
        "  This is a trained model of a **Reinforce** agent playing **{env_id}** .\n",
        "  To learn to use this model and train yours check Unit 4 of the Deep Reinforcement Learning Course: https://huggingface.co/deep-rl-course/unit4/introduction\n",
        "  \"\"\"\n",
        "\n",
        "    readme_path = local_directory / \"README.md\"\n",
        "    readme = \"\"\n",
        "    if readme_path.exists():\n",
        "        with readme_path.open(\"r\", encoding=\"utf8\") as f:\n",
        "          readme = f.read()\n",
        "    else:\n",
        "      readme = model_card\n",
        "\n",
        "    with readme_path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "      f.write(readme)\n",
        "\n",
        "    # ë©”íƒ€ë°ì´í„°ë¥¼ READMEì— ì €ì¥\n",
        "    metadata_save(readme_path, metadata)\n",
        "\n",
        "    # 6ë‹¨ê³„: ì˜ìƒ ë…¹í™”\n",
        "    video_path =  local_directory / \"replay.mp4\"\n",
        "    record_video(env, model, video_path, video_fps)\n",
        "\n",
        "    # 7ë‹¨ê³„: Hubì— ì—…ë¡œë“œ\n",
        "    api.upload_folder(\n",
        "          repo_id=repo_id,\n",
        "          folder_path=local_directory,\n",
        "          path_in_repo=\".\",\n",
        "    )\n",
        "\n",
        "    print(f\"ëª¨ë¸ì´ Hubì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: {repo_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w17w8CxzoURM"
      },
      "source": [
        "### .\n",
        "\n",
        "`push_to_hub`ì„ ì‚¬ìš©í•˜ë©´ **ì—ì´ì „íŠ¸ë¥¼ í‰ê°€í•˜ê³ , ë¦¬í”Œë ˆì´ë¥¼ ë…¹í™”í•˜ë©°, ëª¨ë¸ ì¹´ë“œë¥¼ ìƒì„±í•œ ë’¤ Hubì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤**.\n",
        "\n",
        "ì´ë ‡ê²Œ í•˜ë©´:\n",
        "- **ì‘ì—… ê²°ê³¼ë¥¼ ë©‹ì§€ê²Œ ë³´ì—¬ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤** ğŸ”¥  \n",
        "- **ì—ì´ì „íŠ¸ê°€ í”Œë ˆì´í•˜ëŠ” ëª¨ìŠµì„ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤** ğŸ‘€  \n",
        "- **ë‹¤ë¥¸ ì‚¬ëŒë“¤ì´ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì—ì´ì „íŠ¸ë¥¼ ì»¤ë®¤ë‹ˆí‹°ì— ê³µìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤** ğŸ’¾  \n",
        "- **í´ë˜ìŠ¤ë©”ì´íŠ¸ë“¤ê³¼ ë¹„êµí•´ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì„ ë¦¬ë”ë³´ë“œì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤** ğŸ‘‰ https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWnFC0iZooTw"
      },
      "source": [
        "ì»¤ë®¤ë‹ˆí‹°ì™€ ëª¨ë¸ì„ ê³µìœ í•˜ë ¤ë©´ ë‹¤ìŒì˜ ì„¸ ê°€ì§€ ë‹¨ê³„ë¥¼ ì¶”ê°€ë¡œ ë”°ë¼ì•¼ í•©ë‹ˆë‹¤:\n",
        "\n",
        "---\n",
        "\n",
        "1ï¸âƒ£ **(ì•„ì§ í•˜ì§€ ì•Šì•˜ë‹¤ë©´)** Hugging Face ê³„ì •ì„ ìƒì„±í•˜ì„¸ìš” â¡ https://huggingface.co/join\n",
        "\n",
        "2ï¸âƒ£ ë¡œê·¸ì¸í•œ í›„, Hugging Face ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì¸ì¦ í† í°ì„ ì €ì¥í•´ì•¼ í•©ë‹ˆë‹¤.  \n",
        "- ìƒˆ í† í°ì„ ìƒì„±í•˜ì„¸ìš” (https://huggingface.co/settings/tokens)  \n",
        "  **ê¶Œí•œì€ *ì“°ê¸°(write)* ë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤**\n",
        "\n",
        "---\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QB5nIcxR8paT"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyWc1x3-o3xG"
      },
      "source": [
        "Google Colabì´ë‚˜ Jupyter Notebookì„ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ê²½ìš°, ì•„ë˜ ëª…ë ¹ì–´ë¥¼ ëŒ€ì‹  ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-D-zhbRoeOm"
      },
      "source": [
        "3ï¸âƒ£ ì´ì œ í›ˆë ¨ëœ ì—ì´ì „íŠ¸ë¥¼ ğŸ¤— í—ˆë¸Œì— í‘¸ì‹œí•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤ ğŸ”¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNwkTS65Uq3Q"
      },
      "outputs": [],
      "source": [
        "repo_id = \"\"  # TODO: ìì‹ ì˜ ì €ì¥ì†Œ IDë¥¼ ì •ì˜í•˜ì„¸ìš”. í˜•ì‹: {username}/Reinforce-{model_id}\n",
        "push_to_hub(\n",
        "    repo_id,\n",
        "    cartpole_policy,            # ì €ì¥í•  ëª¨ë¸\n",
        "    cartpole_hyperparameters,   # í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "    eval_env,                   # í‰ê°€ í™˜ê²½\n",
        "    video_fps=30                # ë¦¬í”Œë ˆì´ ì˜ìƒì˜ FPS\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrnuKH1gYZSz"
      },
      "source": [
        "ì´ì œ êµ¬í˜„ì˜ ê²¬ê³ í•¨ì„ í…ŒìŠ¤íŠ¸í•´ë´¤ìœ¼ë‹ˆ, ì¢€ ë” ë³µì¡í•œ í™˜ê²½ì— ë„ì „í•´ë´…ì‹œë‹¤: **PixelCopter** ğŸš"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNLVmKKVKA6j"
      },
      "source": [
        "## ë‘ ë²ˆì§¸ ì—ì´ì „íŠ¸: PixelCopter ğŸš\n",
        "\n",
        "### PixelCopter í™˜ê²½ ì‚´í´ë³´ê¸° ğŸ‘€  \n",
        "- [í™˜ê²½ ë¬¸ì„œ ë³´ê¸°](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBSc8mlfyin3"
      },
      "outputs": [],
      "source": [
        "env_id = \"Pixelcopter-PLE-v0\"\n",
        "env = gym.make(env_id)\n",
        "eval_env = gym.make(env_id)\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5u_zAHsKBy7"
      },
      "outputs": [],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"ìƒíƒœ ê³µê°„ì˜ í¬ê¸°:\", s_size)\n",
        "print(\"ì„ì˜ì˜ ìƒíƒœ ìƒ˜í”Œ:\", env.observation_space.sample())  # ëœë¤ ìƒíƒœ ê°€ì ¸ì˜¤ê¸°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7yJM9YXKNbq"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"í–‰ë™ ê³µê°„ì˜ í¬ê¸°:\", a_size)\n",
        "print(\"ì„ì˜ì˜ í–‰ë™ ìƒ˜í”Œ:\", env.action_space.sample())  # ëœë¤ í–‰ë™ ì„ íƒ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNWvlyvzalXr"
      },
      "source": [
        "ê´€ì°° ê³µê°„ (7) ğŸ‘€:  \n",
        "- í”Œë ˆì´ì–´ì˜ y ìœ„ì¹˜  \n",
        "- í”Œë ˆì´ì–´ì˜ ì†ë„  \n",
        "- í”Œë ˆì´ì–´ì™€ ë°”ë‹¥ ê°„ ê±°ë¦¬  \n",
        "- í”Œë ˆì´ì–´ì™€ ì²œì¥ ê°„ ê±°ë¦¬  \n",
        "- ë‹¤ìŒ ë¸”ë¡ê¹Œì§€ì˜ x ê±°ë¦¬  \n",
        "- ë‹¤ìŒ ë¸”ë¡ì˜ ìƒë‹¨ y ìœ„ì¹˜  \n",
        "- ë‹¤ìŒ ë¸”ë¡ì˜ í•˜ë‹¨ y ìœ„ì¹˜  \n",
        "\n",
        "í–‰ë™ ê³µê°„ (2) ğŸ®:  \n",
        "- ìœ„ë¡œ (ê°€ì†ê¸° ëˆ„ë¥´ê¸°)  \n",
        "- ì•„ë¬´ê²ƒë„ í•˜ì§€ ì•Šê¸° (ê°€ì†ê¸° ëˆ„ë¥´ì§€ ì•Šê¸°)  \n",
        "\n",
        "ë³´ìƒ í•¨ìˆ˜ ğŸ’°:  \n",
        "- ìˆ˜ì§ ë¸”ë¡ í•˜ë‚˜ë¥¼ í†µê³¼í•  ë•Œë§ˆë‹¤ +1ì˜ ì–‘ì˜ ë³´ìƒì„ ì–»ìŒ.  \n",
        "- í„°ë¯¸ë„ ìƒíƒœì— ë„ë‹¬í•  ë•Œë§ˆë‹¤ -1ì˜ ìŒì˜ ë³´ìƒì„ ë°›ìŒ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV1466QP8crz"
      },
      "source": [
        "### ìƒˆë¡œìš´ ì •ì±… ì •ì˜ ğŸ§   \n",
        "- í™˜ê²½ì´ ë” ë³µì¡í•˜ë¯€ë¡œ ë” ê¹Šì€ ì‹ ê²½ë§ì´ í•„ìš”í•¨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1eBkCiX2X_S"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        # ì„¸ ê°œì˜ ë ˆì´ì–´ ì •ì˜\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # ìˆœì „íŒŒ ê³¼ì • ì •ì˜ \n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47iuAFqV8Ws-"
      },
      "source": [
        "#### í•´ê²° ë°©ë²•"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrNuVcHC8Xu7"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, h_size)\n",
        "        self.fc3 = nn.Linear(h_size, a_size)\n",
        "        self.swish = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.swish(self.fc1(x))\n",
        "        x = self.swish(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM1QiGCSbBkM"
      },
      "source": [
        "### í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ì˜ âš™ï¸  \n",
        "- ì´ í™˜ê²½ì€ ë” ë³µì¡í•˜ë¯€ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•´ì•¼ í•¨  \n",
        "- íŠ¹íˆ ì€ë‹‰ì¸µ í¬ê¸°ëŠ” ë” ë§ì€ ë‰´ëŸ°ì´ í•„ìš”í•¨  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0uujOR_ypB6"
      },
      "outputs": [],
      "source": [
        "pixelcopter_hyperparameters = {\n",
        "    \"h_size\": 128,\n",
        "    \"n_training_episodes\": 50000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 10000,\n",
        "    \"gamma\": 0.99,\n",
        "    \"lr\": 5e-5,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyvXTJWm9GJG"
      },
      "source": [
        "### í›ˆë ¨ ì‹œì‘  \n",
        "- ì´ì œ ì—ì´ì „íŠ¸ë¥¼ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆë‹¤ ğŸ”¥."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mM2P_ckysFE"
      },
      "outputs": [],
      "source": [
        "# ì •ì±… ìƒì„± ë° ë””ë°”ì´ìŠ¤ì— ë°°ì¹˜\n",
        "torch.manual_seed(50)\n",
        "pixelcopter_policy = Policy(pixelcopter_hyperparameters[\"state_space\"], pixelcopter_hyperparameters[\"action_space\"], pixelcopter_hyperparameters[\"h_size\"]).to(device)\n",
        "pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1HEqP-fy-Rf"
      },
      "outputs": [],
      "source": [
        "scores = reinforce(pixelcopter_policy,\n",
        "                   pixelcopter_optimizer,\n",
        "                   pixelcopter_hyperparameters[\"n_training_episodes\"], \n",
        "                   pixelcopter_hyperparameters[\"max_t\"],\n",
        "                   pixelcopter_hyperparameters[\"gamma\"], \n",
        "                   1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_reward, std_reward = evaluate_agent(eval_env, \n",
        "                                            pixelcopter_hyperparameters[\"max_t\"],\n",
        "                                            pixelcopter_hyperparameters[\"n_evaluation_episodes\"], \n",
        "                                            pixelcopter_policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(mean_reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kwFQ-Ip85BE"
      },
      "source": [
        "### í•™ìŠµëœ ëª¨ë¸ì„ í—ˆë¸Œì— ê²Œì‹œí•˜ì„¸ìš” ğŸ”¥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6PtB7LRbTKWK"
      },
      "outputs": [],
      "source": [
        "repo_id = \"\" #TODO ìì‹ ì˜ ì €ì¥ì†Œ IDë¥¼ ì •ì˜í•˜ì„¸ìš” {username/Reinforce-{model-id}}\n",
        "push_to_hub(\n",
        "    repo_id,\n",
        "    pixelcopter_policy,          # ì €ì¥í•  ëª¨ë¸\n",
        "    pixelcopter_hyperparameters, # í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
        "    eval_env,                    # í‰ê°€ í™˜ê²½\n",
        "    video_fps=30\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VDcJ29FcOyb"
      },
      "source": [
        "## ëª‡ ê°€ì§€ ì¶”ê°€ ë„ì „ ê³¼ì œ ğŸ†\n",
        "ê°€ì¥ ì¢‹ì€ í•™ìŠµ ë°©ë²•ì€ **ì§ì ‘ ì‹œë„í•´ë³´ëŠ” ê²ƒ**ì…ë‹ˆë‹¤! ë³´ì…¨ë“¯ì´ í˜„ì¬ ì—ì´ì „íŠ¸ì˜ ì„±ëŠ¥ì´ ì¢‹ì§€ ì•ŠìŠµë‹ˆë‹¤. ì²« ë²ˆì§¸ ì œì•ˆìœ¼ë¡œëŠ” ë” ë§ì€ ë‹¨ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ë” ë‚˜ì€ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ì•„ë³´ì„¸ìš”.\n",
        "\n",
        "[ë¦¬ë”ë³´ë“œ](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)ì—ì„œ ì—¬ëŸ¬ë¶„ì˜ ì—ì´ì „íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì •ìƒì— ì˜¤ë¥¼ ìˆ˜ ìˆì„ê¹Œìš”?\n",
        "\n",
        "ì´ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ëª‡ ê°€ì§€ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n",
        "* ë” ë§ì€ ë‹¨ê³„ í•™ìŠµ\n",
        "* ë™ë£Œë“¤ì´ ë¬´ì—‡ì„ í–ˆëŠ”ì§€ ë³´ë©´ì„œ ë‹¤ë¥¸ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì‹œë„í•´ ë³´ì„¸ìš” ğŸ‘‰ https://huggingface.co/models?other=reinforce\n",
        "* **ìƒˆë¡­ê²Œ í•™ìŠµëœ ëª¨ë¸ì„** Hubì— í‘¸ì‹œí•˜ê¸° ğŸ”¥\n",
        "* **ë” ë³µì¡í•œ í™˜ê²½ì— ëŒ€í•œ êµ¬í˜„ ê°œì„ ** (ì˜ˆë¥¼ ë“¤ì–´, ê´€ì°°ê°’ìœ¼ë¡œ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë„¤íŠ¸ì›Œí¬ë¥¼ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì€ ì–´ë–¨ê¹Œìš”?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x62pP0PHdA-y"
      },
      "source": [
        "________________________________________________________________________\n",
        "\n",
        "**ì´ ìœ ë‹›ì„ ë§ˆì¹˜ì‹  ê²ƒì„ ì¶•í•˜ë“œë¦½ë‹ˆë‹¤**! ë§ì€ ì •ë³´ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.\n",
        "ê·¸ë¦¬ê³  íŠœí† ë¦¬ì–¼ì„ ë§ˆì¹˜ì‹  ê²ƒë„ ì¶•í•˜ë“œë¦½ë‹ˆë‹¤. PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ì²« ë²ˆì§¸ ì‹¬ì¸µ ê°•í™” í•™ìŠµ ì—ì´ì „íŠ¸ë¥¼ ì²˜ìŒë¶€í„° ì½”ë”©í•˜ê³  Hubì— ê³µìœ í•˜ì…¨ìŠµë‹ˆë‹¤ ğŸ¥³.\n",
        "\n",
        "**ë” ë³µì¡í•œ í™˜ê²½ì— ëŒ€í•œ êµ¬í˜„ì„ ê°œì„ í•˜ì—¬** ì´ ìœ ë‹›ì„ ë°˜ë³µí•˜ëŠ” ê²ƒì„ ì£¼ì €í•˜ì§€ ë§ˆì„¸ìš” (ì˜ˆë¥¼ ë“¤ì–´, ê´€ì°°ê°’ìœ¼ë¡œ í”„ë ˆì„ì„ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë„¤íŠ¸ì›Œí¬ë¥¼ ì»¨ë³¼ë£¨ì…˜ ì‹ ê²½ë§ìœ¼ë¡œ ë³€ê²½í•˜ëŠ” ê²ƒì€ ì–´ë–¨ê¹Œìš”?).\n",
        "\n",
        "ë‹¤ìŒ ìœ ë‹›ì—ì„œëŠ” Unity í™˜ê²½ì—ì„œ ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚¤ë©´ì„œ **Unity MLAgentsì— ëŒ€í•´ ë” ìì„¸íˆ ë°°ìš¸ ê²ƒì…ë‹ˆë‹¤**. ì´ë¡œì¨ **ëˆˆì‹¸ì›€ê³¼ ì¶•êµ¬ ê²Œì„ì—ì„œ ë‹¤ë¥¸ ì—ì´ì „íŠ¸ë“¤ê³¼ ê²½ìŸí•˜ê¸° ìœ„í•´ ì—ì´ì „íŠ¸ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” AI ëŒ€ AI ì±Œë¦°ì§€**ì— ì°¸ì—¬í•  ì¤€ë¹„ê°€ ë  ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "ì¬ë¯¸ìˆì„ ê²ƒ ê°™ë‚˜ìš”? ë‹¤ìŒì— ë§Œë‚˜ìš”!\n",
        "\n",
        "ë§ˆì§€ë§‰ìœ¼ë¡œ, **ê°•ì¢Œì— ëŒ€í•´ ì–´ë–»ê²Œ ìƒê°í•˜ì‹œëŠ”ì§€, ì–´ë–»ê²Œ ê°œì„ í•  ìˆ˜ ìˆëŠ”ì§€** ë“£ê³  ì‹¶ìŠµë‹ˆë‹¤. í”¼ë“œë°±ì´ ìˆìœ¼ì‹œë©´ ğŸ‘‰ [ì´ ì–‘ì‹ì„ ì‘ì„±í•´ì£¼ì„¸ìš”](https://forms.gle/BzKXWzLAGZESGNaE9)\n",
        "\n",
        "ìœ ë‹› 5ì—ì„œ ë§Œë‚˜ìš”! ğŸ”¥\n",
        "\n",
        "### ê³„ì† ë°°ìš°ê³ , ë©‹ì§€ê²Œ ì§€ë‚´ì„¸ìš” ğŸ¤—"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "BPLwsPajb1f8",
        "L_WSo0VUV99t",
        "mjY-eq3eWh9O",
        "JoTC9o2SczNn",
        "gfGJNZBUP7Vn",
        "YB0Cxrw1StrP",
        "47iuAFqV8Ws-",
        "x62pP0PHdA-y"
      ],
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "learn2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
