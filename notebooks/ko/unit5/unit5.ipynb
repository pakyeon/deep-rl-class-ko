{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pakyeon/deep-rl-class-ko/blob/main/notebooks/ko/unit5/unit5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2D3NL_e4crQv"
      },
      "source": [
        "# 유닛 5: ML-Agents 소개\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97ZiytXEgqIz"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/thumbnail.png\" alt=\"Thumbnail\"/>\n",
        "\n",
        "이 노트북에서는 ML-Agents에 대해 배우고 두 개의 에이전트를 훈련시킬 것입니다.\n",
        "\n",
        "- 첫 번째 에이전트는 **생성되는 표적에 눈덩이를 발사하는 법**을 학습합니다.  \n",
        "- 두 번째 에이전트는 피라미드를 생성하기 위해 버튼을 누르고, 피라미드로 이동해 넘어뜨린 후, **맨 위에 있는 금 벽돌로 이동해야** 합니다. 이를 위해 환경을 탐색해야 하며, 호기심(Curiosity)이라는 기법을 사용할 것입니다.\n",
        "\n",
        "그 후에는 **브라우저에서 직접 에이전트가 작동하는 모습을 볼 수 있습니다**.\n",
        "\n",
        "자격증 과정에 대한 자세한 정보는 다음 섹션을 확인하세요 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMYrDriDujzX"
      },
      "source": [
        "⬇️ **이 유닛을 마치면 여러분이 달성하게 될 예시입니다.** ⬇️"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBmFlh8suma-"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids.gif\" alt=\"Pyramids\"/>\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget.gif\" alt=\"SnowballTarget\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-cYE0K5iL-w"
      },
      "source": [
        "### 🎮 환경:\n",
        "\n",
        "- [피라미드](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Learning-Environment-Examples.md#pyramids)  \n",
        "- SnowballTarget\n",
        "\n",
        "### 📚 강화학습 라이브러리:\n",
        "\n",
        "- [ML-Agents](https://github.com/Unity-Technologies/ml-agents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEhtaFh9i31S"
      },
      "source": [
        "튜토리얼을 지속적으로 개선하고 있으니, **이 노트북에서 문제가 발견되면** [GitHub 저장소에 이슈를 등록해주세요](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7f63r3Yi5vE"
      },
      "source": [
        "## 이 노트북의 목표 🏆\n",
        "\n",
        "노트북을 완료하면 다음을 할 수 있습니다:\n",
        "\n",
        "- 환경 라이브러리인 **ML-Agents의 작동 방식**을 이해합니다.  \n",
        "- **Unity 환경에서 에이전트를 훈련시키는 방법**을 익힙니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viNzVbVaYvY3"
      },
      "source": [
        "## 이 노트북은 딥 강화학습 과정의 일부입니다  \n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p5HnEefISCB"
      },
      "source": [
        "이 무료 강의에서는 다음을 배울 수 있습니다:\n",
        "\n",
        "- 📖 **이론과 실습**을 통해 딥 강화학습(Deep Reinforcement Learning)을 학습합니다.  \n",
        "- 🧑‍💻 Stable Baselines3, RL Baselines3 Zoo, CleanRL, Sample Factory 2.0 같은 **유명한 Deep RL 라이브러리 사용법**을 익힙니다.  \n",
        "- 🤖 **다양한 환경에서 에이전트를 학습**시켜 봅니다.  \n",
        "\n",
        "그리고 더 많은 내용을 원한다면 📚 **전체 커리큘럼은 여기서 확인하세요** 👉 https://simoninithomas.github.io/deep-rl-course\n",
        "\n",
        "또한, 매 유닛이 공개될 때마다 링크와 도전 과제, 업데이트 내용을 받아보려면  \n",
        "**[수강 신청](http://eepurl.com/ic5ZUD)** 을 꼭 해주세요!  \n",
        "(이메일을 통해 정보를 보내드리기 위함입니다.)\n",
        "\n",
        "커뮤니티 및 강사진과의 교류를 원한다면  \n",
        "**디스코드 서버에 참여하세요** 👉🏻 https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-mo_6rXIjRi"
      },
      "source": [
        "## 사전 준비 사항 🏗️  \n",
        "노트북을 시작하기 전에 다음을 완료해야 합니다:\n",
        "\n",
        "🔲 📚 **[유닛 5를 읽고 ML-Agents가 무엇이며 어떻게 작동하는지 학습하세요](https://huggingface.co/deep-rl-course/unit5/introduction)** 🤗"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYO1uD5Ujgdh"
      },
      "source": [
        "# 에이전트를 훈련시켜 봅시다 🚀\n",
        "\n",
        "**자격증 과정을 위한 실습 인증을 받으려면, 훈련된 모델을 Hugging Face Hub에 업로드하기만 하면 됩니다.**  \n",
        "이 유닛에서는 특정 성과를 달성하지 않아도 인증이 가능합니다.  \n",
        "하지만 좋은 성과를 내고 싶다면 다음을 목표로 해보세요:\n",
        "\n",
        "- `Pyramids` : 평균 보상 = 1.75  \n",
        "- `SnowballTarget` : 평균 보상 = 15 또는 에피소드당 30개의 표적 명중"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DssdIjk_8vZE"
      },
      "source": [
        "## GPU 설정하기 💪  \n",
        "- **에이전트의 훈련을 가속하기 위해 GPU를 사용할 예정입니다**. 이를 위해 `런타임 > 런타임 유형 변경`으로 이동하세요.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTfCXHy68xBv"
      },
      "source": [
        "- `하드웨어 가속기 > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 리포지토리 클론하기 🔽\n",
        "\n",
        "- **ML-Agents**가 포함된 리포지토리를 클론해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# 저장소 복제(3분 소요)\n",
        "!git clone --depth 1 https://github.com/Unity-Technologies/ml-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 가상 환경 설정하기 🔽  \n",
        "- **ML-Agents**가 Colab에서 정상적으로 실행되기 위해서는, Colab의 Python 버전이 해당 라이브러리의 Python 요구 사항을 충족해야 합니다.\n",
        "\n",
        "- 지원되는 Python 버전은 `setup.py` 파일의 `python_requires` 항목에서 확인할 수 있습니다. 이 파일들은 **ML-Agents** 라이브러리를 사용할 수 있도록 설정하는 데 필요하며, 다음 위치에 있습니다:  \n",
        "  - `/content/ml-agents/ml-agents/setup.py`  \n",
        "  - `/content/ml-agents/ml-agents-envs/setup.py`\n",
        "\n",
        "- 현재 Colab의 Python 버전(`!python --version`으로 확인 가능)은 라이브러리의 `python_requires` 요구 사항과 일치하지 않기 때문에, 설치가 조용히 실패하고 나중에 아래와 같은 오류가 발생할 수 있습니다:  \n",
        "  - `/bin/bash: line 1: mlagents-learn: command not found`  \n",
        "  - `/bin/bash: line 1: mlagents-push-to-hf: command not found`\n",
        "\n",
        "- 이를 해결하기 위해, **ML-Agents** 라이브러리와 호환되는 Python 버전으로 가상 환경을 생성합니다.\n",
        "\n",
        "`참고:` *향후 호환성을 위해, 항상 설치 파일의 `python_requires` 항목을 확인하고, Colab의 Python 버전이 맞지 않을 경우 아래 스크립트에서 설정된 최대 지원 Python 버전으로 가상 환경을 구성하세요.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab의 현재 Python 버전 (ML-Agents와 호환되지 않음)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# virtualenv 설치 및 가상 환경 생성\n",
        "!pip install virtualenv\n",
        "!virtualenv myenv\n",
        "\n",
        "# Miniconda 다운로드 및 설치\n",
        "!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
        "!chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
        "!./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
        "\n",
        "# Miniconda 활성화 후 Python 3.10.12 버전 설치\n",
        "!source /usr/local/bin/activate\n",
        "!conda install -q -y --prefix /usr/local python=3.10.12 ujson  # 여기서 버전 명시\n",
        "\n",
        "# Python 및 conda 경로에 대한 환경 변수 설정\n",
        "!export PYTHONPATH=/usr/local/lib/python3.10/site-packages/\n",
        "!export CONDA_PREFIX=/usr/local/envs/myenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 새로운 가상 환경의 Python 버전 (ML-Agents와 호환됨)\n",
        "!python --version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 의존성 설치하기 🔽"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "# 리포지토리 내부로 이동하여 패키지 설치 (약 3분 소요)\n",
        "%cd ml-agents\n",
        "!pip3 install -e ./ml-agents-envs\n",
        "!pip3 install -e ./ml-agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5_7Ptd_kEcG"
      },
      "source": [
        "## SnowballTarget ⛄\n",
        "\n",
        "이 환경이 어떻게 동작하는지 다시 확인하고 싶다면 아래 링크를 참고하세요 👉  \n",
        "https://huggingface.co/deep-rl-course/unit5/snowball-target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRY5ufKUKfhI"
      },
      "source": [
        "### 환경 zip 파일을 `./training-envs-executables/linux/` 경로로 다운로드 및 이동  \n",
        "- 환경 실행 파일은 zip 파일로 제공됨  \n",
        "- 해당 파일을 다운로드하여 `./training-envs-executables/linux/` 경로에 배치해야 함  \n",
        "- Colab을 사용하므로, OS가 Ubuntu(Linux)이기 때문에 Linux 실행 파일을 사용함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Ls6_6eOKiA"
      },
      "outputs": [],
      "source": [
        "# 여기에서 training-envs-executables 및 linux 디렉토리를 생성함\n",
        "!mkdir ./training-envs-executables\n",
        "!mkdir ./training-envs-executables/linux"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekSh8LWawkB5"
      },
      "source": [
        "`wget`을 사용하여 https://github.com/huggingface/Snowball-Target 에서 SnowballTarget.zip 파일을 다운로드함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LosWO50wa77"
      },
      "outputs": [],
      "source": [
        "!wget \"https://github.com/huggingface/Snowball-Target/raw/main/SnowballTarget.zip\" -O ./training-envs-executables/linux/SnowballTarget.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LLVaEEK3ayi"
      },
      "source": [
        "실행 파일인 executable.zip을 압축 해제함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FPx0an9IAwO"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/SnowballTarget.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyumV5XfPKzu"
      },
      "source": [
        "파일에 접근할 수 있는지 확인함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdFsLJ11JvQf"
      },
      "outputs": [],
      "source": [
        "!chmod -R 755 ./training-envs-executables/linux/SnowballTarget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAuEq32Mwvtz"
      },
      "source": [
        "### SnowballTarget 설정 파일 정의  \n",
        "- ML-Agents에서는 **훈련 하이퍼파라미터를 config.yaml 파일에 정의**함  \n",
        "\n",
        "여러 하이퍼파라미터가 존재하며, 자세한 설명은 [공식 문서](https://github.com/Unity-Technologies/ml-agents/blob/release_20_docs/docs/Training-Configuration-File.md)를 참고할 것  \n",
        "\n",
        "따라서 `./content/ml-agents/config/ppo/` 경로에 `SnowballTarget.yaml` 설정 파일을 생성해야 함  \n",
        "\n",
        "아래는 `SnowballTarget.yaml`에 복사해 넣을 수 있는 초기 버전이며, **수정이 필요함**\n",
        "\n",
        "```\n",
        "behaviors:\n",
        "  SnowballTarget:\n",
        "    trainer_type: ppo\n",
        "    summary_freq: 10000\n",
        "    keep_checkpoints: 10\n",
        "    checkpoint_interval: 50000\n",
        "    max_steps: 200000\n",
        "    time_horizon: 64\n",
        "    threaded: false\n",
        "    hyperparameters:\n",
        "      learning_rate: 0.0003\n",
        "      learning_rate_schedule: linear\n",
        "      batch_size: 128\n",
        "      buffer_size: 2048\n",
        "      beta: 0.005\n",
        "      epsilon: 0.2\n",
        "      lambd: 0.95\n",
        "      num_epoch: 3\n",
        "    network_settings:\n",
        "      normalize: false\n",
        "      hidden_units: 256\n",
        "      num_layers: 2\n",
        "      vis_encode_type: simple\n",
        "    reward_signals:\n",
        "      extrinsic:\n",
        "        gamma: 0.99\n",
        "        strength: 1.0\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U3sRH4N4h_l"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballfight_config1.png\" alt=\"Config SnowballTarget\"/>\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballfight_config2.png\" alt=\"Config SnowballTarget\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJJdo_5AyoGo"
      },
      "source": [
        "실험을 위해 일부 하이퍼파라미터를 **수정해보는 것도 추천됨**. 각 하이퍼파라미터에 대한 자세한 설명은 Unity의 [공식 문서](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md)에서 확인 가능함.\n",
        "\n",
        "이제 설정 파일을 만들고 대부분의 하이퍼파라미터에 대한 이해도 되었으므로, **에이전트 훈련을 시작할 준비가 완료됨 🔥**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9fI555bO12v"
      },
      "source": [
        "### 에이전트 훈련  \n",
        "\n",
        "에이전트를 훈련시키기 위해서는 **`mlagents-learn` 명령어를 실행하고 환경 실행 파일을 지정**하면 됨  \n",
        "\n",
        "다음 네 가지 파라미터를 정의함:  \n",
        "\n",
        "1. `mlagents-learn <config>`: 하이퍼파라미터 설정 파일 경로  \n",
        "2. `--env`: 환경 실행 파일 경로  \n",
        "3. `--run_id`: 훈련 실행에 부여할 이름  \n",
        "4. `--no-graphics`: 훈련 중 시각화를 비활성화  \n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/mlagentslearn.png\" alt=\"MlAgents learn\"/>\n",
        "\n",
        "모델을 훈련시킬 때, 중단된 훈련을 이어서 하려면 `--resume` 플래그를 사용함  \n",
        "\n",
        "> `--resume` 사용 시 첫 실행은 실패할 수 있음. 에러가 발생하면 블록을 다시 실행하면 해결됨."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN32oWF8zPjs"
      },
      "source": [
        "훈련은 구성에 따라 **약 10~35분** 소요될 수 있음.  \n",
        "☕️ 잠시 쉬고 와도 좋음. 당신은 그럴 자격이 있어요 🤗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bS-Yh1UdHfzy"
      },
      "outputs": [],
      "source": [
        "!mlagents-learn ./config/ppo/SnowballTarget.yaml --env=./training-envs-executables/linux/SnowballTarget/SnowballTarget --run-id=\"SnowballTarget1\" --no-graphics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Vue94AzPy1t"
      },
      "source": [
        "### 에이전트를 🤗 Hub에 푸시하기\n",
        "\n",
        "- 이제 에이전트를 훈련했으니, **브라우저에서 플레이하는 것을 시각화할 수 있도록 Hub에 푸시할 준비가 되었습니다🔥.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izT6FpgNzZ6R"
      },
      "source": [
        "모델을 커뮤니티와 공유하려면 다음 세 단계를 더 따라야 합니다.\n",
        "\n",
        "1️⃣ (아직 하지 않았다면) HF 계정 생성 ➡ [https://huggingface.co/join](https://huggingface.co/join)\n",
        "\n",
        "2️⃣ 로그인한 후, Hugging Face 웹사이트에서 인증 토큰을 저장해야 합니다.\n",
        "- 새 토큰 생성 ([https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)) **쓰기 역할 포함**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n",
        "\n",
        "- 토큰 복사\n",
        "- 아래 셀을 실행하고 토큰 붙여넣기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKt2vsYoK56o"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSU9qD9_6dem"
      },
      "source": [
        "Google Colab이나 Jupyter Notebook을 사용하지 않는 경우, 아래 명령어를 대신 사용해야 합니다: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK4fPfnczunT"
      },
      "source": [
        "그런 다음 간단히 `mlagents-push-to-hf`를 실행하면 됩니다.\n",
        "\n",
        "그리고 4가지 매개변수를 정의합니다:\n",
        "\n",
        "1.  `--run-id`: 훈련 실행 ID의 이름입니다.\n",
        "2.  `--local-dir`: 에이전트가 저장된 위치로, results/\\<run\\_id 이름\\>입니다. 따라서 제 경우에는 results/First Training입니다.\n",
        "3.  `--repo-id`: 생성하거나 업데이트하려는 Hugging Face 저장소 이름입니다. 항상 \\<여러분의 huggingface 사용자 이름\\>/\\<저장소 이름\\> 형식입니다.\n",
        "    저장소가 존재하지 않으면 **자동으로 생성됩니다**\n",
        "4.  `--commit-message`: HF 저장소는 Git 저장소이므로 커밋 메시지를 정의해야 합니다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/mlagentspushtohub.png\" alt=\"Push to Hub\"/>\n",
        "\n",
        "예시:\n",
        "\n",
        "`!mlagents-push-to-hf  --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"ThomasSimonini/ppo-SnowballTarget\"  --commit-message=\"First Push\"`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAFzVB7OYj_H"
      },
      "outputs": [],
      "source": [
        "!mlagents-push-to-hf --run-id=\"SnowballTarget1\" --local-dir=\"./results/SnowballTarget1\" --repo-id=\"ThomasSimonini/ppo-SnowballTarget\" --commit-message=\"First Push\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGEFAIboLVc6"
      },
      "outputs": [],
      "source": [
        "!mlagents-push-to-hf  --run-id= # 실행 ID 추가  --local-dir= # 로컬 디렉토리  --repo-id= # 저장소 ID  --commit-message= # 커밋 메시지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yborB0850FTM"
      },
      "source": [
        "그렇지 않고 모든 것이 제대로 작동했다면 프로세스 끝에 다음과 같은 내용이 표시될 것입니다(URL은 다르겠지만 😆):\n",
        "\n",
        "```\n",
        "Your model is pushed to the hub. You can view your model here: https://huggingface.co/ThomasSimonini/ppo-SnowballTarget\n",
        "```\n",
        "\n",
        "이것은 여러분의 모델 링크이며, 사용 방법을 설명하는 모델 카드, Tensorboard 및 설정 파일이 포함되어 있습니다. **멋진 점은 이것이 Git 저장소라는 것입니다. 즉, 다른 커밋을 가질 수 있고, 새로운 푸시로 저장소를 업데이트하는 등을 할 수 있습니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uaon2cg0NrL"
      },
      "source": [
        "하지만 이제 가장 좋은 점이 나옵니다: **온라인에서 에이전트를 시각화할 수 있다는 것 👀.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMc4oOsE0QiZ"
      },
      "source": [
        "### 에이전트 플레이 시청하기 👀\n",
        "\n",
        "이 단계는 간단합니다:\n",
        "\n",
        "1. 여기로 이동합니다: https://huggingface.co/spaces/ThomasSimonini/ML-Agents-SnowballTarget\n",
        "\n",
        "2. 게임을 실행하고 오른쪽 하단 버튼을 클릭하여 전체 화면으로 만듭니다\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/snowballtarget_load.png\" alt=\"Snowballtarget load\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Djs8c5rR0Z8a"
      },
      "source": [
        "1. 1단계에서 사용자 이름(사용자 이름은 대소문자를 구분합니다. 예를 들어 제 사용자 이름은 ThomasSimonini이지 thomassimonini 또는 ThOmasImoNInI가 아닙니다)을 입력하고 검색 버튼을 클릭합니다.\n",
        "\n",
        "2. 2단계에서 모델 저장소를 선택합니다.\n",
        "\n",
        "3. 3단계에서 **다시 플레이할 모델을 선택합니다**:\n",
        "  - 500000 타임스텝마다 모델을 저장했기 때문에 여러 개가 있습니다.\n",
        "  - 하지만 최신 모델을 원하므로 `SnowballTarget.onnx`를 선택합니다.\n",
        "\n",
        "👉 **다양한 모델 스텝으로 시도하여 에이전트의 개선 사항을 보는 것**이 좋습니다.\n",
        "\n",
        "그리고 Discord의 #rl-i-made-this 채널에 에이전트가 얻은 최고 점수를 마음껏 공유해주세요 🔥\n",
        "\n",
        "이제 Pyramids라고 불리는 더 어려운 환경을 시도해 봅시다..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVMwRi4y_tmx"
      },
      "source": [
        "## 피라미드 🏆\n",
        "\n",
        "### 환경 zip 파일을 `./training-envs-executables/linux/`로 다운로드 및 이동\n",
        "- 우리의 환경 실행 파일은 zip 파일 안에 있습니다.\n",
        "- 이 파일을 다운로드하여 `./training-envs-executables/linux/`에 놓아야 합니다.\n",
        "- Colab을 사용하기 때문에 Linux 실행 파일을 사용합니다. Colab 머신의 OS는 Ubuntu(Linux)이기 때문입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2C48SGZjZYw"
      },
      "source": [
        "`wget`을 사용하여 [https://www.google.com/search?q=https://huggingface.co/spaces/unity/ML-Agents-Pyramids/resolve/main/Pyramids.zip%EC%97%90%EC%84%9C](https://www.google.com/search?q=https://huggingface.co/spaces/unity/ML-Agents-Pyramids/resolve/main/Pyramids.zip%EC%97%90%EC%84%9C) Pyramids.zip 파일을 다운로드했습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eWh8Pl3sjZY2"
      },
      "outputs": [],
      "source": [
        "!wget \"https://huggingface.co/spaces/unity/ML-Agents-Pyramids/resolve/main/Pyramids.zip\" -O ./training-envs-executables/linux/Pyramids.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5LXPOPujZY3"
      },
      "source": [
        "실행 파일 zip 파일의 압축을 풉니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmNgFdXhjZY3"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip -d ./training-envs-executables/linux/ ./training-envs-executables/linux/Pyramids.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1jxwhrJjZY3"
      },
      "source": [
        "파일에 접근 가능한지 확인하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fDd03btjZY3"
      },
      "outputs": [],
      "source": [
        "!chmod -R 755 ./training-envs-executables/linux/Pyramids/Pyramids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqceIATXAgih"
      },
      "source": [
        "### PyramidsRND 설정 파일 수정하기\n",
        "- 사용자 지정 환경이었던 첫 번째 환경과 달리, **피라미드는 Unity 팀이 만들었습니다**.\n",
        "- 따라서 PyramidsRND 설정 파일은 이미 존재하며 ./content/ml-agents/config/ppo/PyramidsRND.yaml에 있습니다.\n",
        "- PyramidsRND의 \"RND\"가 왜 붙었는지 궁금해하실 수 있습니다. RND는 *random network distillation*의 약자로, 호기심 보상을 생성하는 한 가지 방법입니다. 이에 대해 더 자세히 알고 싶다면 이 기술을 설명하는 글을 작성했습니다: https://medium.com/data-from-the-trenches/curiosity-driven-learning-through-random-network-distillation-488ffd8e5938\n",
        "\n",
        "이 훈련을 위해 한 가지를 수정할 것입니다:\n",
        "- 전체 훈련 스텝 하이퍼파라미터는 너무 높습니다. 100만 훈련 스텝만으로도 벤치마크(평균 보상 = 1.75)에 도달할 수 있기 때문입니다.\n",
        "👉 이를 위해 config/ppo/PyramidsRND.yaml로 이동하여 **이것들을 max_steps 1000000으로 수정합니다.**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit7/pyramids-config.png\" alt=\"Pyramids config\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RI-5aPL7BWVk"
      },
      "source": [
        "실험 삼아 다른 하이퍼파라미터도 수정해 보세요. Unity는 [각 하이퍼파라미터를 설명하는 아주 좋은 문서를 여기에서 제공합니다](https://github.com/Unity-Technologies/ml-agents/blob/main/docs/Training-Configuration-File.md).\n",
        "\n",
        "이제 에이전트를 훈련할 준비가 되었습니다 🔥."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5hr1rvIBdZH"
      },
      "source": [
        "### 에이전트 훈련하기\n",
        "\n",
        "훈련은 머신에 따라 30분에서 45분 정도 소요될 것입니다. ☕️ 한 잔 하세요. 그럴 자격이 있습니다 🤗."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXi4-IaHBhqD"
      },
      "outputs": [],
      "source": [
        "!mlagents-learn ./config/ppo/PyramidsRND.yaml --env=./training-envs-executables/linux/Pyramids/Pyramids --run-id=\"Pyramids Training\" --no-graphics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "txonKxuSByut"
      },
      "source": [
        "### 에이전트를 🤗 Hub에 푸시하기\n",
        "\n",
        "- 이제 에이전트를 훈련했으니, **브라우저에서 플레이하는 것을 시각화할 수 있도록 Hub에 푸시할 준비가 되었습니다🔥.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiEQbv7rB4mU"
      },
      "outputs": [],
      "source": [
        "!mlagents-push-to-hf  --run-id= # 실행 ID 추가  --local-dir= # 로컬 디렉토리  --repo-id= # 저장소 ID  --commit-message= # 커밋 메시지"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aZfgxo-CDeQ"
      },
      "source": [
        "### 에이전트 플레이 시청하기 👀\n",
        "\n",
        "👉 https://huggingface.co/spaces/unity/ML-Agents-Pyramids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGG_oq2n0wjB"
      },
      "source": [
        "### 🎁 보너스: 다른 환경에서 훈련해 보는 건 어떠세요?\n",
        "이제 MLAgents를 사용하여 에이전트를 훈련하는 방법을 알게 되었으니, **다른 환경에서 시도해 보는 건 어떨까요?**\n",
        "\n",
        "MLAgents는 17가지의 다양한 환경을 제공하며, 저희는 몇 가지 사용자 지정 환경을 구축하고 있습니다. 배우는 가장 좋은 방법은 직접 시도해보는 것입니다. 즐거운 시간 보내세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSAkJxSr0z6-"
      },
      "source": [
        "![cover](https://miro.medium.com/max/1400/0*xERdThTRRM2k_U9f.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiyF4FX-04JB"
      },
      "source": [
        "Unity 공식 환경 전체 목록은 여기에서 확인할 수 있습니다 👉 https://github.com/Unity-Technologies/ml-agents/blob/develop/docs/Learning-Environment-Examples.md\n",
        "\n",
        "에이전트를 시각화할 수 있는 데모는 여기 있습니다 👉 https://huggingface.co/unity\n",
        "\n",
        "현재 통합된 환경은 다음과 같습니다:\n",
        "- [Worm](https://huggingface.co/spaces/unity/ML-Agents-Worm) 데모: **벌레가 기어가도록** 가르칩니다.\n",
        "- [Walker](https://huggingface.co/spaces/unity/ML-Agents-Walker) 데모: 에이전트에게 **목표를 향해 걷도록** 가르칩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI6dPWmh064H"
      },
      "source": [
        "오늘 할 내용은 여기까지입니다. 이 튜토리얼을 마치신 것을 축하드립니다!\n",
        "\n",
        "배우는 가장 좋은 방법은 연습하고 시도해보는 것입니다. 다른 환경에서 시도해 보는 건 어떨까요? ML-Agents에는 17가지의 다른 환경이 있지만, 여러분만의 환경을 만들 수도 있습니다. 문서를 확인하고 즐거운 시간 보내세요!\n",
        "\n",
        "Unit 6에서 만나요 🔥,\n",
        "\n",
        "## 계속 배우고, 멋지게 지내세요 🤗"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "learn2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
