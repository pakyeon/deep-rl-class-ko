{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pakyeon/deep-rl-class-ko/blob/main/notebooks/ko/unit3/unit3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7xBVPzoXxOg"
      },
      "source": [
        "# 유닛 3: RL Baselines3 Zoo를 사용한 아타리 게임 👾에서의 Deep Q-러닝\n",
        "\n",
        "\\<img src=\"[invalid URL removed]\" alt=\"Unit 3 Thumbnail\"\\>\n",
        "\n",
        "이 노트북에서는 [RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)를 사용하여 스페이스 인베이더를 플레이하는 **Deep Q-러닝 에이전트를 훈련시킬 것입니다**. RL Baselines3 Zoo는 [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/) 기반의 훈련 프레임워크로, 에이전트 훈련, 평가, 하이퍼파라미터 튜닝, 결과 플롯팅, 비디오 녹화 스크립트를 제공합니다.\n",
        "\n",
        "저희는 Double-DQN, Dueling-DQN, Prioritized Experience Replay와 같은 확장 기능이 없는 [RL-Baselines-3 Zoo 통합 버전인 바닐라 Deep Q-러닝](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)을 사용합니다.\n",
        "\n",
        "⬇️ 다음은 **여러분들이 달성할 것**의 예시입니다 ⬇️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9S713biXntc"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykJiGevCMVc5"
      },
      "source": [
        "### 🎮 환경:\n",
        "\n",
        "- [SpacesInvadersNoFrameskip-v4](https://gymnasium.farama.org/environments/atari/space_invaders/)\n",
        "\n",
        "스페이스 인베이더 버전 간의 차이점은 여기 👉 [https://gymnasium.farama.org/environments/atari/space_invaders/#variants](https://gymnasium.farama.org/environments/atari/space_invaders/#variants) 에서 확인할 수 있습니다.\n",
        "\n",
        "### 📚 RL 라이브러리:\n",
        "\n",
        "- [RL-Baselines3-Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wciHGjrFYz9m"
      },
      "source": [
        "## 이 노트북의 목표 🏆\n",
        "이 노트북의 끝에서는 다음을 할 수 있게 됩니다:\n",
        "- **RL Baselines3 Zoo가 어떻게 작동하는지** 더 깊이 이해할 수 있습니다.\n",
        "- 멋진 비디오 리플레이 및 평가 점수와 함께 **훈련된 에이전트와 코드를 허브에 푸시**할 수 있습니다 🔥."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsnP0rjxMn1e"
      },
      "source": [
        "## 이 노트북은 Deep Reinforcement Learning Course에서 가져온 것입니다.\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw6fJHIAZd-J"
      },
      "source": [
        "이 무료 강의에서는 다음을 배울 수 있습니다:\n",
        "\n",
        "- 📖 **이론과 실습**을 통해 딥 강화학습(Deep Reinforcement Learning)을 학습합니다.  \n",
        "- 🧑‍💻 Stable Baselines3, RL Baselines3 Zoo, CleanRL, Sample Factory 2.0 같은 **유명한 Deep RL 라이브러리 사용법**을 익힙니다.  \n",
        "- 🤖 **다양한 환경에서 에이전트를 학습**시켜 봅니다.  \n",
        "\n",
        "그리고 더 많은 내용을 원한다면 📚 **전체 커리큘럼은 여기서 확인하세요** 👉 https://simoninithomas.github.io/deep-rl-course\n",
        "\n",
        "또한, 매 유닛이 공개될 때마다 링크와 도전 과제, 업데이트 내용을 받아보려면  \n",
        "**[수강 신청](http://eepurl.com/ic5ZUD)** 을 꼭 해주세요!  \n",
        "(이메일을 통해 정보를 보내드리기 위함입니다.)\n",
        "\n",
        "커뮤니티 및 강사진과의 교류를 원한다면  \n",
        "**디스코드 서버에 참여하세요** 👉🏻 https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0vgANIBBZg1p"
      },
      "source": [
        "## 필수 조건 🏗️\n",
        "노트북을 시작하기 전에 다음을 해야 합니다:\n",
        "\n",
        "🔲 📚 **[유닛 3을 읽고 Deep Q-러닝 학습](https://huggingface.co/deep-rl-course/unit3/introduction)** 🤗"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kszpGFaRVhq"
      },
      "source": [
        "우리는 튜토리얼을 지속적으로 개선하고 있습니다. 따라서 **이 노트북에서 문제가 발견되면**, [GitHub 저장소에 이슈를 등록해주세요](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR0jZtYreSI5"
      },
      "source": [
        "# 딥 Q-러닝 에이전트를 Atari 'Space Invaders' 👾 게임에 훈련시키고 Hub에 업로드해보자.\n",
        "\n",
        "학생들은 **개인 컴퓨터에서 실행하는 것보다 Google Colab을 사용하는 것을 강력히 권장합니다**.\n",
        "\n",
        "Google Colab을 사용하면 **환경 설정 같은 기술적인 부분에 신경 쓰지 않고 학습과 실험에 집중할 수 있습니다**.\n",
        "\n",
        "이 실습을 인증 과정에 제출하려면, 훈련된 모델을 Hub에 업로드하고 **결과가 >= 200** 이상이어야 합니다.\n",
        "\n",
        "결과를 확인하려면 리더보드에서 본인의 모델을 찾고, **결과 = 평균 보상 - 보상의 표준편차**를 계산하세요.\n",
        "\n",
        "인증 과정에 대한 자세한 정보는 다음 링크를 확인하세요 👉 https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nc8BnyVEc3Ys"
      },
      "source": [
        "## 팁 💡  \n",
        "이 Colab 노트북은 **Google Drive에 복사본을 저장한 후 실행하는 것이 좋습니다**. 이렇게 하면 **타임아웃이 발생해도** Google Drive에 저장된 노트북을 통해 처음부터 다시 작성할 필요가 없습니다.\n",
        "\n",
        "복사하려면 `Ctrl + S` 또는 `파일 > Google Drive에 사본 저장`을 클릭하세요.\n",
        "\n",
        "또한, 우리는 **1M 타임스텝으로 약 90분 동안 훈련**할 예정입니다.  \n",
        "어떤 GPU를 사용하는지 알고 싶다면 `!nvidia-smi`를 입력하세요.\n",
        "\n",
        "만약 1천만 스텝처럼 더 길게 훈련하고 싶다면, **약 9시간이 소요되며 Colab이 타임아웃될 수 있습니다**. 그런 경우에는 로컬 컴퓨터(또는 다른 환경)에서 실행하는 것을 추천합니다.  \n",
        "노트북을 다운로드하려면 `파일 > 다운로드`를 클릭하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU4FVzaoM6fC"
      },
      "source": [
        "## GPU 설정하기 💪  \n",
        "- **에이전트의 훈련을 가속하기 위해 GPU를 사용할 예정입니다**. 이를 위해 `런타임 > 런타임 유형 변경`으로 이동하세요.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV0NyFdQM9ZG"
      },
      "source": [
        "- `하드웨어 가속기 > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS_cVefO-aYg"
      },
      "source": [
        "# RL-Baselines3 Zoo 및 그 의존성 설치하기 📚\n",
        "\n",
        "`ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.`  \n",
        "**이 오류 메시지가 보여도 정상이며, 치명적인 오류가 아닙니다.** 버전 간 충돌이 있을 수 있지만, 필요한 패키지들은 정상적으로 설치됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1A_E4z3awa_"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/DLR-RM/rl-baselines3-zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_MllY6Om1eI"
      },
      "outputs": [],
      "source": [
        "!apt-get install swig cmake ffmpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S9mJiKg6SqC"
      },
      "source": [
        "Atari 게임을 Gymnasium에서 사용하려면 `atari` 패키지를 설치해야 합니다. 그리고 ROM 파일(게임 파일)을 다운로드하려면 `accept-rom-license`에 동의해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsRP-lX1_2fC"
      },
      "outputs": [],
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install gymnasium[accept-rom-license]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTpYcVZVMzUI"
      },
      "source": [
        "## 가상 디스플레이 생성 🔽\n",
        "\n",
        "노트북을 실행하는 동안 리플레이 비디오를 생성해야 합니다. 이를 위해 코랩에서는 **환경을 렌더링(그리고 프레임을 기록)할 수 있도록 가상 화면이 필요합니다.**\n",
        "\n",
        "따라서 다음 셀은 라이브러리를 설치하고 가상 화면 🖥을 생성 및 실행합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV6wjQ7Be7p5"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!apt install python-opengl\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE5JWP5rQIKf"
      },
      "outputs": [],
      "source": [
        "# 가상 디스플레이\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iPgzluo9z-u"
      },
      "source": [
        "## Deep Q-러닝 에이전트를 훈련시켜 스페이스 인베이더 플레이하기 👾\n",
        "\n",
        "RL-Baselines3-Zoo로 에이전트를 훈련시키려면 두 가지를 해야 합니다:\n",
        "\n",
        "1. `dqn.yml`이라는 훈련 하이퍼파라미터를 포함할 하이퍼파라미터 설정 파일을 생성합니다.\n",
        "\n",
        "다음은 템플릿 예시입니다:\n",
        "\n",
        "```\n",
        "SpaceInvadersNoFrameskip-v4:\n",
        "  env_wrapper:\n",
        "    - stable_baselines3.common.atari_wrappers.AtariWrapper\n",
        "  frame_stack: 4\n",
        "  policy: 'CnnPolicy'\n",
        "  n_timesteps: !!float 1e6\n",
        "  buffer_size: 100000\n",
        "  learning_rate: !!float 1e-4\n",
        "  batch_size: 32\n",
        "  learning_starts: 100000\n",
        "  target_update_interval: 1000\n",
        "  train_freq: 4\n",
        "  gradient_steps: 1\n",
        "  exploration_fraction: 0.1\n",
        "  exploration_final_eps: 0.01\n",
        "  # 값이 True인 경우, replay_buffer_kwargs에서 handle_timeout_termination을 비활성화해야 합니다.\n",
        "  optimize_memory_usage: False\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VjblFSVDQOj"
      },
      "source": [
        "여기서 다음을 알 수 있습니다:\n",
        "- 입력 데이터를 전처리하는 `Atari Wrapper`를 사용합니다 (프레임 축소, 그레이스케일, 4개 프레임 스태킹).\n",
        "- 프레임을 처리하기 위해 Convolutional 레이어를 사용하므로 `CnnPolicy`를 사용합니다.\n",
        "- 1000만 `n_timesteps` 동안 훈련합니다.\n",
        "- 메모리 (경험 리플레이) 크기는 100000이며, 이는 에이전트를 다시 훈련시키기 위해 저장한 경험 스텝의 양을 의미합니다.\n",
        "\n",
        "💡 제 조언은 **훈련 타임스텝을 1M으로 줄이는 것**입니다. P100에서 약 90분 소요됩니다. `!nvidia-smi` 명령어로 사용 중인 GPU를 확인할 수 있습니다. 1000만 스텝의 경우 약 9시간이 소요되어 코랩 시간이 초과될 수 있습니다. 로컬 컴퓨터나 다른 곳에서 실행하는 것을 추천합니다. *파일>다운로드*를 클릭하세요."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qTkbWrkECOJ"
      },
      "source": [
        "하이퍼파라미터 최적화 측면에서, 제가 드리는 조언은 다음 세 가지 하이퍼파라미터에 집중하는 것입니다:\n",
        "- `learning_rate`\n",
        "- `buffer_size (경험 메모리 크기)`\n",
        "- `batch_size`\n",
        "\n",
        "좋은 습관으로, 각 하이퍼파라미터가 무엇을 하는지 이해하기 위해 **문서를 확인해야 합니다**: [https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html#parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn8bRTHvERRL"
      },
      "source": [
        "2. 훈련을 시작하고 `logs` 폴더 📁에 모델을 저장합니다.\n",
        "\n",
        "- `--algo` 다음에 알고리즘을 정의하고, `-f` 다음에 모델을 저장할 위치를, `-c` 다음에 하이퍼파라미터 설정 파일 위치를 지정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xr1TVW4xfbz3"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.train --algo ________ --env SpaceInvadersNoFrameskip-v4  -f _________  -c _________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeChoX-3SZfP"
      },
      "source": [
        "#### 해결 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuocgdokSab9"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.train --algo dqn  --env SpaceInvadersNoFrameskip-v4 -f logs/ -c dqn.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dLomIiMKQaf"
      },
      "source": [
        "## 에이전트를 평가해봅시다 👀\n",
        "- RL-Baselines3-Zoo는 에이전트를 평가하는 파이썬 스크립트인 `enjoy.py`를 제공합니다. 대부분의 RL 라이브러리에서 평가 스크립트를 `enjoy.py`라고 부릅니다.\n",
        "- 5000 타임스텝 동안 평가해 봅시다 🔥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "co5um_KeKbBJ"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps _________  --folder logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q24K1tyWSj7t"
      },
      "source": [
        "#### 해결 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_uSmwGRSk0z"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.enjoy  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --no-render  --n-timesteps 5000  --folder logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liBeTltiHJtr"
      },
      "source": [
        "## 훈련된 모델을 Hub에 게시하기 🚀  \n",
        "훈련 후 좋은 결과를 확인했다면, 이제 한 줄의 코드로 훈련된 모델을 🤗 Hub에 게시할 수 있습니다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/space-invaders-model.gif\" alt=\"Space Invaders model\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezbHS1q3HYVV"
      },
      "source": [
        "`rl_zoo3.push_to_hub`를 사용하여 **에이전트를 평가하고, 리플레이를 기록하고, 모델 카드를 생성한 다음 허브에 푸시**합니다.\n",
        "\n",
        "이 방법을 사용하면:\n",
        "- **작업물을 선보일 수 있습니다** 🔥\n",
        "- **에이전트가 플레이하는 모습을 시각화할 수 있습니다** 👀\n",
        "- **다른 사람들이 사용할 수 있는 에이전트를 커뮤니티와 공유할 수 있습니다** 💾\n",
        "- **리더보드 🏆에 접속하여 학우들과 비교하여 에이전트 성능이 얼마나 좋은지 확인할 수 있습니다** 👉 [https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMSeZRBiHk6X"
      },
      "source": [
        "커뮤니티와 모델을 공유하려면 다음 세 가지 단계를 따라야 합니다:\n",
        "\n",
        "1️⃣ (아직 하지 않았다면) HF 계정을 생성하세요 ➡ https://huggingface.co/join\n",
        "\n",
        "2️⃣ 로그인한 후 Hugging Face 웹사이트에서 인증 토큰을 저장해야 합니다.  \n",
        "- [새 토큰 만들기](https://huggingface.co/settings/tokens) 페이지로 이동하여 **쓰기 권한(write role)** 으로 토큰을 생성하세요.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9O6FI0F8HnzE"
      },
      "source": [
        "- 토큰을 복사하세요.\n",
        "- 아래 셀을 실행하고 토큰을 붙여넣으세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ppu9yePwHrZX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RVEdunPHs8B"
      },
      "source": [
        "Google Colab이나 Jupyter Notebook을 사용하지 않는 경우, 아래 명령어를 대신 사용해야 합니다: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSLwdmvhHvjw"
      },
      "source": [
        "3️⃣ 이제 훈련된 에이전트를 🤗 허브에 푸시할 준비가 되었습니다 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW436XnhHw1H"
      },
      "source": [
        "이제 `push_to_hub.py` 파일을 실행해서 훈련된 에이전트를 Hub에 업로드해봅시다.\n",
        "\n",
        "- `--repo-name`: 업로드할 저장소의 이름  \n",
        "- `-orga`: 본인의 Hugging Face 사용자 이름  \n",
        "- `-f`: 훈련된 모델이 있는 폴더 경로 (이번 경우는 `logs`)\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit3/select-id.png\" alt=\"Select Id\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ygk2sEktTDEw"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name _____________________ -orga _____________________ -f logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otgpa0rhS9wR"
      },
      "source": [
        "#### 해결 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HQNlAXuEhci"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.push_to_hub  --algo dqn  --env SpaceInvadersNoFrameskip-v4  --repo-name dqn-SpaceInvadersNoFrameskip-v4  -orga ThomasSimonini  -f logs/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D4F5zsTTJ-L"
      },
      "source": [
        "###."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff89kd2HL1_s"
      },
      "source": [
        "축하합니다 🥳 RL-Baselines-3 Zoo를 사용해 첫 딥 Q-러닝 에이전트를 훈련하고 Hub에 업로드했습니다!  \n",
        "위 스크립트를 실행하면 아래와 같은 모델 저장소 링크가 표시되었을 겁니다:  \n",
        "예) https://huggingface.co/ThomasSimonini/dqn-SpaceInvadersNoFrameskip-v4\n",
        "\n",
        "이 링크에서 할 수 있는 일은 다음과 같습니다:\n",
        "\n",
        "- 오른쪽에서 **에이전트의 플레이 영상 미리보기**를 확인할 수 있습니다.  \n",
        "- \"Files and versions\"를 클릭하면 저장소의 모든 파일을 볼 수 있습니다.  \n",
        "- \"Use in stable-baselines3\"를 클릭하면 모델을 불러오는 코드 스니펫을 확인할 수 있습니다.  \n",
        "- 모델 카드(`README.md` 파일)에는 모델 설명과 사용한 하이퍼파라미터 정보가 포함되어 있습니다.\n",
        "\n",
        "내부적으로 Hub는 git 기반 저장소를 사용하므로(모르면 신경 안 써도 됩니다) 실험을 반복하면서 모델을 **새 버전으로 업데이트**할 수도 있습니다.\n",
        "\n",
        "**동료들과 에이전트 성능을 비교하려면 [리더보드](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)**를 확인해보세요 🏆"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyRKcCYY-dIo"
      },
      "source": [
        "## 강력하게 훈련된 모델 불러오기 🔥  \n",
        "- Stable-Baselines3 팀은 **150개 이상의 딥 강화학습 에이전트 모델을 Hub에 업로드**했습니다.\n",
        "\n",
        "여기에서 확인할 수 있습니다 👉 https://huggingface.co/sb3\n",
        "\n",
        "예시 모델들:  \n",
        "- Asteroids: https://huggingface.co/sb3/dqn-AsteroidsNoFrameskip-v4  \n",
        "- Beam Rider: https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4  \n",
        "- Breakout: https://huggingface.co/sb3/dqn-BreakoutNoFrameskip-v4  \n",
        "- Road Runner: https://huggingface.co/sb3/dqn-RoadRunnerNoFrameskip-v4  \n",
        "\n",
        "이제 Beam Rider를 플레이하는 에이전트를 불러와 봅시다:  \n",
        "👉 https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-9QVFIROI5Y"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZQNY_r6NJtC"
      },
      "source": [
        "1. `rl_zoo3.load_from_hub`을 사용하여 모델을 다운로드한 뒤, `rl_trained`라는 새 폴더에 저장합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdBNZHy0NGTR"
      },
      "outputs": [],
      "source": [
        "# Download model and save it into the logs/ folder\n",
        "!python -m rl_zoo3.load_from_hub --algo dqn --env BeamRiderNoFrameskip-v4 -orga sb3 -f rl_trained/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFt6hmWsNdBo"
      },
      "source": [
        "2. 5000개의 타임스텝에 대해 평가해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOxs0rNuN0uS"
      },
      "outputs": [],
      "source": [
        "!python -m rl_zoo3.enjoy --algo dqn --env BeamRiderNoFrameskip-v4 -n 5000  -f rl_trained/ --no-render"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxMDuDfPON57"
      },
      "source": [
        "여러분의 **Deep Q-러닝 에이전트가 BeamRiderNoFrameskip-v4를 플레이하도록 훈련시켜 보는 건 어떨까요? 🏆.**\n",
        "\n",
        "시도해보고 싶다면, https://huggingface.co/sb3/dqn-BeamRiderNoFrameskip-v4\\#hyperparameters 를 확인하세요. **모델 카드에 훈련된 에이전트의 하이퍼파라미터가 있습니다.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL_ZtUgpOuY6"
      },
      "source": [
        "하지만 하이퍼파라미터를 찾는 것은 어려운 작업일 수 있습니다. 다행히 다음 유닛에서 **Optuna를 사용하여 하이퍼파라미터를 최적화하는 방법🔥**을 배우게 될 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pqaco8W-huW"
      },
      "source": [
        "## 추가 도전 과제 🏆  \n",
        "**직접 시도해보는 것만큼 좋은 학습 방법은 없습니다!**\n",
        "\n",
        "[리더보드](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)에서 당신의 에이전트를 확인해보세요.  \n",
        "**1위를 차지할 수 있을까요?**\n",
        "\n",
        "다음은 에이전트를 훈련시켜볼 수 있는 환경 목록입니다:  \n",
        "- BeamRiderNoFrameskip-v4  \n",
        "- BreakoutNoFrameskip-v4  \n",
        "- EnduroNoFrameskip-v4  \n",
        "- PongNoFrameskip-v4  \n",
        "\n",
        "또한, **딥 Q-러닝을 직접 구현해보고 싶다면** CleanRL의 구현을 꼭 확인해보세요:  \n",
        "🔗 https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit4/atari-envs.gif\" alt=\"Environments\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paS-XKo4-kmu"
      },
      "source": [
        "________________________________________________________________________  \n",
        "이 챕터를 완료한 걸 축하합니다! 🎉\n",
        "\n",
        "혹시 아직도 많은 개념들이 헷갈린다면... **정상입니다!**  \n",
        "저도 그랬고, 강화학습을 공부한 모든 사람들이 겪는 과정이에요.\n",
        "\n",
        "다음으로 넘어가기 전에 시간을 들여 **내용을 확실히 이해하고, 추가 도전 과제도 꼭 시도해보세요.**  \n",
        "이러한 요소들을 잘 익히는 것이 튼튼한 기초를 만드는 데 중요합니다.\n",
        "\n",
        "다음 유닛에서는 **[Optuna](https://optuna.org/)** 에 대해 배울 예정입니다.  \n",
        "딥 강화학습에서 가장 중요한 과제 중 하나는 **좋은 훈련 하이퍼파라미터를 찾는 것**인데, Optuna는 이 과정을 자동화해주는 라이브러리입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WRx7tO7-mvC"
      },
      "source": [
        "### 이 강의는 여러분과 함께 만들어갑니다 👷🏿‍♀️\n",
        "\n",
        "마지막으로, 여러분의 피드백을 통해 이 강의를 지속적으로 개선하고자 합니다.  \n",
        "의견이 있다면 아래 폼을 작성해주세요 👉 https://forms.gle/3HgA7bEHwAmmLfwh9\n",
        "\n",
        "또한, **이 노트북에서 문제를 발견했다면**,  \n",
        "[GitHub 저장소에 이슈를 등록](https://github.com/huggingface/deep-rl-class/issues)해 주세요.  \n",
        "더 나은 튜토리얼을 만들기 위해 항상 노력하고 있습니다!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc3udPT-RcXc"
      },
      "source": [
        "보너스 유닛 2에서 만나요! 🔥"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS3Xerx0fIMV"
      },
      "source": [
        "### 계속 배우고, 멋진 모습을 유지하세요 🤗"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "learn2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
