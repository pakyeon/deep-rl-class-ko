{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njb_ProuHiOe"
      },
      "source": [
        "## 유닛 1: 나만의 첫 딥 강화학습 에이전트 훈련하기 🤖\n",
        "\n",
        "![Cover](https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/thumbnail.jpg)\n",
        "\n",
        "이 노트북에서는 여러분의 **첫 딥 강화학습(Deep Reinforcement Learning) 에이전트**, 즉 **달에 올바르게 착륙하는 방법을 학습하는 Lunar Lander 에이전트**를 훈련하게 됩니다 🌕  \n",
        "[Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)라는 딥 강화학습 라이브러리를 사용하여 모델을 만들고, 커뮤니티와 공유하며, 다양한 설정으로 실험해볼 수 있어요.\n",
        "\n",
        "⬇️ **단 몇 분 만에 여러분이 달성하게 될 결과 예시입니다.** ⬇️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PF46MwbZD00b"
      },
      "outputs": [],
      "source": [
        "%%html\n",
        "<video controls autoplay><source src=\"https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4\" type=\"video/mp4\"></video>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7oR6R-ZIbeS"
      },
      "source": [
        "### 환경 🎮\n",
        "\n",
        "- [LunarLander-v2](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n",
        "### 사용 라이브러리 📚\n",
        "\n",
        "- [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwEcFHe9RRZW"
      },
      "source": [
        "우리는 항상 튜토리얼을 개선하기 위해 노력하고 있으니, **이 노트북에서 문제가 발견되면**, [Github 저장소에 이슈를 등록해 주세요](https://github.com/huggingface/deep-rl-class/issues)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i6tjI2tHQ8j"
      },
      "source": [
        "## 이 노트북의 목표 🏆\n",
        "\n",
        "이 노트북을 마치면, 여러분은 다음을 할 수 있게 됩니다:\n",
        "\n",
        "- **Gymnasium**이라는 환경 라이브러리를 사용할 수 있게 됩니다.  \n",
        "- **Stable-Baselines3**이라는 딥 강화학습 라이브러리를 사용할 수 있게 됩니다.  \n",
        "- 훈련한 에이전트를 **허브(Hub)에 업로드**하고, 멋진 비디오 리플레이와 평가 점수도 함께 공유할 수 있게 됩니다 🔥."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff-nyJdzJPND"
      },
      "source": [
        "## 이 노트북은 Deep Reinforcement Learning Course에서 제공되었습니다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/deep-rl-course-illustration.jpg\" alt=\"Deep RL Course illustration\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p5HnEefISCB"
      },
      "source": [
        "이 무료 강의에서는 다음과 같은 내용을 배우게 됩니다:\n",
        "\n",
        "- 📖 **이론과 실습**을 통해 딥 강화학습(Deep Reinforcement Learning)을 공부합니다.  \n",
        "- 🧑‍💻 Stable Baselines3, RL Baselines3 Zoo, CleanRL, Sample Factory 2.0 같은 **유명한 딥 RL 라이브러리들을 활용하는 방법**을 배웁니다.  \n",
        "- 🤖 **특색 있는 환경에서 에이전트를 훈련**합니다.  \n",
        "- 🎓 과제의 80%를 완료하면 **수료 인증서**를 받을 수 있습니다.\n",
        "\n",
        "그리고 더 많은 것들이 준비되어 있어요!\n",
        "\n",
        "📚 강의 계획서를 확인해보세요 👉 [강의 커리큘럼 보기](https://simoninithomas.github.io/deep-rl-course)\n",
        "\n",
        "각 유닛이 공개될 때마다 링크와 도전 과제, 업데이트 정보를 받아보고 싶다면, 꼭 **<a href=\"http://eepurl.com/ic5ZUD\">강의에 등록하세요</a>** (이메일 주소는 이 목적에만 사용됩니다).  \n",
        "\n",
        "또한 **디스코드 서버**에 참여하면 커뮤니티 및 강사진과 소통할 수 있어요 👉🏻 https://discord.gg/ydHrjt3WP5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-mo_6rXIjRi"
      },
      "source": [
        "## 사전 준비 사항 🏗️\n",
        "\n",
        "이 노트북을 시작하기 전에, 다음을 완료해 주세요:\n",
        "\n",
        "🔲 📝 **[유닛 0 읽기](https://huggingface.co/deep-rl-course/unit0/introduction)**  \n",
        "해당 유닛은 강의에 대한 **전체적인 정보를 제공하고, 입문을 도와줍니다** 🤗\n",
        "\n",
        "🔲 📚 **강화학습의 기초 개념 이해하기**  \n",
        "(RL 프로세스, 보상 가설 등)  \n",
        "이해를 돕기 위해 [유닛 1을 읽어주세요](https://huggingface.co/deep-rl-course/unit1/introduction)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoeqMnr5LuYE"
      },
      "source": [
        "## 심층 강화 학습에 대한 간략한 요약 📚\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"The RL process\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcQYx9ynaFMD"
      },
      "source": [
        "이전 유닛에서 배운 내용을 간단히 복습해볼까요?\n",
        "\n",
        "- 강화학습(Reinforcement Learning)은 **행동을 통해 학습하는 계산적 접근 방식**입니다. 우리는 **시행착오(trial and error)를 통해 환경과 상호작용**하고, 그에 대한 보상(긍정적이든 부정적이든)을 받으면서 학습하는 에이전트를 설계합니다.\n",
        "\n",
        "- RL 에이전트의 목표는 **기대 누적 보상(expected cumulative reward)** 을 최대화하는 것입니다. 이는 강화학습의 기반이 되는 _보상 가설(reward hypothesis)_, 즉 모든 목표는 기대 누적 보상을 최대화하는 것으로 표현될 수 있다는 개념에서 비롯됩니다.\n",
        "\n",
        "- RL 과정은 **상태(state), 행동(action), 보상(reward), 다음 상태(next state)** 로 이어지는 **반복 루프** 입니다.\n",
        "\n",
        "- 기대 누적 보상을 계산하기 위해, **보상에 할인율(discount factor)** 을 적용합니다. 이는 게임 초반에 받는 보상이 먼 미래에 받는 보상보다 예측 가능성이 높고 더 중요한 것으로 간주되기 때문입니다.\n",
        "\n",
        "- RL 문제를 해결하기 위해서는 **최적 정책(optimal policy)** 을 찾아야 합니다. 정책(policy)은 현재 상태에서 어떤 행동을 취할지 결정해주는 AI의 \"두뇌\"이며, **기대 보상을 최대화해주는 정책이 최적 정책** 입니다.\n",
        "\n",
        "최적 정책을 찾는 방법은 두 가지가 있습니다:\n",
        "\n",
        "1. **정책을 직접 학습하는 방법**: 정책 기반(policy-based) 방법  \n",
        "2. **가치 함수를 학습하여** 상태마다 기대되는 보상을 추정하고, 이를 기반으로 정책을 정의하는 방법: 가치 기반(value-based) 방법\n",
        "\n",
        "- 마지막으로, 우리는 **딥 강화학습(Deep RL)** 에 대해 이야기했습니다. 여기서는 딥 신경망을 도입하여, **행동을 결정하는 정책(policy-based)** 이나 **상태의 가치를 추정하는 가치 함수(value-based)** 를 추정하게 됩니다. 그래서 이름에 “deep”이 붙은 것이죠."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDploC3jSH99"
      },
      "source": [
        "# 첫 번째 심층 강화 학습 에이전트를 훈련하고 허브에 업로드해 봅시다 🚀\n",
        "\n",
        "## 인증서 받기 🎓\n",
        "\n",
        "[인증 절차](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)를 위한 실습을 검증하려면, 훈련된 모델을 허브에 푸시하고 **결과가 >= 200**이어야 합니다.\n",
        "\n",
        "결과를 확인하려면 [리더보드](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)로 이동하여 모델을 찾으세요. **결과 = 평균 보상 - 보상 표준 편차**입니다.\n",
        "\n",
        "인증 절차에 대한 자세한 내용은 이 섹션을 확인하세요 👉 [https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process](https://huggingface.co/deep-rl-course/en/unit0/introduction#certification-process)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqzznTzhNfAC"
      },
      "source": [
        "## GPU 설정 💪\n",
        "\n",
        "  - **에이전트 훈련 속도를 높이기 위해 GPU를 사용**할 것입니다. 그렇게 하려면 `Runtime > Change Runtime type`으로 이동하세요.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step1.jpg\" alt=\"GPU Step 1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38HBd3t1SHJ8"
      },
      "source": [
        "- `하드웨어 가속기 > GPU`\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/gpu-step2.jpg\" alt=\"GPU Step 2\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeDAH0h0EBiG"
      },
      "source": [
        "## 종속성 설치 및 가상 화면 생성 🔽\n",
        "\n",
        "첫 번째 단계는 종속성을 설치하는 것입니다. 여러 가지를 설치할 것입니다.\n",
        "\n",
        "- `gymnasium[box2d]`: LunarLander-v2 환경 포함 🌛\n",
        "- `stable-baselines3[extra]`: 심층 강화 학습 라이브러리.\n",
        "- `huggingface_sb3`: Stable-baselines3가 Hugging Face 🤗 Hub에서 모델을 로드하고 업로드하는 데 필요한 추가 코드.\n",
        "\n",
        "작업을 더 쉽게 만들기 위해 이러한 모든 종속성을 설치하는 스크립트를 만들었습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQIGLPDkGhgG"
      },
      "outputs": [],
      "source": [
        "!apt install swig cmake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XaULfDZDvrC"
      },
      "outputs": [],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEKeXQJsQCYm"
      },
      "source": [
        "이 노트북에서는 리플레이 비디오를 생성해야 합니다. 이를 위해 Colab에서는 **환경을 렌더링(및 프레임 기록)하기 위해 가상 화면이 필요합니다**.  \n",
        "\n",
        "따라서 다음 셀에서는 가상 화면 라이브러리를 설치하고, 가상 화면을 생성 및 실행합니다 🖥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5f2cGkdP-mb"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y python3-opengl\n",
        "!sudo apt install ffmpeg\n",
        "!sudo apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCwBTAwAW9JJ"
      },
      "source": [
        "새로 설치된 라이브러리를 적용하려면 **런타임을 재시작해야 할 때가 있습니다**.  \n",
        "다음 셀은 **런타임을 강제로 중단시킬 것이므로, 재연결 후 여기부터 코드를 다시 실행해야 합니다**.  \n",
        "이 트릭 덕분에 **가상 화면을 정상적으로 실행할 수 있을 것입니다**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cYvkbef7XEMi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE5JWP5rQIKf"
      },
      "outputs": [],
      "source": [
        "# 가상 화면\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrgpVFqyENVf"
      },
      "source": [
        "## 패키지 임포트 📦\n",
        "\n",
        "추가로 임포트하는 라이브러리는 huggingface_hub입니다. **이를 통해 허브에서 훈련된 모델을 업로드하고 다운로드할 수 있습니다**.\n",
        "\n",
        "허깅 페이스 허브 🤗는 누구나 모델과 데이터셋을 공유하고 탐색할 수 있는 중앙 저장소 역할을 합니다. 버전 관리, 메트릭, 시각화 등의 기능을 제공하여 다른 사람들과 쉽게 협업할 수 있게 해줍니다.\n",
        "\n",
        "여기에서 사용 가능한 모든 딥 강화 학습 모델을 확인할 수 있습니다 👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cygWLPGsEQ0m"
      },
      "outputs": [],
      "source": [
        "import gymnasium\n",
        "\n",
        "from huggingface_sb3 import load_from_hub, package_to_hub\n",
        "from huggingface_hub import (\n",
        "    notebook_login,\n",
        ")  # 허브에 모델 업로드를 위해 Hugging Face 계정 로그인\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRqRuRUl8CsB"
      },
      "source": [
        "## Gymnasium 이해와 작동 방식 🤖\n",
        "\n",
        "🏋 우리의 환경을 포함하는 라이브러리는 Gymnasium이라고 합니다.\n",
        "**딥 강화 학습에서 Gymnasium을 많이 사용하게 될 것입니다.**\n",
        "\n",
        "Gymnasium은 **Gym 라이브러리의 새 버전**으로 [Farama Foundation에서 유지보수](https://farama.org/)하고 있습니다.\n",
        "\n",
        "Gymnasium 라이브러리는 두 가지를 제공합니다:\n",
        "\n",
        "- **RL 환경을 생성할 수 있는 인터페이스**\n",
        "- **환경 컬렉션** (gym-control, atari, box2D 등)\n",
        "\n",
        "예시를 살펴보기 전에 먼저 RL 루프를 상기해봅시다.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/RL_process_game.jpg\" alt=\"강화 학습 과정\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TzNN0bQ_j-3"
      },
      "source": [
        "각 단계별로:\n",
        "\n",
        "- 에이전트는 **환경(Environment)** 으로부터 **상태(S0)** 를 받습니다 - 게임의 첫 프레임을 수신합니다.\n",
        "- 해당 **상태(S0)** 를 기반으로 에이전트는 **행동(A0)** 을 취합니다 - 예: 오른쪽으로 이동\n",
        "- 환경은 **새로운 상태(S1)** 로 전환됩니다 - 새로운 프레임\n",
        "- 환경은 에이전트에게 **보상(R1)** 을 제공합니다 - 생존 시 *(양의 보상 +1)*\n",
        "\n",
        "Gymnasium 사용 시:\n",
        "\n",
        "1️⃣ `gymnasium.make()`로 환경 생성\n",
        "\n",
        "2️⃣ `observation = env.reset()`으로 환경 초기 상태 리셋\n",
        "\n",
        "각 단계에서:\n",
        "\n",
        "3️⃣ 모델을 사용해 행동 선택 (예시에서는 랜덤 행동)\n",
        "\n",
        "4️⃣ `env.step(action)` 실행 시 다음 반환값 얻음:\n",
        "- `observation`: 새로운 상태(st+1)\n",
        "- `reward`: 행동 실행 후 얻은 보상\n",
        "- `terminated`: 에피소드 종료 여부 (에이전트가 종료 상태 도달 시)\n",
        "- `truncated`: 새 버전에서 추가됨, 시간 제한 초과 또는 환경 경계 이탈 시 True\n",
        "- `info`: 추가 정보를 제공하는 딕셔너리 (환경에 따라 다름)\n",
        "\n",
        "자세한 설명은 👉 https://gymnasium.farama.org/api/env/#gymnasium.Env.step\n",
        "\n",
        "에피소드 종료 시:\n",
        "- `observation = env.reset()`으로 환경 초기화\n",
        "\n",
        "**예제를 살펴봅시다!** 코드를 꼼꼼히 읽어보세요"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7vOFlpA_ONz"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "# 먼저 LunarLander-v2 환경을 생성합니다\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# 환경을 초기화합니다\n",
        "observation, info = env.reset()\n",
        "\n",
        "for _ in range(20):\n",
        "    # 무작위 액션 선택\n",
        "    action = env.action_space.sample()\n",
        "    print(\"실행된 액션:\", action)\n",
        "\n",
        "    # 선택한 액션을 환경에서 실행하고\n",
        "    # 다음 상태, 보상, 종료 여부, 중단 여부, 추가 정보를 얻습니다\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "    # 게임이 종료(착륙 또는 충돌)되거나 중단(시간 초과)된 경우\n",
        "    if terminated or truncated:\n",
        "        # 환경 재설정\n",
        "        print(\"환경이 재설정되었습니다\")\n",
        "        observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIrKGGSlENZB"
      },
      "source": [
        "## LunarLander 환경 생성 🌛 및 작동 원리 이해\n",
        "\n",
        "### [환경 설명 🎮](https://gymnasium.farama.org/environments/box2d/lunar_lander/)\n",
        "\n",
        "이 첫 번째 튜토리얼에서는 우리의 에이전트인 [Lunar Lander](https://gymnasium.farama.org/environments/box2d/lunar_lander/)가 **달 표면에 정확히 착륙하도록** 훈련시킬 것입니다. 이를 위해 에이전트는 **속도와 위치(수평, 수직, 각도)를 조절하여 올바르게 착륙하는 방법을 학습**해야 합니다.\n",
        "\n",
        "---\n",
        "\n",
        "💡 새로운 환경을 사용할 때는 항상 문서를 확인하는 것이 좋은 습관입니다\n",
        "\n",
        "👉 https://gymnasium.farama.org/environments/box2d/lunar_lander/\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poLBgRocF9aT"
      },
      "source": [
        "환경이 어떻게 생겼는지 살펴봅시다:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZNPG0g_UGCfh"
      },
      "outputs": [],
      "source": [
        "# gym.make(\"<name_of_the_environment>\")을 사용하여 환경을 생성합니다.\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "env.reset()\n",
        "print(\"_____관측 공간_____ \\n\")\n",
        "print(\"관측 공간 형태:\", env.observation_space.shape)\n",
        "print(\n",
        "    \"임의의 관측 샘플:\", env.observation_space.sample()\n",
        ")  # 임의의 관측값을 가져옵니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MXc15qFE0M9"
      },
      "source": [
        "---\n",
        "\n",
        "## 🌌 Lunar Lander - 환경 개요\n",
        "\n",
        "### 🎮 행동 공간 (Action Space)\n",
        "동작 공간(에이전트가 선택할 수 있는 가능한 행동의 집합)은 총 4개의 이산적(discrete) 동작으로 구성되어 있습니다.\n",
        "- 이산형(discrete) 4가지 행동:\n",
        "  1. 아무 것도 하지 않기\n",
        "  2. 왼쪽 방향 엔진 작동\n",
        "  3. 메인 엔진 작동\n",
        "  4. 오른쪽 방향 엔진 작동\n",
        "\n",
        "### 👁️ 관찰 공간 (Observation Space)\n",
        "우리는 `관측 공간 형태 (8,)`를 통해 관측값이 크기 8의 벡터임을 알 수 있습니다. 각 값은 착륙선에 대한 서로 다른 정보를 담고 있습니다:\n",
        "- 수평 패드 좌표 (x)  \n",
        "- 수직 패드 좌표 (y)  \n",
        "- 수평 속도 (x)  \n",
        "- 수직 속도 (y)  \n",
        "- 기울기 각도  \n",
        "- 각속도  \n",
        "- 왼쪽 다리 접촉 지점이 땅에 닿았는지 여부 (bool)  \n",
        "- 오른쪽 다리 접촉 지점이 땅에 닿았는지 여부 (bool)\n",
        "\n",
        "### 💰 보상 시스템 (Reward System)\n",
        "각 스텝마다 보상이 주어지며, 에피소드의 총 보상은 **그 에피소드 내 모든 스텝의 보상의 합**입니다.\n",
        "- 각 스텝에서 보상은 다음과 같이 계산됩니다:\n",
        "    - 착륙 지점에 가까워질수록 높은 보상\n",
        "    - 느린 속도, 작은 기울기일수록 높은 보상\n",
        "    - 다리 하나당 지면 접촉 시 +10점\n",
        "    - 엔진 사용 시 페널티:\n",
        "        - 측면 엔진: -0.03점/프레임\n",
        "        - 메인 엔진: -0.3점/프레임\n",
        "    - 에피소드 종료 시:\n",
        "        - 착륙 성공: +100점\n",
        "        - 충돌: -100점\n",
        "\n",
        "### 🧩 기타 특징\n",
        "- 연료는 무제한\n",
        "- 착륙 지점: 항상 (0, 0)\n",
        "- 이산(discrete) 및 연속(continuous) 환경 모두 제공\n",
        "- 에피소드가 **200점 이상을 기록하면 해결된 것으로 간주**됩니다.\n",
        "\n",
        "--- "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "We5WqOBGLoSm"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____동작 공간_____ \\n\")\n",
        "print(\"동작 공간 크기:\", env.action_space.n)\n",
        "print(\"임의의 동작 샘플:\", env.action_space.sample())  # 임의의 동작을 선택합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFD9RAFjG8aq"
      },
      "source": [
        "#### 벡터화된 환경 (Vectorized Environment)\n",
        "\n",
        "- 우리는 16개의 환경을 하나의 벡터화된 환경으로 생성합니다. (여러 개의 독립된 환경을 하나로 묶는 방식)  \n",
        "이렇게 하면 **훈련 중에 더 다양한 경험을 얻을 수 있습니다.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99hqQ_etEy1N"
      },
      "outputs": [],
      "source": [
        "# 환경을 생성합니다\n",
        "env = make_vec_env(\"LunarLander-v2\", n_envs=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgrE86r5E5IK"
      },
      "source": [
        "## 모델 생성 🤖  \n",
        "- 우리는 환경을 학습했고, 문제를 이해했습니다: **왼쪽, 오른쪽, 메인 엔진을 제어하여 Lunar Lander를 착륙 지점에 정확히 착륙시키는 것**. 이제 이 문제를 해결할 알고리즘을 만들어봅시다 🚀.\n",
        "\n",
        "- 이를 위해 우리가 처음으로 사용할 딥 강화학습 라이브러리는 [Stable Baselines3 (SB3)](https://stable-baselines3.readthedocs.io/en/master/)입니다.\n",
        "\n",
        "- SB3는 **PyTorch 기반의 강화학습 알고리즘들을 안정적으로 구현한 라이브러리**입니다.\n",
        "\n",
        "---\n",
        "\n",
        "💡 새로운 라이브러리를 사용할 때 좋은 습관은 먼저 문서를 자세히 살펴보는 것입니다:  \n",
        "https://stable-baselines3.readthedocs.io/en/master/  \n",
        "그리고 튜토리얼을 직접 따라 해보는 것도 좋습니다.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLlClRW37Q7e"
      },
      "source": [
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit1/sb3.png\" alt=\"Stable Baselines3\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV4yiUM_9_Ka"
      },
      "source": [
        "이 문제를 해결하기 위해 우리는 SB3의 **PPO**를 사용할 것입니다.  \n",
        "[PPO(Proximal Policy Optimization)](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example)는 이 과정에서 배우게 될 **최신(state-of-the-art) 딥 강화학습 알고리즘 중 하나**입니다.\n",
        "\n",
        "PPO는 다음 두 가지 방식을 결합한 알고리즘입니다:\n",
        "\n",
        "- **가치 기반 강화학습(Value-based reinforcement learning)**:  \n",
        "  상태와 행동이 주어졌을 때, **가장 높은 가치를 가지는 행동을 학습**합니다.  \n",
        "\n",
        "- **정책 기반 강화학습(Policy-based reinforcement learning)**:  \n",
        "  **행동에 대한 확률 분포를 출력하는 정책을 학습**합니다.  \n",
        "\n",
        "두 접근법의 장점을 모두 살려 안정성과 효율성을 높인 것이 PPO입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qL_4HeIOrEJ"
      },
      "source": [
        "Stable-Baselines3는 설정이 간단합니다:\n",
        "\n",
        "1️⃣ **환경을 생성**합니다 (우리는 위에서 이미 생성했습니다)\n",
        "\n",
        "2️⃣ 사용할 **모델을 정의하고 인스턴스화**합니다  \n",
        "   예: `model = PPO(\"MlpPolicy\", env)`\n",
        "\n",
        "3️⃣ `model.learn`을 사용해 **에이전트를 훈련**시킵니다.  \n",
        "   이때 훈련에 사용할 timestep 수를 지정합니다.\n",
        "\n",
        "```python\n",
        "# 환경 생성\n",
        "env = gym.make('LunarLander-v2')\n",
        "\n",
        "# 에이전트 인스턴스화\n",
        "model = PPO('MlpPolicy', env, verbose=1)\n",
        "\n",
        "# 에이전트 훈련\n",
        "model.learn(total_timesteps=int(2e5))\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxI6hT1GE4-A"
      },
      "outputs": [],
      "source": [
        "# TODO: PPO MlpPolicy 아키텍처 정의하기\n",
        "# 입력이 벡터이므로 다층 퍼셉트론(MlpPolicy)을 사용합니다.\n",
        "# 만약 입력이 이미지 프레임이라면 CnnPolicy를 사용했을 것입니다.\n",
        "model ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAN7B0_HCVZC"
      },
      "source": [
        "#### 해결 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "543OHYDfcjK4"
      },
      "outputs": [],
      "source": [
        "# 해결 방법\n",
        "# 학습 속도를 높이기 위해 몇 가지 하이퍼파라미터를 추가했습니다\n",
        "model = PPO(\n",
        "    policy=\"MlpPolicy\", # 사용할 정책의 종류 (Multi-Layer Perceptron Policy)(다층 퍼셉트론)\n",
        "    env=env,            # 학습 환경 (LunarLander-v2)\n",
        "    n_steps=1024,       # 각 업데이트 단계에서 수집할 스텝 수\n",
        "    batch_size=64,      # 각 업데이트 단계에서 사용할 미니 배치 크기\n",
        "    n_epochs=4,         # 각 데이터 배치에 대해 수행할 업데이트 횟수\n",
        "    gamma=0.999,        # 감가율 (미래 보상의 현재 가치에 대한 할인율)\n",
        "    gae_lambda=0.98,    # GAE (Generalized Advantage Estimation)에 사용되는 람다 파라미터\n",
        "    ent_coef=0.01,      # 정책의 엔트로피에 대한 가중치 (탐험 장려)\n",
        "    verbose=1,          # 학습 과정 출력 수준 (0: 없음, 1: 상세, 2: 매우 상세)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJJk88yoBUi"
      },
      "source": [
        "## PPO 에이전트 훈련하기 🏃  \n",
        "- 에이전트를 **1,000,000 timesteps** 동안 훈련시켜봅시다. Colab에서는 **GPU를 꼭 활성화**하세요. 약 **20분 정도 소요**될 수 있지만, 단순 테스트용이라면 더 적은 timestep으로도 충분합니다.  \n",
        "- 훈련하는 동안 ☕ 쉬는 시간 가지세요. 충분히 자격 있어요 🤗"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qKnYkNiVp89p"
      },
      "outputs": [],
      "source": [
        "# TODO: 1,000,000 timesteps 동안 훈련하기\n",
        "\n",
        "# TODO: 모델 저장을 위한 파일 이름 지정 및 저장\n",
        "model_name = \"ppo-LunarLander-v2\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bQzQ-QcE3zo"
      },
      "source": [
        "#### 해결 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poBCy9u_csyR"
      },
      "outputs": [],
      "source": [
        "# 1,000,000 timesteps 동안 훈련\n",
        "model.learn(total_timesteps=1000000)\n",
        "\n",
        "# 모델 저장\n",
        "model_name = \"ppo-LunarLander-v2\"\n",
        "model.save(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "아래는 PPO 학습 로그의 각 **항목**과 **역할**을 정리한 표입니다:\n",
        "\n",
        "### **PPO 학습 로그 해설**  \n",
        "\n",
        "| **카테고리**       | **항목**               | **역할**                                                                 |\n",
        "|--------------------|------------------------|--------------------------------------------------------------------------|\n",
        "| **rollout/**       | `ep_len_mean`          | 에피소드의 평균 길이                                                     |\n",
        "|                    | `ep_rew_mean`          | 에피소드의 평균 보상                                                     |\n",
        "| **time/**          | `fps`                  | 초당 처리되는 step 수 (Frames Per Second)                                |\n",
        "|                    | `iterations`           | 전체 업데이트 반복 횟수                                                  |\n",
        "|                    | `time_elapsed`         | 총 학습 시간 (초)                                                        |\n",
        "|                    | `total_timesteps`      | 환경과 상호작용한 총 step 수                                             |\n",
        "| **train/**         | `approx_kl`            | 정책 업데이트 시 KL 발산 근사값                                          |\n",
        "|                    | `clip_fraction`        | 클리핑된 그래디언트 비율                                                 |\n",
        "|                    | `clip_range`           | PPO의 클리핑 범위 하이퍼파라미터                                         |\n",
        "|                    | `entropy_loss`         | 정책의 엔트로피 (불확실성) 손실                                          |\n",
        "|                    | `explained_variance`   | 값 함수가 보상을 얼마나 잘 예측하는지                                    |\n",
        "|                    | `learning_rate`        | 최적화기의 학습률                                                       |\n",
        "|                    | `loss`                 | 총 손실 (정책 + 값 함수 + 엔트로피)                                      |\n",
        "|                    | `n_updates`            | 네트워크 가중치 업데이트 횟수                                            |\n",
        "|                    | `policy_gradient_loss` | 정책 그래디언트 손실                                                     |\n",
        "|                    | `value_loss`           | 값 함수 (Critic)의 예측 오차                                             |\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY_HuedOoISR"
      },
      "source": [
        "## 에이전트 성능 평가하기 📈  \n",
        "- 평가 전, 환경을 반드시 [Monitor](https://stable-baselines3.readthedocs.io/en/master/common/monitor.html)로 감싸야 합니다.  \n",
        "- 이제 우리의 Lunar Lander 에이전트가 훈련을 마쳤으니 🚀, **그 성능을 확인해보는 단계**입니다.  \n",
        "- Stable-Baselines3는 이를 위한 함수 `evaluate_policy`를 제공합니다.\n",
        "\n",
        "- 아래 내용을 작성하려면 문서를 참고하세요:  \n",
        "  👉 [SB3 기본 사용법: 훈련, 저장, 불러오기](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)\n",
        "\n",
        "- 다음 단계에서는 **자동으로 에이전트를 평가하고, 리더보드에서 경쟁하는 법**도 다루겠지만, 지금은 수동으로 평가해봅시다.\n",
        "\n",
        "---\n",
        "\n",
        "💡 에이전트를 평가할 때는 **훈련에 사용한 환경이 아닌, 별도의 평가용 환경**을 생성해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yRpno0glsADy"
      },
      "outputs": [],
      "source": [
        "# TODO: 에이전트 평가하기\n",
        "# 평가용 새 환경 생성\n",
        "eval_env =\n",
        "\n",
        "# 10 에피소드 동안 모델 평가 (deterministic=True로 설정하여 항상 같은 행동 선택)\n",
        "mean_reward, std_reward =\n",
        "\n",
        "# 결과 출력\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqPKw3jt_pG5"
      },
      "source": [
        "#### 해결 방법"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpz8kHlt_a_m"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "eval_env = Monitor(gym.make(\"LunarLander-v2\", render_mode=\"rgb_array\"))\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    model, eval_env, n_eval_episodes=10, deterministic=True\n",
        ")\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reBhoODwcXfr"
      },
      "source": [
        "- 내 경우, **100만 스텝 학습 후 평균 보상은 `200.20 ± 20.80`** 이었습니다.  \n",
        "이는 우리의 Lunar Lander 에이전트가 **달 착륙 준비를 마쳤다는 뜻**입니다 🌛🥳."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK_kR78NoNb2"
      },
      "source": [
        "## 학습된 모델을 Hugging Face Hub에 업로드하기 🔥  \n",
        "훈련 결과가 좋았다면, 이제 학습된 모델을 🤗 **Hub에 업로드**해봅시다. 단 **한 줄의 코드로 가능**합니다!\n",
        "\n",
        "📚 라이브러리 공식 문서 👉  \n",
        "[Hugging Face 🤗 × Stable-Baselines3 통합](https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20)\n",
        "\n",
        "다음은 Space Invaders 모델 카드의 예시입니다 (우리도 비슷한 방식으로 LunarLander를 게시할 수 있습니다):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs-Ew7e1gXN3"
      },
      "source": [
        "`package_to_hub`을 사용하면, 다음 작업을 **한 줄로 자동화**할 수 있습니다:\n",
        "\n",
        "- 에이전트 **평가**\n",
        "- 에이전트가 플레이하는 **리플레이 영상 녹화**\n",
        "- **모델 카드 자동 생성**\n",
        "- 학습된 모델을 **Hugging Face Hub에 업로드**\n",
        "\n",
        "이 과정을 통해:\n",
        "\n",
        "- 여러분의 작업을 **멋지게 공개**할 수 있고 🔥  \n",
        "- 에이전트가 직접 플레이하는 모습을 **시각적으로 확인**할 수 있으며 👀  \n",
        "- 다른 사람들이 사용할 수 있도록 **모델을 공유**할 수 있고 💾  \n",
        "- 리더보드를 통해 **다른 사람들과 성능 비교도 가능**합니다 🏆 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JquRrWytA6eo"
      },
      "source": [
        "모델을 커뮤니티에 공유하기 위해서는 다음 **3가지 단계를 추가로 진행해야 합니다**:\n",
        "\n",
        "1️⃣ (아직 안 했다면) Hugging Face 계정 생성 ➡ [https://huggingface.co/join](https://huggingface.co/join)\n",
        "\n",
        "2️⃣ 로그인한 후, Hugging Face 웹사이트에서 인증 토큰을 저장해야 합니다.  \n",
        "- 새 토큰 생성: [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens)  \n",
        "- **권한은 write 역할로 설정해야 합니다**\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/create-token.jpg\" alt=\"Create HF Token\">\n",
        "\n",
        "- 토큰을 복사합니다  \n",
        "- 아래 셀을 실행하고 토큰을 붙여넣습니다"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZiFBBlzxzxY"
      },
      "outputs": [],
      "source": [
        "notebook_login()\n",
        "!git config --global credential.helper store"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tsf2uv0g_4p"
      },
      "source": [
        "Google Colab이나 Jupyter Notebook을 사용하지 않는 경우, 아래 명령어를 대신 사용해야 합니다: `huggingface-cli login`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGNh9VsZok0i"
      },
      "source": [
        "3️⃣ 이제 학습된 에이전트를 🤗 Hub에 업로드할 준비가 되었습니다 🔥`package_to_hub()` 함수를 사용하면 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay24l6bqFF18"
      },
      "source": [
        "`package_to_hub` 함수를 채워봅시다:  \n",
        "- `model`: 우리가 학습시킨 모델입니다.  \n",
        "- `model_name`: `model_save`에서 정의한 학습된 모델의 이름입니다.  \n",
        "- `model_architecture`: 우리가 사용한 모델 구조로, 여기서는 PPO입니다.  \n",
        "- `env_id`: 환경의 이름으로, 여기서는 `LunarLander-v2`입니다.  \n",
        "- `eval_env`: 평가 환경으로, `eval_env`에 정의되어 있습니다.  \n",
        "- `repo_id`: 생성되거나 업데이트될 Hugging Face Hub 저장소의 이름입니다 `(repo_id = {username}/{repo_name})`  \n",
        "\n",
        "💡 **좋은 이름 예시: {username}/{model_architecture}-{env_id}**\n",
        "\n",
        "- `commit_message`: 커밋 메시지"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPG7ofdGIHN8"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "from huggingface_sb3 import package_to_hub\n",
        "\n",
        "## TODO: repo_id를 정의하세요\n",
        "## repo_id는 Hugging Face Hub에 있는 모델 저장소의 ID입니다 (예: repo_id = {organization}/{repo_name}, ThomasSimonini/ppo-LunarLander-v2 와 같은 형식)\n",
        "repo_id =\n",
        "\n",
        "# TODO: 환경 이름을 정의하세요\n",
        "env_id =\n",
        "\n",
        "# 평가 환경을 생성하고 render_mode=\"rgb_array\"로 설정하세요\n",
        "eval_env = DummyVecEnv([lambda: Monitor(gym.make(env_id, render_mode=\"rgb_array\"))])\n",
        "\n",
        "# TODO: 사용한 모델 아키텍처를 정의하세요\n",
        "model_architecture = \"\"\n",
        "\n",
        "## TODO: 커밋 메시지를 정의하세요\n",
        "commit_message = \"\"\n",
        "\n",
        "# 모델 저장, 평가, 모델 카드 생성 및 에이전트의 리플레이 비디오 녹화를 수행한 뒤 Hub에 저장소를 업로드합니다\n",
        "package_to_hub(model=model, # 학습된 모델\n",
        "               model_name=model_name, # 학습된 모델의 이름\n",
        "               model_architecture=model_architecture, # 사용한 모델 아키텍처: 여기서는 PPO\n",
        "               env_id=env_id, # 환경 이름\n",
        "               eval_env=eval_env, # 평가 환경\n",
        "               repo_id=repo_id, # Hugging Face Hub에 있는 모델 저장소 ID (예: repo_id = {organization}/{repo_name}, ThomasSimonini/ppo-LunarLander-v2)\n",
        "               commit_message=commit_message)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Avf6gufJBGMw"
      },
      "source": [
        "#### 해결 방법\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2E--IJu8JYq"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "\n",
        "from huggingface_sb3 import package_to_hub\n",
        "\n",
        "# 위에서 정의한 변수들을 여기에 배치하세요\n",
        "# 환경 이름을 정의합니다\n",
        "env_id = \"LunarLander-v2\"\n",
        "\n",
        "# 사용한 모델 아키텍처를 정의합니다\n",
        "model_architecture = \"PPO\"\n",
        "\n",
        "## repo_id를 정의합니다\n",
        "## repo_id는 Hugging Face Hub에 있는 모델 저장소의 ID입니다 (예: repo_id = {organization}/{repo_name}, 예시: ThomasSimonini/ppo-LunarLander-v2)\n",
        "## 당신의 repo_id로 변경하세요\n",
        "repo_id = \"ThomasSimonini/ppo-LunarLander-v2\"  # 당신의 repo id로 변경하세요. 이건 제가 쓴 거라 푸시할 수 없어요 😄\n",
        "\n",
        "## 커밋 메시지를 정의합니다\n",
        "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
        "\n",
        "# 평가 환경을 생성하고 render_mode=\"rgb_array\"로 설정합니다\n",
        "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
        "\n",
        "# 위에서 작성한 package_to_hub 함수를 여기에 배치하세요\n",
        "package_to_hub(\n",
        "    model=model,  # 학습된 모델\n",
        "    model_name=model_name,  # 학습된 모델의 이름\n",
        "    model_architecture=model_architecture,  # 사용한 모델 아키텍처: 여기서는 PPO\n",
        "    env_id=env_id,  # 환경 이름\n",
        "    eval_env=eval_env,  # 평가 환경\n",
        "    repo_id=repo_id,  # Hugging Face Hub에 있는 모델 저장소 ID (예: repo_id = {organization}/{repo_name}, 예시: ThomasSimonini/ppo-LunarLander-v2)\n",
        "    commit_message=commit_message,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T79AEAWEFIxz"
      },
      "source": [
        "축하합니다 🥳 방금 첫 번째 딥 강화 학습 에이전트를 학습시키고 업로드했습니다! 위 스크립트를 실행했다면 다음과 같은 모델 저장소 링크가 출력되었을 거예요: https://huggingface.co/osanseviero/test_sb3 와 같은 형식입니다. 이 링크에서 다음을 확인할 수 있습니다:\n",
        "\n",
        "* 오른쪽에서 에이전트의 **비디오 미리보기**를 볼 수 있습니다.  \n",
        "* \"Files and versions\"를 클릭하면 저장소에 포함된 **모든 파일**을 확인할 수 있습니다.  \n",
        "* \"Use in stable-baselines3\"를 클릭하면 모델을 로드하는 데 사용할 수 있는 **코드 스니펫**을 볼 수 있습니다.  \n",
        "* 모델에 대한 설명이 담긴 **모델 카드(`README.md`)** 파일도 포함되어 있습니다.  \n",
        "\n",
        "Hub는 내부적으로 **git 기반 저장소**를 사용합니다 (git이 무엇인지 몰라도 걱정하지 마세요). 이 덕분에 실험하고 모델을 개선해 나가면서 새 버전으로 **모델을 업데이트**할 수 있습니다.\n",
        "\n",
        "그리고 이제 동료들과 함께 LunarLander-v2의 결과를 비교해 보세요!  \n",
        "🏆 **리더보드 확인하기** 👉 https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nWnuQHRfFRa"
      },
      "source": [
        "## 🤗 Hugging Face Hub에서 저장된 LunarLander 모델 불러오기  \n",
        "[ironbar](https://github.com/ironbar) 님의 기여에 감사드립니다.\n",
        "\n",
        "Hub에서 저장된 모델을 불러오는 건 아주 간단합니다.\n",
        "\n",
        "### 1. Stable-Baselines3 모델 목록 확인  \n",
        "아래 링크에서 Hugging Face Hub에 저장된 Stable-Baselines3 모델들의 목록을 확인할 수 있습니다:  \n",
        "👉 https://huggingface.co/models?library=stable-baselines3\n",
        "\n",
        "### 2. 원하는 모델 선택 후 `repo_id` 복사  \n",
        "불러오고 싶은 모델을 클릭한 뒤, 아래 이미지처럼 `repo_id`를 복사하세요:\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/copy-id.png\" alt=\"Copy-id\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNPLJF2bfiUw"
      },
      "source": [
        "2. 그런 다음 load_from_hub를 다음과 같이 사용하기만 하면 됩니다:\n",
        "- 저장소 ID\n",
        "- 파일명: 저장된 모델 파일과 해당 확장자(*.zip)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhb9-NtsinKB"
      },
      "source": [
        "Hub에서 다운로드한 모델은 Gym(Gymnasium의 이전 버전)으로 학습되었기 때문에, 환경을 올바르게 실행하려면 API 변환 도구인 shimmy를 설치해야 합니다.\n",
        "\n",
        "Shimmy 문서: https://github.com/Farama-Foundation/Shimmy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03WI-bkci1kH"
      },
      "outputs": [],
      "source": [
        "!pip install shimmy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "아래 코드에서 ModuleNotFoundError: No module named 'gymnasium.wrappers.monitoring' 과 같은 문제가 발생한다면:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip uninstall gymnasium\n",
        "!pip install gymnasium==0.28.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oj8PSGHJfwz3"
      },
      "outputs": [],
      "source": [
        "from huggingface_sb3 import load_from_hub\n",
        "\n",
        "repo_id = \"Classroom-workshop/assignment2-omar\"  # 저장소 ID\n",
        "filename = \"ppo-LunarLander-v2.zip\"  # 모델 파일 이름\n",
        "\n",
        "# 모델이 Python 3.8에서 학습된 경우 피클 프로토콜은 5번을 사용\n",
        "# 하지만 Python 3.6, 3.7은 프로토콜 4번을 사용\n",
        "# 호환성을 위해 다음을 수행해야 함:\n",
        "# 1. pickle5 설치 (colab 초반에 이미 설치함)\n",
        "# 2. PPO.load()에 전달할 사용자 정의 빈 객체 생성\n",
        "custom_objects = {\n",
        "    \"learning_rate\": 0.0,\n",
        "    \"lr_schedule\": lambda _: 0.0,\n",
        "    \"clip_range\": lambda _: 0.0,\n",
        "}\n",
        "\n",
        "checkpoint = load_from_hub(repo_id, filename)\n",
        "model = PPO.load(checkpoint, custom_objects=custom_objects, print_system_info=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs0Y-qgPgLUf"
      },
      "source": [
        "이 에이전트를 평가해보자:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAEVwK-aahfx"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
        "mean_reward, std_reward = evaluate_policy(\n",
        "    model, eval_env, n_eval_episodes=10, deterministic=True\n",
        ")\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQAwLnYFPk-s"
      },
      "source": [
        "## 몇 가지 추가 도전 과제 🏆  \n",
        "**직접 시도해보는 것**이 배우는 가장 좋은 방법이다! 지금까지 본 것처럼 현재 에이전트의 성능은 그다지 좋지 않다. 첫 번째 제안으로는 더 많은 스텝으로 학습시키는 것이다. 1,000,000 스텝으로 학습했을 때는 좋은 결과를 얻을 수 있었다!\n",
        "\n",
        "[리더보드](https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard)에서 너의 에이전트를 확인할 수 있다. 최고 순위에 도달할 수 있을까?\n",
        "\n",
        "다음은 순위를 올릴 수 있는 몇 가지 아이디어다:\n",
        "* 더 많은 스텝으로 학습하기\n",
        "* `PPO`의 다양한 하이퍼파라미터를 실험해보기  \n",
        "  하이퍼파라미터 목록: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters\n",
        "* [Stable-Baselines3 문서](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)를 참고해서 DQN 같은 다른 모델을 시도해보기\n",
        "* **새로 학습시킨 모델을 Hub에 업로드하기** 🔥\n",
        "\n",
        "**리더보드에서 클래스메이트들과 너의 LunarLander-v2 결과를 비교해보자** 🏆  \n",
        "https://huggingface.co/spaces/huggingface-projects/Deep-Reinforcement-Learning-Leaderboard\n",
        "\n",
        "달 착륙이 너무 지루하게 느껴진다면? **환경을 바꿔보는 것도 좋은 방법**이다. MountainCar-v0, CartPole-v1, CarRacing-v0 같은 환경을 써보자.  \n",
        "환경 작동 방식은 [Gym 문서](https://www.gymlibrary.dev/)를 참고하면 된다. 즐겁게 도전해보자 🎉"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lM95-dvmif8"
      },
      "source": [
        "________________________________________________________________________  \n",
        "이 챕터를 끝낸 걸 축하해! 이번 챕터는 가장 길었고, **정말 정보가 많았어.**\n",
        "\n",
        "여전히 이 모든 요소들이 헷갈린다면... 그건 아주 자연스러운 일이야! **나도 그랬고, RL을 공부한 모든 사람들도 마찬가지였어.**\n",
        "\n",
        "계속하기 전에 **내용을 확실히 이해하고 추가 도전 과제들을 시도해보는 시간을 가져**. 이런 요소들을 잘 익히고 탄탄한 기초를 갖추는 게 정말 중요해.\n",
        "\n",
        "물론 이 과정에서 앞으로도 이런 개념들을 더 깊이 다루겠지만, **다음 챕터로 넘어가기 전에 지금 제대로 이해해두는 게 좋아.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjLhT70TEZIn"
      },
      "source": [
        "다음 시간, 보너스 유닛 1에서는 Huggy the Dog에게 막대기를 가져오도록 훈련시킬 거야.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/notebooks/unit1/huggy.jpg\" alt=\"Huggy\"/>\n",
        "\n",
        "## 계속 배워나가자, 멋진 모습 그대로! 🤗"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QAN7B0_HCVZC",
        "BqPKw3jt_pG5"
      ],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "learn",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
