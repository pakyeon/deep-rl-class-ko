{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "# Optuna를 이용한 하이퍼파라미터 튜닝\n",
        "\n",
        "Github 저장소: https://github.com/araffin/tools-for-robotic-rl-icra2022\n",
        "\n",
        "Optuna: https://github.com/optuna/optuna\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "문서: https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "SB3 Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "\n",
        "RL Baselines3 Zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo)는 Stable-Baselines3를 사용해 사전 학습된 강화학습 에이전트들의 모음입니다.\n",
        "\n",
        "또한 에이전트 훈련, 평가, 하이퍼파라미터 튜닝, 영상 녹화를 위한 기본 스크립트들도 제공합니다.\n",
        "\n",
        "\n",
        "## 소개\n",
        "\n",
        "이 노트북에서는 하이퍼파라미터 튜닝의 중요성을 배웁니다. 먼저 수동으로 파라미터를 최적화한 뒤, Optuna를 사용해 탐색을 자동화하는 방법을 알아봅니다.\n",
        "\n",
        "\n",
        "## Pip을 이용한 의존성 및 Stable Baselines3 설치\n",
        "\n",
        "전체 의존성 목록은 [README](https://github.com/DLR-RM/stable-baselines3)에서 확인할 수 있습니다.\n",
        "\n",
        "```\n",
        "pip install stable-baselines3[extra]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYdv2ygjLaFL"
      },
      "outputs": [],
      "source": [
        "!pip install stable-baselines3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oexj67yWN5_k"
      },
      "outputs": [],
      "source": [
        "# 선택 사항: 추가 알고리즘 사용을 위해 SB3 Contrib 설치\n",
        "!pip install sb3-contrib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNah91r9x9EL"
      },
      "outputs": [],
      "source": [
        "# 하이퍼파라미터 튜닝 마지막 단계에서 Optuna를 사용할 예정입니다\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae32CtgzTG3R"
      },
      "source": [
        "가장 먼저 강화학습(RL) 모델을 임포트해야 하며, 어떤 문제에 어떤 모델을 사용할 수 있는지는 문서를 참고하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7tKaBFrTR0a"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcsXmYRMON9W"
      },
      "outputs": [],
      "source": [
        "# Contrib 저장소의 알고리즘들\n",
        "# https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "from sb3_contrib import QRDQN, TQC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLwjcfvuqtGE"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-khNkrgcI6Z1"
      },
      "source": [
        "# 파트 I: 튜닝된 하이퍼파라미터의 중요성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PytOtL9GdmrE"
      },
      "source": [
        "지도 학습과 비교했을 때, 딥 강화학습은 학습률, 뉴런 수, 레이어 수, 옵티마이저 등과 같은 하이퍼파라미터의 선택에 훨씬 더 민감합니다.\n",
        "\n",
        "하이퍼파라미터를 잘못 선택하면 수렴이 불안정하거나 성능이 저하될 수 있습니다. 이 문제는 네트워크 가중치와 환경 초기화를 위한 랜덤 시드에 따라 성능이 달라진다는 점에서 더 복잡해집니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hk8HSIC3qUjc"
      },
      "source": [
        "하이퍼파라미터 외에도 적절한 알고리즘을 선택하는 것 역시 중요한 결정입니다. 간단한 Pendulum 태스크를 통해 이를 시연해보겠습니다.\n",
        "\n",
        "[gym 문서](https://gym.openai.com/envs/Pendulum-v0/) 참고: \"도립진자 스윙업 문제는 제어 이론에서 고전적인 문제입니다. 이 버전에서는 진자가 임의의 위치에서 시작하며, 목표는 진자를 스윙업하여 수직 상태를 유지하도록 하는 것입니다.\"\n",
        "\n",
        "먼저 PPO 알고리즘과 4000 스텝(20 에피소드)이라는 작은 예산으로 시도해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ToIvihGq2N0"
      },
      "outputs": [],
      "source": [
        "env_id = \"Pendulum-v1\"\n",
        "# 평가에만 사용되는 Env\n",
        "eval_envs = make_vec_env(env_id, n_envs=10)\n",
        "# 4000 스텝의 훈련 시간\n",
        "budget_pendulum = 4000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWT2r6QE4yew"
      },
      "source": [
        "### PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCHk_-_4ndux"
      },
      "outputs": [],
      "source": [
        "ppo_model = PPO(\"MlpPolicy\", env_id, seed=0, verbose=1).learn(budget_pendulum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TP9C9AqLndxz",
        "outputId": "dd8e423c-dd4d-43cf-eac5-639e6748f02c"
      },
      "outputs": [],
      "source": [
        "mean_reward, std_reward = evaluate_policy(ppo_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
        "\n",
        "print(f\"PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHmJaJLl5ds4"
      },
      "source": [
        "### A2C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLL_pws25jh0"
      },
      "outputs": [],
      "source": [
        "# A2C 모델 정의 및 학습\n",
        "a2c_model = A2C(\"MlpPolicy\",env_id, seed=0, verbose=0).learn(budget_pendulum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic83jZwB5nVk"
      },
      "outputs": [],
      "source": [
        "# 학습된 A2C 모델 평가\n",
        "mean_reward, std_reward = evaluate_policy(a2c_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
        "\n",
        "print(f\"A2C Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_z1zFx2rVpG"
      },
      "source": [
        "둘 다 환경을 해결하는 데는 거리가 멉니다(평균 보상 약 -200).  \n",
        "이제 오프-폴리시 알고리즘을 시도해봅시다:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wYaVZJU5VL5"
      },
      "source": [
        "### PPO 더 오래 학습하기?\n",
        "\n",
        "더 오래 학습하면 도움이 될까?\n",
        "\n",
        "예산을 10배로 늘려서 시도해볼 수는 있지만, A2C/PPO의 경우에는 학습을 오래 한다고 해서 크게 나아지지 않는다. 대신 더 나은 하이퍼파라미터를 찾는 것이 필요하다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHsHpnQY6TWA"
      },
      "outputs": [],
      "source": [
        "# 더 긴 학습\n",
        "new_budget = 10 * budget_pendulum\n",
        "\n",
        "ppo_model = PPO(\"MlpPolicy\", env_id, seed=0, verbose=0).learn(new_budget)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OD9y1o36Xta"
      },
      "outputs": [],
      "source": [
        "mean_reward, std_reward = evaluate_policy(ppo_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
        "\n",
        "print(f\"PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEvQ9SJ15Xmh"
      },
      "source": [
        "### PPO - 튜닝된 하이퍼파라미터\n",
        "\n",
        "Optuna를 사용하면, 실제로 하이퍼파라미터를 튜닝하여 작동하는 솔루션을 찾을 수 있습니다([RL Zoo](https://github.com/DLR-RM/rl-baselines3-zoo/blob/master/hyperparams/ppo.yml) 참고):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-D_vvsb6jOZ"
      },
      "outputs": [],
      "source": [
        "tuned_params = {\n",
        "    \"gamma\": 0.9,\n",
        "    \"use_sde\": True,\n",
        "    \"sde_sample_freq\": 4,\n",
        "    \"learning_rate\": 1e-3,\n",
        "}\n",
        "\n",
        "# budget = 10 * budget_pendulum\n",
        "ppo_tuned_model = PPO(\"MlpPolicy\", env_id, seed=1, verbose=1, **tuned_params).learn(50_000, log_interval=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLuxoLxt67xO",
        "outputId": "6bc7479b-689f-4d0f-9f01-379c31afdb4e"
      },
      "outputs": [],
      "source": [
        "mean_reward, std_reward = evaluate_policy(ppo_tuned_model, eval_envs, n_eval_episodes=100, deterministic=True)\n",
        "\n",
        "print(f\"Tuned PPO Mean episode reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H33u_apWPp5"
      },
      "source": [
        "참고: 간단한 MountainCarContinuous 환경에서 SAC를 시도하면, 하이퍼파라미터를 튜닝하지 않으면 몇 가지 문제에 직면할 수 있다: https://github.com/rail-berkeley/softlearning/issues/76\n",
        "\n",
        "간단한 환경조차도 최신 알고리즘(SOTA)에게는 도전적일 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vdpPJ04nebx"
      },
      "source": [
        "# 파트 II: 대학원생 하강법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8PNN9kcgolk"
      },
      "source": [
        "### 챌린지 (10분): \"대학원생 하강법\"  \n",
        "이 챌린지의 목표는 제한된 예산(20,000 학습 스텝) 내에서 `CartPole-v1` 환경에서 A2C 알고리즘의 최대 성능을 끌어낼 수 있는 최적의 하이퍼파라미터를 찾는 것이다.  \n",
        "\n",
        "`CartPole-v1`에서의 최대 보상: 500  \n",
        "\n",
        "하이퍼파라미터는 서로 다른 랜덤 시드에서도 잘 작동해야 한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6aqxsini7H3"
      },
      "outputs": [],
      "source": [
        "budget = 20_000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDQ805DBi3KM"
      },
      "source": [
        "#### 기준선: 기본 하이퍼파라미터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyOCKf4Vt-HK"
      },
      "outputs": [],
      "source": [
        "eval_envs_cartpole = make_vec_env(\"CartPole-v1\", n_envs=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1PSNGcsi2dP"
      },
      "outputs": [],
      "source": [
        "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1).learn(budget)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d3X0G0ng2OE",
        "outputId": "8d550b14-a673-4abd-b9b8-c539d9c79c05"
      },
      "outputs": [],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, eval_envs_cartpole, n_eval_episodes=50, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-fi1-oKnUI2"
      },
      "source": [
        "**목표는 해당 기준선을 넘어서고 최적 점수인 500에 더 가까워지는 것입니다**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvq8zizok1X_"
      },
      "source": [
        "튜닝할 시간입니다!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaqCCH4gkRH_"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uDUfeZcyjPKS"
      },
      "outputs": [],
      "source": [
        "policy_kwargs = dict(\n",
        "    net_arch=[\n",
        "      dict(vf=[64, 64], pi=[64, 64]), # 액터/크리틱을 위한 네트워크 아키텍처\n",
        "    ],\n",
        "    activation_fn=nn.Tanh,\n",
        ")\n",
        "\n",
        "hyperparams = dict(\n",
        "    n_steps=5, # 정책 업데이트 전 데이터를 수집할 단계 수\n",
        "    learning_rate=7e-4,\n",
        "    gamma=0.99, # 할인 계수\n",
        "    max_grad_norm=0.5, # 그래디언트 클리핑의 최댓값\n",
        "    ent_coef=0.0, # 손실 계산을 위한 엔트로피 계수\n",
        ")\n",
        "\n",
        "model = A2C(\"MlpPolicy\", \"CartPole-v1\", seed=8, verbose=1, **hyperparams).learn(budget)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdNkEuJDKWJN"
      },
      "outputs": [],
      "source": [
        "mean_reward, std_reward = evaluate_policy(model, eval_envs_cartpole, n_eval_episodes=50, deterministic=True)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iL_G9DurUV75"
      },
      "source": [
        "힌트 - 권장 하이퍼파라미터 범위\n",
        "\n",
        "```python\n",
        "gamma = trial.suggest_float(\"gamma\", 0.9, 0.99999, log=True)\n",
        "max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
        "# 2**3 = 8에서 2**10 = 1024까지\n",
        "n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
        "learning_rate = trial.suggest_float(\"lr\", 1e-5, 1, log=True)\n",
        "ent_coef = trial.suggest_float(\"ent_coef\", 0.00000001, 0.1, log=True)\n",
        "# net_arch tiny: {\"pi\": [64], \"vf\": [64]}\n",
        "# net_arch default: {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
        "# activation_fn = nn.Tanh / nn.ReLU\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwFOp0j-ga-_"
      },
      "source": [
        "# 파트 III: 자동 하이퍼파라미터 튜닝"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88x7wMyyud5p"
      },
      "source": [
        "이 파트에서는 최적의 하이퍼파라미터를 자동으로 탐색할 수 있는 스크립트를 생성할 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auwR-30IvHeY"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VM6tUr-yuekR"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.pruners import MedianPruner\n",
        "from optuna.samplers import TPESampler\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQVfmM1dzA1d"
      },
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyBTVcAGzCRk"
      },
      "outputs": [],
      "source": [
        "N_TRIALS = 100  # 최대 트라이얼 수\n",
        "N_JOBS = 1 # 병렬로 실행할 작업 수\n",
        "N_STARTUP_TRIALS = 5  # N_STARTUP_TRIALS 후 무작위 샘플링 중지\n",
        "N_EVALUATIONS = 2  # 학습 중 평가 횟수\n",
        "N_TIMESTEPS = int(2e4)  # 학습 예산\n",
        "EVAL_FREQ = int(N_TIMESTEPS / N_EVALUATIONS)\n",
        "N_EVAL_ENVS = 5\n",
        "N_EVAL_EPISODES = 10\n",
        "TIMEOUT = int(60 * 15)  # 15분\n",
        "\n",
        "ENV_ID = \"CartPole-v1\"\n",
        "\n",
        "DEFAULT_HYPERPARAMS = {\n",
        "    \"policy\": \"MlpPolicy\",\n",
        "    \"env\": ENV_ID,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25HgcDYzvJ0b"
      },
      "source": [
        "### 실습 (5분): 탐색 공간 정의하기\n",
        "\n",
        "참고: https://github.com/optuna/optuna-examples/blob/main/rl/sb3_simple.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXo8AwGAvN8Q"
      },
      "outputs": [],
      "source": [
        "from typing import Any, Dict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def sample_a2c_params(trial: optuna.Trial) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    A2C 하이퍼파라미터 샘플러\n",
        "\n",
        "    :param trial: Optuna 트라이얼 객체\n",
        "    :return: 주어진 트라이얼에 대한 샘플링된 하이퍼파라미터\n",
        "    \"\"\"\n",
        "    # 0.9~0.9999 사이의 할인율\n",
        "    gamma = 1.0 - trial.suggest_float(\"gamma\", 0.0001, 0.1, log=True)\n",
        "    max_grad_norm = trial.suggest_float(\"max_grad_norm\", 0.3, 5.0, log=True)\n",
        "    # 8, 16, 32, ... 1024\n",
        "    n_steps = 2 ** trial.suggest_int(\"exponent_n_steps\", 3, 10)\n",
        "\n",
        "    ### 여기에 코드를 입력하세요\n",
        "    # TODO:\n",
        "    # - 학습률 탐색 공간 정의하기 [1e-5, 1] (log) -> `suggest_float`\n",
        "    # - 네트워크 아키텍처 탐색 공간 정의하기 [\"tiny\", \"small\"] -> `suggest_categorical`\n",
        "    # - 활성화 함수 탐색 공간 정의하기 [\"tanh\", \"relu\"]\n",
        "    learning_rate = ...\n",
        "    net_arch = ...\n",
        "    activation_fn = ...\n",
        "\n",
        "    ### 코드 끝\n",
        "\n",
        "    # 실제 값 표시\n",
        "    trial.set_user_attr(\"gamma_\", gamma)\n",
        "    trial.set_user_attr(\"n_steps\", n_steps)\n",
        "\n",
        "    net_arch = [\n",
        "        {\"pi\": [64], \"vf\": [64]}\n",
        "        if net_arch == \"tiny\"\n",
        "        else {\"pi\": [64, 64], \"vf\": [64, 64]}\n",
        "    ]\n",
        "\n",
        "    activation_fn = {\"tanh\": nn.Tanh, \"relu\": nn.ReLU}[activation_fn]\n",
        "\n",
        "    return {\n",
        "        \"n_steps\": n_steps,\n",
        "        \"gamma\": gamma,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"max_grad_norm\": max_grad_norm,\n",
        "        \"policy_kwargs\": {\n",
        "            \"net_arch\": net_arch,\n",
        "            \"activation_fn\": activation_fn,\n",
        "        },\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iybymNiJxNu7"
      },
      "source": [
        "### 목적 함수를 정의하세요"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJY8Z8tuxai7"
      },
      "source": [
        "먼저 Optuna에 주기적인 평가 결과를 보고하는 커스텀 콜백을 정의합니다:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5ijWTPzxSmd"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "\n",
        "class TrialEvalCallback(EvalCallback):\n",
        "    \"\"\"\n",
        "    트라이얼 평가 및 보고에 사용되는 콜백입니다.\n",
        "\n",
        "    :param eval_env: 평가 환경\n",
        "    :param trial: Optuna 트라이얼 객체\n",
        "    :param n_eval_episodes: 평가 에피소드 수\n",
        "    :param eval_freq:   콜백이 ``eval_freq``번 호출될 때마다 에이전트를 평가\n",
        "    :param deterministic: 평가가 확률적 정책을 사용해야 하는지, 결정적 정책을 사용해야 하는지 여부\n",
        "    :param verbose: 출력 없음의 경우 0, 정보 메시지의 경우 1, 디버그 메시지의 경우 2\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_env: gym.Env,\n",
        "        trial: optuna.Trial,\n",
        "        n_eval_episodes: int = 5,\n",
        "        eval_freq: int = 10000,\n",
        "        deterministic: bool = True,\n",
        "        verbose: int = 0,\n",
        "    ):\n",
        "\n",
        "        super().__init__(\n",
        "            eval_env=eval_env,\n",
        "            n_eval_episodes=n_eval_episodes,\n",
        "            eval_freq=eval_freq,\n",
        "            deterministic=deterministic,\n",
        "            verbose=verbose,\n",
        "        )\n",
        "        self.trial = trial\n",
        "        self.eval_idx = 0\n",
        "        self.is_pruned = False\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
        "            # 정책 평가(부모 클래스에서 수행)\n",
        "            super()._on_step()\n",
        "            self.eval_idx += 1\n",
        "            # Optuna에 보고서 보내기\n",
        "            self.trial.report(self.last_mean_reward, self.eval_idx)\n",
        "            # 필요한 경우 가지치기 시도\n",
        "            if self.trial.should_prune():\n",
        "                self.is_pruned = True\n",
        "                return False\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cHNM_cFO3vs"
      },
      "source": [
        "### 실습 (10분): 목적 함수 정의하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76voi9AXxlCq"
      },
      "source": [
        "그런 다음 하이퍼파라미터를 샘플링하고 모델을 생성한 후 그 결과를 Optuna로 반환하는 목적 함수를 정의합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0yEokTDxhrC"
      },
      "outputs": [],
      "source": [
        "def objective(trial: optuna.Trial) -> float:\n",
        "    \"\"\"\n",
        "    Optuna에서 평가에 사용하는 목적 함수입니다.\n",
        "\n",
        "    하나의 설정(즉, 하나의 하이퍼파라미터 세트)을 평가합니다.\n",
        "\n",
        "    트라이얼 객체가 주어지면 하이퍼파라미터를 샘플링하고,\n",
        "    평가한 후 결과를 보고합니다(학습 후 평균 에피소드 보상).\n",
        "\n",
        "    :param trial: Optuna 트라이얼 객체\n",
        "    :return: 학습 후 평균 에피소드 보상\n",
        "    \"\"\"\n",
        "\n",
        "    kwargs = DEFAULT_HYPERPARAMS.copy()\n",
        "    ### 여기에 코드 작성\n",
        "    # TODO:\n",
        "    # 1. 하이퍼파라미터를 샘플링하고 기본 키워드 인수를 업데이트합니다: `kwargs.update(other_params)`\n",
        "    # 2. 평가 환경을 생성합니다.\n",
        "    # 3. `TrialEvalCallback`을 생성합니다.\n",
        "    # 1. 하이퍼파라미터를 샘플링하고 키워드 인수를 업데이트합니다.\n",
        "\n",
        "    # RL 모델 생성\n",
        "\n",
        "\n",
        "    # 2. `make_vec_env`, `ENV_ID`, `N_EVAL_ENVS`를 사용하여 평가에 사용될 환경을 생성합니다.\n",
        "\n",
        "    \n",
        "    # 3. 위에 정의된 `TrialEvalCallback` 콜백을 생성하여 `EVAL_FREQ`마다 `N_EVAL_EPISODES`를 사용하여 주기적으로 평가하고\n",
        "    # 성능을 보고합니다.\n",
        "    # TrialEvalCallback 시그니처:\n",
        "    # TrialEvalCallback(eval_env, trial, n_eval_episodes, eval_freq, deterministic, verbose)\n",
        "    eval_callback = ...\n",
        "\n",
        "    ### 코드 끝\n",
        "\n",
        "    nan_encountered = False\n",
        "    try:\n",
        "        # 모델 학습\n",
        "        model.learn(N_TIMESTEPS, callback=eval_callback)\n",
        "    except AssertionError as e:\n",
        "        # 때때로 무작위 하이퍼파라미터는 NaN을 생성할 수 있습니다.\n",
        "        print(e)\n",
        "        nan_encountered = True\n",
        "    finally:\n",
        "        # 메모리 해제\n",
        "        model.env.close()\n",
        "        eval_envs.close()\n",
        "\n",
        "    # 옵티마이저에게 트라이얼이 실패했음을 알립니다.\n",
        "    if nan_encountered:\n",
        "        return float(\"nan\")\n",
        "\n",
        "    if eval_callback.is_pruned:\n",
        "        raise optuna.exceptions.TrialPruned()\n",
        "\n",
        "    return eval_callback.last_mean_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMFLu_M0ymzj"
      },
      "source": [
        "### 최적화 루프"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UU17YpjymPr"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "\n",
        "# 더 빠른 학습을 위해 pytorch num threads를 1로 설정\n",
        "th.set_num_threads(1)\n",
        "# 샘플러 선택, random, TPESampler, CMAES 등 가능\n",
        "sampler = TPESampler(n_startup_trials=N_STARTUP_TRIALS)\n",
        "# 최대 예산의 1/3을 사용하기 전에는 가지치기하지 않음\n",
        "pruner = MedianPruner(\n",
        "    n_startup_trials=N_STARTUP_TRIALS, n_warmup_steps=N_EVALUATIONS // 3\n",
        ")\n",
        "# 스터디를 생성하고 하이퍼파라미터 최적화 시작\n",
        "study = optuna.create_study(sampler=sampler, pruner=pruner, direction=\"maximize\")\n",
        "\n",
        "try:\n",
        "    study.optimize(objective, n_trials=N_TRIALS, n_jobs=N_JOBS, timeout=TIMEOUT)\n",
        "except KeyboardInterrupt:\n",
        "    pass\n",
        "\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "\n",
        "print(f\"  Value: {trial.value}\")\n",
        "\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "print(\"  User attrs:\")\n",
        "for key, value in trial.user_attrs.items():\n",
        "    print(f\"    {key}: {value}\")\n",
        "\n",
        "# 보고서 작성\n",
        "study.trials_dataframe().to_csv(\"study_results_a2c_cartpole.csv\")\n",
        "\n",
        "fig1 = plot_optimization_history(study)\n",
        "fig2 = plot_param_importances(study)\n",
        "\n",
        "fig1.show()\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCbep6z1h3D1"
      },
      "source": [
        "전체 코드 예시: https://github.com/DLR-RM/rl-baselines3-zoo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yUeYnfJVpB2"
      },
      "source": [
        "# 결론\n",
        "\n",
        "이 노트북에서 우리는 다음과 같은 내용을 살펴보았습니다:\n",
        "- 좋은 하이퍼파라미터의 중요성  \n",
        "- Optuna를 활용한 자동 하이퍼파라미터 탐색 방법"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "icra22_optuna_lab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "learn2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
